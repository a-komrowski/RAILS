{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db966c2",
   "metadata": {},
   "source": [
    "# Explorative Cluster-Analyse der Gleisbilder\n",
    "\n",
    "Diese Notebook führt eine unüberwachte Cluster-Analyse der Center-Bilder durch, um ohne Vorwissen Strukturen und Muster in den Daten zu entdecken.\n",
    "\n",
    "**Ziele:**\n",
    "- Feature-Extraktion mit vortrainiertem CNN (ResNet50)\n",
    "- K-Means und DBSCAN Clustering\n",
    "- Evaluierung der Cluster-Qualität\n",
    "- Visualisierung und Interpretation der Ergebnisse\n",
    "\n",
    "**Erwartete Erkenntnisse:**\n",
    "- Unterschiedliche Untergrundtypen (Schotter vs. Asphalt)\n",
    "- Verschiedene Beleuchtungsbedingungen\n",
    "- Stadtspezifische Unterschiede\n",
    "- Anomalien oder seltene Situationen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa00d48",
   "metadata": {},
   "source": [
    "### 1. Setup: Importe, Umgebung & Reproduzierbarkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import shutil\n",
    "# Force TensorFlow to use CPU only - MUST BE SET BEFORE IMPORTING TENSORFLOW\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# Deep Learning Libraries - Import AFTER setting environment variables\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Image Processing\n",
    "from img_preprocessing import ImagePreprocessor\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c1b32",
   "metadata": {},
   "source": [
    "### 2. Konfiguration: Pfade, Modellparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with timestamped results\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp for this run\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = \"./datasets/clustering_sample_100\"  # Path to sampled dataset\n",
    "BASE_RESULTS_PATH = f\"./results_{TIMESTAMP}\"                       # Base results directory\n",
    "RESULTS_PATH = f\"{BASE_RESULTS_PATH}/clustering_analysis\"  # Timestamped analysis results\n",
    "CLUSTERS_PATH = f\"{BASE_RESULTS_PATH}/clustered_images\"    # Timestamped clustered images\n",
    "\n",
    "BATCH_SIZE = 32                                     # Batch size for feature extraction\n",
    "IMG_SIZE = (224, 224)                              # Input size for ResNet50\n",
    "FEATURE_DIM = 2048                                 # ResNet50 feature dimension\n",
    "N_CLUSTERS_RANGE = range(3, 16)                    # Range of k values to test\n",
    "PCA_COMPONENTS = 50                                # PCA components for dimensionality reduction\n",
    "RANDOM_STATE = 42                                  # Random state for reproducibility\n",
    "\n",
    "print(f\"Results will be saved to:\")\n",
    "print(f\"  Analysis results: {RESULTS_PATH}\")\n",
    "print(f\"  Clustered images: {CLUSTERS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051fce4",
   "metadata": {},
   "source": [
    "### 3. Ergebnisse-Verzeichnis & Datensatz-Check\n",
    "- Legt das Ergebnisverzeichnis an (idempotent).\n",
    "- Gibt Pfade für Dataset und Results aus.\n",
    "- Prüft, ob der Dataset-Pfad existiert; falls nicht, listet verfügbare Unterordner in ./datasets inkl. Bildanzahl.\n",
    "- Zählt die .png-Bilder im gewählten Dataset und gibt die Anzahl aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamped results directories\n",
    "Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(CLUSTERS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created timestamped directories:\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Analysis results path: {RESULTS_PATH}\")\n",
    "print(f\"Clustered images path: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Dataset path {DATASET_PATH} does not exist!\")\n",
    "    print(\"Available dataset directories:\")\n",
    "    datasets_dir = Path(\"./datasets\")\n",
    "    if datasets_dir.exists():\n",
    "        for subdir in datasets_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                img_count = len([f for f in subdir.iterdir() if f.suffix == '.png'])\n",
    "                print(f\"  {subdir.name}: {img_count} images\")\n",
    "else:\n",
    "    img_count = len([f for f in Path(DATASET_PATH).iterdir() if f.suffix == '.png'])\n",
    "    print(f\"Found {img_count} images in dataset\")\n",
    "\n",
    "# Save run metadata\n",
    "run_metadata = {\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'dataset_path': str(Path(DATASET_PATH).resolve()),\n",
    "    'total_images_found': img_count if os.path.exists(DATASET_PATH) else 0,\n",
    "    'analysis_results_path': str(Path(RESULTS_PATH).resolve()),\n",
    "    'clustered_images_path': str(Path(CLUSTERS_PATH).resolve()),\n",
    "    'configuration': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'feature_dim': FEATURE_DIM,\n",
    "        'n_clusters_range': list(N_CLUSTERS_RANGE),\n",
    "        'pca_components': PCA_COMPONENTS,\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save run metadata\n",
    "with open(f\"{RESULTS_PATH}/run_metadata.json\", 'w') as f:\n",
    "    json.dump(run_metadata, f, indent=2)\n",
    "    \n",
    "print(f\"\\nRun metadata saved to: {RESULTS_PATH}/run_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838965c4",
   "metadata": {},
   "source": [
    "### 4. Dateiname-Parsing, Dataset-Metadaten & Tenant-Verteilung\n",
    "- Extrahiert aus `*_C.png`-Dateinamen: **tenant**, **SID** und **original_filename** (`parse_filename`).\n",
    "- Liest das Dataset ein und baut eine **Dateiliste** + **Tenant-Verteilung** auf (`load_dataset_info`).\n",
    "- Gibt **Gesamtanzahl Bilder**, **#Tenants** und **Verteilung je Tenant (%)** aus.\n",
    "- Hinweis: Nur Dateien mit Suffix **`_C.png`** werden berücksichtigt; Parser bei abweichender Konvention anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c45d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename: str):\n",
    "    \"\"\"Parse filename to extract tenant, SID, and original filename.\"\"\"\n",
    "    if not filename.endswith('_C.png'):\n",
    "        return None, None, None\n",
    "    \n",
    "    name_without_ext = filename[:-6]  # Remove '_C.png'\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        tenant = parts[0]\n",
    "        sid = parts[1]\n",
    "        original_filename = '_'.join(parts[2:])\n",
    "        return tenant, sid, original_filename\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "def load_dataset_info(dataset_path: str):\n",
    "    \"\"\"Load and analyze dataset information.\"\"\"\n",
    "    image_files = []\n",
    "    tenant_distribution = defaultdict(int)\n",
    "    \n",
    "    for file_path in Path(dataset_path).glob('*_C.png'):\n",
    "        filename = file_path.name\n",
    "        tenant, sid, original_name = parse_filename(filename)\n",
    "        \n",
    "        if tenant:\n",
    "            image_files.append({\n",
    "                'filepath': str(file_path),\n",
    "                'filename': filename,\n",
    "                'tenant': tenant,\n",
    "                'sid': sid,\n",
    "                'original_name': original_name\n",
    "            })\n",
    "            tenant_distribution[tenant] += 1\n",
    "    \n",
    "    return image_files, dict(tenant_distribution)\n",
    "\n",
    "# Load dataset information\n",
    "print(\"Loading dataset information...\")\n",
    "image_files, tenant_distribution = load_dataset_info(DATASET_PATH)\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_files)}\")\n",
    "print(f\"Number of tenants: {len(tenant_distribution)}\")\n",
    "print(\"\\nTenant distribution:\")\n",
    "for tenant, count in sorted(tenant_distribution.items()):\n",
    "    percentage = (count / len(image_files)) * 100\n",
    "    print(f\"  {tenant}: {count} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad44ed",
   "metadata": {},
   "source": [
    "### 5. Feature-Extraktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f40cd",
   "metadata": {},
   "source": [
    "#### 5.1 Feature-Extraktion vorbereiten: Funktionen & Pipeline\n",
    "- Definiert die Hilfsfunktionen `load_and_preprocess_image`, `create_feature_extractor` und `extract_features_batch`.\n",
    "- Zweck: Aufbau der Pipeline (Laden, Normalisieren, Batch-Verarbeitung) — **keine Ausführung** der Extraktion.\n",
    "- Erwartung: ResNet50-Encoder mit GlobalAveragePooling → Feature-Vektor-Dimension **2048**.\n",
    "- Voraussetzungen: `IMG_SIZE`, `BATCH_SIZE`, `preprocess_input` sind gesetzt.\n",
    "- Wird in **Zelle 6** aufgerufen, um die Features tatsächlich zu berechnen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b4949",
   "metadata": {},
   "source": [
    "#### 5.0 Preprocessing-Methoden Konfiguration\n",
    "\n",
    "**Wählen Sie eine Preprocessing-Methode für die Feature-Extraktion:**\n",
    "- `'none'`: Keine zusätzliche Vorverarbeitung (Standard ResNet50 Preprocessing)\n",
    "- `'hist_eq'`: Histogram Equalization - Verbessert den Kontrast\n",
    "- `'clahe'`: CLAHE (Contrast Limited Adaptive Histogram Equalization) - Adaptive Kontrastverbesserung\n",
    "\n",
    "**Hinweis:** Ändern Sie die Variable `PREPROCESSING_METHOD` in der nächsten Zelle, um zwischen den Methoden zu wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Method Selection\n",
    "# =============================\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Available preprocessing methods\n",
    "PREPROCESSING_METHODS = {\n",
    "    'none': 'Keine Vorverarbeitung (Standard)',\n",
    "    'hist_eq': 'Histogram Equalization',\n",
    "    'clahe': 'CLAHE (Adaptive Histogram Equalization)'\n",
    "}\n",
    "\n",
    "# Create options as list of tuples (display_name, value)\n",
    "method_options = [(display_name, key) for key, display_name in PREPROCESSING_METHODS.items()]\n",
    "\n",
    "# Create interactive dropdown for method selection\n",
    "method_selector = widgets.Dropdown(\n",
    "    options=method_options,\n",
    "    value='none',  # Now this matches the actual key values\n",
    "    description='Methode:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "info_text = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style=\"padding: 10px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <h4>🔧 Preprocessing-Methoden Vergleich</h4>\n",
    "    <ul>\n",
    "    <li><b>Keine Vorverarbeitung:</b> Standard ResNet50 Preprocessing ohne zusätzliche Bildverbesserung</li>\n",
    "    <li><b>Histogram Equalization:</b> Globale Kontrastverbesserung durch Histogramm-Angleichung</li>\n",
    "    <li><b>CLAHE:</b> Adaptive lokale Kontrastverbesserung mit Begrenzung</li>\n",
    "    </ul>\n",
    "    <p><i>Wählen Sie eine Methode und führen Sie die nachfolgenden Zellen aus.</i></p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Global variable to store selected method\n",
    "PREPROCESSING_METHOD = 'none'\n",
    "\n",
    "def on_method_change(change):\n",
    "    global PREPROCESSING_METHOD\n",
    "    PREPROCESSING_METHOD = change['new']\n",
    "    print(f\"✅ Preprocessing-Methode geändert zu: {PREPROCESSING_METHODS[PREPROCESSING_METHOD]}\")\n",
    "\n",
    "method_selector.observe(on_method_change, names='value')\n",
    "\n",
    "# Display the interface\n",
    "print(\"🔧 Wählen Sie eine Preprocessing-Methode:\")\n",
    "display(widgets.VBox([method_selector, info_text]))\n",
    "\n",
    "print(f\"\\n📋 Aktuelle Auswahl: {PREPROCESSING_METHODS[PREPROCESSING_METHOD]}\")\n",
    "print(\"💡 Tipp: Nach Änderung der Methode alle nachfolgenden Zellen neu ausführen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path: str, target_size: tuple = IMG_SIZE):\n",
    "    \"\"\"Load and preprocess image for ResNet50 with optional preprocessing methods.\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = image.load_img(image_path, target_size=target_size)\n",
    "        \n",
    "        # Apply selected preprocessing method\n",
    "        if PREPROCESSING_METHOD == 'hist_eq':\n",
    "            # Convert PIL image to numpy array for preprocessing\n",
    "            img_array = np.array(img)\n",
    "            # Apply histogram equalization\n",
    "            img_processed = ImagePreprocessor.method_1_histogram_equalization(img_array)\n",
    "            # Convert back to PIL Image\n",
    "            img = Image.fromarray(img_processed.astype('uint8'))\n",
    "            \n",
    "        elif PREPROCESSING_METHOD == 'clahe':\n",
    "            # Convert PIL image to numpy array for preprocessing\n",
    "            img_array = np.array(img)\n",
    "            # Apply CLAHE\n",
    "            img_processed = ImagePreprocessor.method_2_clahe(img_array)\n",
    "            # Convert back to PIL Image\n",
    "            img = Image.fromarray(img_processed.astype('uint8'))\n",
    "            \n",
    "        # Note: If PREPROCESSING_METHOD == 'none', no additional preprocessing is applied\n",
    "        \n",
    "        # Convert to array and prepare for ResNet50\n",
    "        img_array = image.img_to_array(img)    \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_feature_extractor():\n",
    "    \"\"\"Create ResNet50 feature extractor (without top classification layer).\"\"\"\n",
    "    print(\"Loading ResNet50 model...\")\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "        pooling='avg'  # Global average pooling\n",
    "    )\n",
    "    \n",
    "    # The model output will be (batch_size, 2048) features\n",
    "    print(f\"Feature extractor output shape: {base_model.output_shape}\")\n",
    "    return base_model\n",
    "\n",
    "def extract_features_batch(model, image_paths: list, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Extract features from images in batches.\"\"\"\n",
    "    features = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    print(f\"Extracting features from {len(image_paths)} images...\")\n",
    "    print(f\"Using preprocessing method: {PREPROCESSING_METHODS.get(PREPROCESSING_METHOD, 'Unknown')}\")\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        batch_valid_paths = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            img_array = load_and_preprocess_image(img_path)\n",
    "            if img_array is not None:\n",
    "                batch_images.append(img_array[0])  # Remove batch dimension\n",
    "                batch_valid_paths.append(img_path)\n",
    "        \n",
    "        if batch_images:\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_features = model.predict(batch_images, verbose=0)\n",
    "            features.extend(batch_features)\n",
    "            valid_paths.extend(batch_valid_paths)\n",
    "        \n",
    "        # Progress update\n",
    "        processed = min(i + batch_size, len(image_paths))\n",
    "        print(f\"Progress: {processed}/{len(image_paths)} images processed\")\n",
    "    \n",
    "    return np.array(features), valid_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd2971",
   "metadata": {},
   "source": [
    "#### 5.1 Feature-Extraktion mit wählbarer Vorverarbeitung\n",
    "\n",
    "- **Adaptive Pipeline**: Nutzt die in Zelle 5.0 gewählte Preprocessing-Methode\n",
    "- **Funktionen**: `load_and_preprocess_image`, `create_feature_extractor` und `extract_features_batch`\n",
    "- **Preprocessing-Integration**: \n",
    "  - `'none'`: Standard ResNet50 Preprocessing\n",
    "  - `'hist_eq'`: Histogram Equalization vor ResNet50 \n",
    "  - `'clahe'`: CLAHE vor ResNet50\n",
    "- **Feature-Extraktion**: ResNet50-Encoder mit GlobalAveragePooling → **2048-dimensionale Features**\n",
    "- **Batch-Verarbeitung**: Effiziente Verarbeitung in konfigurierbaren Batches\n",
    "\n",
    "**Wichtig**: Nach Änderung der Preprocessing-Methode müssen alle nachfolgenden Zellen neu ausgeführt werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "# Extract features using ResNet50\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = create_feature_extractor()\n",
    "\n",
    "# Extract image paths\n",
    "image_paths = [item['filepath'] for item in image_files]\n",
    "\n",
    "# Extract features\n",
    "start_time = time.time()\n",
    "features, valid_paths = extract_features_batch(feature_extractor, image_paths)\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFeature extraction completed in {extraction_time:.2f} seconds\")\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "print(f\"Valid images: {len(valid_paths)}/{len(image_paths)}\")\n",
    "\n",
    "# Update image_files to only include valid images\n",
    "valid_image_files = []\n",
    "for img_file in image_files:\n",
    "    if img_file['filepath'] in valid_paths:\n",
    "        valid_image_files.append(img_file)\n",
    "\n",
    "image_files = valid_image_files\n",
    "print(f\"Updated image files: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Check: Current Preprocessing Configuration\n",
    "# =================================================\n",
    "\n",
    "print(\"🔍 Aktuelle Konfiguration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Preprocessing-Methode: {PREPROCESSING_METHODS.get(PREPROCESSING_METHOD, 'Unbekannt')}\")\n",
    "print(f\"🔧 Methoden-Code: {PREPROCESSING_METHOD}\")\n",
    "print(f\"📁 Dataset-Pfad: {DATASET_PATH}\")\n",
    "print(f\"🖼️  Bildgröße: {IMG_SIZE}\")\n",
    "print(f\"📦 Batch-Größe: {BATCH_SIZE}\")\n",
    "print(f\"🎯 Feature-Dimension: {FEATURE_DIM}\")\n",
    "\n",
    "# Import check for ImagePreprocessor\n",
    "try:\n",
    "    from img_preprocessing import ImagePreprocessor\n",
    "    print(\"✅ ImagePreprocessor erfolgreich importiert\")\n",
    "    \n",
    "    # Check if methods exist\n",
    "    if hasattr(ImagePreprocessor, 'method_1_histogram_equalization'):\n",
    "        print(\"✅ Histogram Equalization Methode verfügbar\")\n",
    "    else:\n",
    "        print(\"❌ Histogram Equalization Methode nicht gefunden\")\n",
    "        \n",
    "    if hasattr(ImagePreprocessor, 'method_2_clahe'):\n",
    "        print(\"✅ CLAHE Methode verfügbar\")\n",
    "    else:\n",
    "        print(\"❌ CLAHE Methode nicht gefunden\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ ImagePreprocessor Import-Fehler: {e}\")\n",
    "    print(\"💡 Stellen Sie sicher, dass img_preprocessing.py im gleichen Verzeichnis liegt\")\n",
    "\n",
    "print(\"\\n🚀 Bereit für Feature-Extraktion!\")\n",
    "\n",
    "if PREPROCESSING_METHOD != 'none':\n",
    "    print(f\"⚠️  Hinweis: Preprocessing-Methode '{PREPROCESSING_METHODS[PREPROCESSING_METHOD]}' wird angewendet\")\n",
    "    print(\"   Dies kann die Verarbeitungszeit verlängern, aber die Clustering-Qualität verbessern.\")\n",
    "else:\n",
    "    print(\"ℹ️  Standard ResNet50 Preprocessing wird verwendet (keine zusätzliche Vorverarbeitung)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e92e21",
   "metadata": {},
   "source": [
    "### 6. Feature-Nachbearbeitung (Normalisierung & PCA -> Vorbereitung fürs Clustering)\n",
    "- L2-Normierung zeilenweise (`normalize(..., axis=1)`): bringt alle Embeddings auf Einheitsnorm → stabilere Distanz-/KMeans-Ergebnisse.\n",
    "- Gibt Kennzahlen vor/nach Normalisierung aus (Mean/Std/Min/Max sowie L2-Norm eines Beispiels).\n",
    "- PCA reduziert Dimension von `FEATURE_DIM` → `PCA_COMPONENTS` (Fit auf normalisierten Features); meldet erklärte Gesamtvarianz und Shape.\n",
    "- Fürs Clustering wird zunächst `features_for_clustering = features_normalized` genutzt; **optional** auf `features_pca` umstellen (schneller/rauschärmer, ggf. bessere Separation).\n",
    "- Reproduzierbarkeit: `random_state=RANDOM_STATE` bei PCA gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "print(\"Normalizing features...\")\n",
    "features_normalized = normalize(features, norm='l2', axis=1)\n",
    "\n",
    "print(f\"Original feature statistics:\")\n",
    "print(f\"  Mean: {features.mean():.4f}\")\n",
    "print(f\"  Std: {features.std():.4f}\")\n",
    "print(f\"  Min: {features.min():.4f}\")\n",
    "print(f\"  Max: {features.max():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized feature statistics:\")\n",
    "print(f\"  Mean: {features_normalized.mean():.4f}\")\n",
    "print(f\"  Std: {features_normalized.std():.4f}\")\n",
    "print(f\"  L2 norm (first sample): {np.linalg.norm(features_normalized[0]):.4f}\")\n",
    "\n",
    "# Optional: Apply PCA for dimensionality reduction\n",
    "print(f\"\\nApplying PCA to reduce dimensions from {FEATURE_DIM} to {PCA_COMPONENTS}...\")\n",
    "pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
    "features_pca = pca.fit_transform(features_normalized)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"PCA features shape: {features_pca.shape}\")\n",
    "\n",
    "# We'll use both normalized full features and PCA features for comparison\n",
    "features_for_clustering = features_normalized  # Start with full features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5e076",
   "metadata": {},
   "source": [
    "### 7. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaaa5af",
   "metadata": {},
   "source": [
    "#### 7.1 K-Means: k-Sweep & Clusterbewertung (Inertia, Silhouette, Clustergrößen)\n",
    "- Ein k-Sweep ist das **systematische Durchtesten mehrerer k-Werte** (z. B. k=3…15) bei K-Means/MiniBatchKMeans, um eine sinnvolle Clusterzahl zu finden.\n",
    "- Führt K-Means/MiniBatchKMeans über `N_CLUSTERS_RANGE` auf `features_for_clustering` aus (Default: `MiniBatchKMeans` für Effizienz).\n",
    "- Für jedes k: fit & predict → berechnet **Inertia**, **Silhouette-Score** und **Clustergrößen** (inkl. min/max); speichert zudem das jeweilige Modell.\n",
    "- Ergebnisliste `kmeans_results`: pro Eintrag `{k, inertia, silhouette_score, min_cluster_size, max_cluster_size, cluster_sizes, model}`.\n",
    "- Praxis: bestes k per **max. Silhouette** wählen und mit **Inertia-Knick** plausibilisieren; bei starken Imbalancen zusätzlich Clustergrößen prüfen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans_clusters(features, k_range, use_minibatch=True):\n",
    "    \"\"\"Evaluate K-Means clustering for different k values.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating K-Means for k in {list(k_range)}...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"  Testing k={k}...\")\n",
    "        \n",
    "        # Use MiniBatchKMeans for efficiency with large datasets\n",
    "        if use_minibatch:\n",
    "            kmeans = MiniBatchKMeans(\n",
    "                n_clusters=k,\n",
    "                random_state=RANDOM_STATE,\n",
    "                batch_size=100,\n",
    "                n_init=10\n",
    "            )\n",
    "        else:\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=k,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_init=10\n",
    "            )\n",
    "        \n",
    "        # Fit and predict\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "        \n",
    "        # Cluster sizes\n",
    "        cluster_sizes = Counter(cluster_labels)\n",
    "        min_cluster_size = min(cluster_sizes.values())\n",
    "        max_cluster_size = max(cluster_sizes.values())\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'max_cluster_size': max_cluster_size,\n",
    "            'cluster_sizes': dict(cluster_sizes),\n",
    "            'model': kmeans\n",
    "        })\n",
    "        \n",
    "        print(f\"    Inertia: {inertia:.2f}, Silhouette: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate K-Means clustering\n",
    "print(\"=\" * 80)\n",
    "print(\"K-MEANS CLUSTERING EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "kmeans_results = evaluate_kmeans_clusters(features_for_clustering, N_CLUSTERS_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee8ace",
   "metadata": {},
   "source": [
    "#### 7.2 K-Means Evaluation visualisieren & k bestimmen\n",
    "- Erzeugt ein 2×2-Dashboard:\n",
    "  - Elbow-Plot (Inertia vs. k)\n",
    "  - Silhouette-Plot (Silhouette vs. k)\n",
    "  - Clustergrößen (min/max vs. k)\n",
    "  - Kombinierte Metriken (1−normierte Inertia vs. normierte Silhouette)\n",
    "- Speichert die Grafik als `kmeans_evaluation.png` in `RESULTS_PATH` und zeigt sie an.\n",
    "- Ermittelt das optimale `k` über das Maximum des Silhouette-Scores (`np.argmax`), gibt `optimal_k` und den besten Silhouette-Wert aus.\n",
    "- Praxis-Hinweis: Prüfe, ob das Silhouette-Maximum stabil ist (keine Mini-Cluster, Domänensinn); Elbow dient als Plausibilisierung, nicht als alleiniges Kriterium.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c81cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot K-Means evaluation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('K-Means Clustering Evaluation', fontsize=16)\n",
    "\n",
    "# Extract data for plotting\n",
    "k_values = [r['k'] for r in kmeans_results]\n",
    "inertias = [r['inertia'] for r in kmeans_results]\n",
    "silhouette_scores = [r['silhouette_score'] for r in kmeans_results]\n",
    "min_cluster_sizes = [r['min_cluster_size'] for r in kmeans_results]\n",
    "max_cluster_sizes = [r['max_cluster_size'] for r in kmeans_results]\n",
    "\n",
    "# Elbow plot\n",
    "axes[0, 0].plot(k_values, inertias, 'bo-')\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 0].set_ylabel('Inertia')\n",
    "axes[0, 0].set_title('Elbow Method')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Silhouette score plot\n",
    "axes[0, 1].plot(k_values, silhouette_scores, 'ro-')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 1].set_ylabel('Silhouette Score')\n",
    "axes[0, 1].set_title('Silhouette Analysis')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Cluster size distribution\n",
    "axes[1, 0].plot(k_values, min_cluster_sizes, 'go-', label='Min cluster size')\n",
    "axes[1, 0].plot(k_values, max_cluster_sizes, 'mo-', label='Max cluster size')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 0].set_ylabel('Cluster Size')\n",
    "axes[1, 0].set_title('Cluster Size Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Combined metrics (normalized)\n",
    "norm_inertias = np.array(inertias) / max(inertias)\n",
    "norm_silhouettes = np.array(silhouette_scores) / max(silhouette_scores)\n",
    "axes[1, 1].plot(k_values, 1 - norm_inertias, 'b-', label='1 - Normalized Inertia')\n",
    "axes[1, 1].plot(k_values, norm_silhouettes, 'r-', label='Normalized Silhouette')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 1].set_ylabel('Normalized Score')\n",
    "axes[1, 1].set_title('Combined Metrics')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/kmeans_evaluation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "best_silhouette_idx = np.argmax(silhouette_scores)\n",
    "optimal_k = k_values[best_silhouette_idx]\n",
    "print(f\"\\nOptimal k based on silhouette score: {optimal_k}\")\n",
    "print(f\"Best silhouette score: {silhouette_scores[best_silhouette_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381f980",
   "metadata": {},
   "source": [
    "#### 7.3 Finales K-Means mit optimalem k & Clusteranalyse\n",
    "- Führt `MiniBatchKMeans` mit `k=optimal_k` aus (`batch_size=100`, `n_init=20`) und weist für alle Samples Cluster-Labels zu.\n",
    "- Ergänzt `image_files` um das Feld `cluster` (int) für die nachgelagerte Auswertung/Speicherung.\n",
    "- Erstellt eine Clusterzusammenfassung: Gesamtanzahl je Cluster sowie **Tenant-Verteilung pro Cluster** (Anzahl & Prozent).\n",
    "- Konsolenoutput: Für jedes Cluster → `#Bilder` und Rangfolge der Tenants (absteigend).\n",
    "- Praxis: Im nächsten Schritt Artefakte persistieren (z. B. `index_with_clusters.csv`, Modell-Params) und Beispielbilder je Cluster visualisieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final K-Means clustering with optimal k\n",
    "print(f\"\\nPerforming final K-Means clustering with k={optimal_k}...\")\n",
    "\n",
    "final_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=RANDOM_STATE,\n",
    "    batch_size=100,\n",
    "    n_init=20  # More initializations for final model\n",
    ")\n",
    "\n",
    "cluster_labels = final_kmeans.fit_predict(features_for_clustering)\n",
    "\n",
    "# Add cluster labels to image files\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['cluster'] = int(cluster_labels[i])\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(f\"\\nCluster Analysis:\")\n",
    "cluster_stats = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for img_file in image_files:\n",
    "    cluster = img_file['cluster']\n",
    "    tenant = img_file['tenant']\n",
    "    cluster_stats[cluster]['total'] += 1\n",
    "    cluster_stats[cluster][tenant] += 1\n",
    "\n",
    "for cluster_id in sorted(cluster_stats.keys()):\n",
    "    stats = cluster_stats[cluster_id]\n",
    "    total = stats['total']\n",
    "    print(f\"\\nCluster {cluster_id}: {total} images\")\n",
    "    \n",
    "    # Show tenant distribution in this cluster\n",
    "    tenant_counts = {k: v for k, v in stats.items() if k != 'total'}\n",
    "    for tenant, count in sorted(tenant_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {tenant}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fe6c4",
   "metadata": {},
   "source": [
    "#### 7.4 K-Means-Clusterbeispiele: Visualisierung von Beispielbildern\n",
    "- Kurzantwort: **Ja.** Die Funktion zeigt Beispielbilder aus den durch **MiniBatchKMeans** (finales k) zugewiesenen Clustern.\n",
    "- Grundlage: `cluster_labels = final_kmeans.fit_predict(...)` → wird in `image_files[i]['cluster']` gespeichert; die Visualisierung sampled daraus pro Cluster bis zu `n_examples` Bilder.\n",
    "- Art der Darstellung: **Bildgrid** (3 Spalten, mehrere Zeilen) pro Cluster; die Tafeln werden als `cluster_<cluster_id>_examples.png` gespeichert.\n",
    "- Hinweis: Es sind **zufällig gewählte Beispiele**, **keine** Clusterzentroid-Bilder. Für „repräsentativste“ Beispiele könnten Bilder mit **minimaler Distanz zum Zentroid** gezeigt werden (optional erweiterbar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_examples(image_files, cluster_id, n_examples=6):\n",
    "    \"\"\"Display example images from a specific cluster.\"\"\"\n",
    "    cluster_images = [img for img in image_files if img['cluster'] == cluster_id]\n",
    "    \n",
    "    if not cluster_images:\n",
    "        print(f\"No images found for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    # Randomly sample examples\n",
    "    examples = np.random.choice(cluster_images, min(n_examples, len(cluster_images)), replace=False)\n",
    "    \n",
    "    # Create subplot\n",
    "    cols = 3\n",
    "    rows = (len(examples) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(f'Cluster {cluster_id} Examples ({len(cluster_images)} total images)', fontsize=16)\n",
    "    \n",
    "    for i, img_info in enumerate(examples):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Load and display image\n",
    "        try:\n",
    "            img = Image.open(img_info['filepath'])\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(f\"{img_info['tenant']}_{img_info['sid']}\", fontsize=10)\n",
    "            axes[row, col].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error loading\\n{img_info['filename']}\", \n",
    "                               ha='center', va='center', transform=axes[row, col].transAxes)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(examples), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_PATH}/cluster_{cluster_id}_examples.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Display examples for each cluster\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    display_cluster_examples(image_files, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f56f7",
   "metadata": {},
   "source": [
    "### 8. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777aff56",
   "metadata": {},
   "source": [
    "#### 8.1 DBSCAN: Parameter-Grid, Heatmaps & Best-Selection\n",
    "- Verwendet **PCA-Features** für effizientere Dichte-Clustering-Läufe (niedrigere Dimension).\n",
    "- Testet ein **Parameter-Grid** aus `eps_values × min_samples_values` und führt für jede Kombination `DBSCAN.fit_predict(features_pca)` aus.\n",
    "- Ermittelt pro Run: **#Cluster** (ohne Noise), **Noise-Count/Noise-Ratio** sowie **Silhouette** (nur wenn >1 Cluster; Noise-Punkte ausgeschlossen).\n",
    "- Visualisiert die Ergebnisse als **3 Heatmaps**: Anzahl Cluster, Silhouette-Score, Noise-Ratio; Achsen: `eps` (x), `min_samples` (y).\n",
    "- Wählt „Besten“ auf zwei Arten:\n",
    "  - **Best by silhouette** (max. Silhouette)\n",
    "  - **Best balanced**: `silhouette * (1 - noise_ratio)` (Trade-off Qualität vs. Noise)\n",
    "  → setzt `best_dbscan` auf den „balanced“ Sieger (robuster in der Praxis).\n",
    "- Speichert die Übersicht als `dbscan_parameter_optimization.png` und zeigt sie an; gibt die besten Parameter und Metriken im Log aus.\n",
    "- Hinweise:\n",
    "  - Negative Silhouette-Werte werden für die Heatmap auf **0** gecappt (reine Visualisierung).\n",
    "  - Bei sehr hoher Noise-Ratio Parameterbereich erweitern oder **PCA-Komponenten** anpassen.\n",
    "  - Für datengetriebene Epsilon-Wahl zusätzlich **k-Distance-Plot** (z. B. k=MinPts) in Erwägung ziehen; **HDBSCAN** als Alternative prüfen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use PCA features for DBSCAN (better performance in lower dimensions)\n",
    "print(\"Applying DBSCAN on PCA-reduced features...\")\n",
    "\n",
    "# Create a grid of parameters to test\n",
    "eps_values = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.5]\n",
    "min_samples_values = [3, 5, 7, 10, 15, 20]\n",
    "\n",
    "print(\"\\nTesting parameter combinations:\")\n",
    "print(f\"eps values: {eps_values}\")\n",
    "print(f\"min_samples values: {min_samples_values}\")\n",
    "print(f\"Total combinations to test: {len(eps_values) * len(min_samples_values)}\")\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "# Test all combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        print(f\"\\nTesting DBSCAN with eps={eps}, min_samples={min_samples}...\")\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan.fit_predict(features_pca)\n",
    "        \n",
    "        # Count clusters and noise points\n",
    "        n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "        n_noise = list(dbscan_labels).count(-1)\n",
    "        \n",
    "        # Calculate silhouette score (only if we have more than 1 cluster)\n",
    "        if n_clusters > 1:\n",
    "            # Filter out noise points for silhouette calculation\n",
    "            mask = dbscan_labels != -1\n",
    "            if mask.sum() > 1:  # Need at least 2 points\n",
    "                silhouette_avg = silhouette_score(features_pca[mask], dbscan_labels[mask])\n",
    "            else:\n",
    "                silhouette_avg = -1\n",
    "        else:\n",
    "            silhouette_avg = -1\n",
    "        \n",
    "        result = {\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_ratio': n_noise / len(dbscan_labels),\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'labels': dbscan_labels\n",
    "        }\n",
    "        dbscan_results.append(result)\n",
    "        \n",
    "        print(f\"  Clusters: {n_clusters}\")\n",
    "        print(f\"  Noise points: {n_noise} ({result['noise_ratio']*100:.1f}%)\")\n",
    "        print(f\"  Silhouette: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Create results visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create a matrix of results\n",
    "eps_grid, min_samples_grid = np.meshgrid(eps_values, min_samples_values)\n",
    "n_clusters_grid = np.zeros_like(eps_grid, dtype=float)\n",
    "silhouette_grid = np.zeros_like(eps_grid, dtype=float)\n",
    "noise_ratio_grid = np.zeros_like(eps_grid, dtype=float)\n",
    "\n",
    "for result in dbscan_results:\n",
    "    i = min_samples_values.index(result['min_samples'])\n",
    "    j = eps_values.index(result['eps'])\n",
    "    n_clusters_grid[i, j] = result['n_clusters']\n",
    "    silhouette_grid[i, j] = max(result['silhouette_score'], 0)  # Replace negative scores with 0\n",
    "    noise_ratio_grid[i, j] = result['noise_ratio']\n",
    "\n",
    "# Plot number of clusters\n",
    "plt.subplot(221)\n",
    "plt.imshow(n_clusters_grid, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Number of Clusters')\n",
    "plt.ylabel('min_samples')\n",
    "plt.xlabel('eps')\n",
    "plt.title('Number of Clusters')\n",
    "plt.xticks(range(len(eps_values)), [f'{x:.1f}' for x in eps_values], rotation=45)\n",
    "plt.yticks(range(len(min_samples_values)), min_samples_values)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.subplot(222)\n",
    "plt.imshow(silhouette_grid, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Silhouette Score')\n",
    "plt.ylabel('min_samples')\n",
    "plt.xlabel('eps')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xticks(range(len(eps_values)), [f'{x:.1f}' for x in eps_values], rotation=45)\n",
    "plt.yticks(range(len(min_samples_values)), min_samples_values)\n",
    "\n",
    "# Plot noise ratio\n",
    "plt.subplot(223)\n",
    "plt.imshow(noise_ratio_grid, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Noise Ratio')\n",
    "plt.ylabel('min_samples')\n",
    "plt.xlabel('eps')\n",
    "plt.title('Noise Ratio')\n",
    "plt.xticks(range(len(eps_values)), [f'{x:.1f}' for x in eps_values], rotation=45)\n",
    "plt.yticks(range(len(min_samples_values)), min_samples_values)\n",
    "\n",
    "# Find best results\n",
    "valid_results = [r for r in dbscan_results if r['silhouette_score'] > 0]\n",
    "if valid_results:\n",
    "    # Sort by different metrics\n",
    "    best_silhouette = max(valid_results, key=lambda x: x['silhouette_score'])\n",
    "    balanced_score = max(valid_results, key=lambda x: x['silhouette_score'] * (1 - x['noise_ratio']))\n",
    "    \n",
    "    print(\"\\nBest results:\")\n",
    "    print(\"\\nBest by silhouette score:\")\n",
    "    print(f\"eps={best_silhouette['eps']}, min_samples={best_silhouette['min_samples']}\")\n",
    "    print(f\"Clusters: {best_silhouette['n_clusters']}\")\n",
    "    print(f\"Noise points: {best_silhouette['n_noise']} ({best_silhouette['noise_ratio']*100:.1f}%)\")\n",
    "    print(f\"Silhouette: {best_silhouette['silhouette_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nBest balanced (silhouette * (1 - noise_ratio)):\")\n",
    "    print(f\"eps={balanced_score['eps']}, min_samples={balanced_score['min_samples']}\")\n",
    "    print(f\"Clusters: {balanced_score['n_clusters']}\")\n",
    "    print(f\"Noise points: {balanced_score['n_noise']} ({balanced_score['noise_ratio']*100:.1f}%)\")\n",
    "    print(f\"Silhouette: {balanced_score['silhouette_score']:.4f}\")\n",
    "    \n",
    "    # Use balanced score as best result\n",
    "    best_dbscan = balanced_score\n",
    "else:\n",
    "    print(\"\\nNo valid DBSCAN results found. Consider adjusting parameters.\")\n",
    "    best_dbscan = dbscan_results[0]  # Use first result as fallback\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/dbscan_parameter_optimization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425b015",
   "metadata": {},
   "source": [
    "### 9. Visualisierung: K-Means & DBSCAN (Side-by-Side) mit t-SNE\n",
    "- Berechnet ein 2D-t-SNE auf den **PCA-Features** (`features_pca`) mit `random_state=RANDOM_STATE`, `perplexity=min(30, N-1)` und `max_iter=1000`.\n",
    "- Linkes Panel: Streudiagramm der **K-Means-Zuordnungen** (`cluster_labels`, cmap `tab10`, alpha 0.7).\n",
    "- Rechtes Panel: **DBSCAN-Zuordnungen** mit spezieller Farbgebung:\n",
    "  - Noise (`-1`) wird auf **grau** gemappt, übrige Cluster erhalten Farben aus `tab10`.\n",
    "  - Labels werden remappt, damit `-1 → 0` (grau) und Cluster fortlaufend folgen.\n",
    "- Legt Achsentitel/Überschriften, erzeugt **Colorbars** für beide Plots und zeigt **#Cluster** sowie **#Noise** (DBSCAN) an.\n",
    "- Speichert die Abbildung als `clustering_tsne_visualization.png` in `RESULTS_PATH` und zeigt sie an.\n",
    "- Hinweise:\n",
    "  - t-SNE ist **stochastisch** und lokal strukturtreu; das globale Layout ist nicht maßstabsgetreu (Interpretation vorsichtig).\n",
    "  - Für große N kann **UMAP** eine schnellere Alternative sein; `random_state` hält Visuals reproduzierbar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results with t-SNE (Fixed Colors & Legends)\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER VISUALIZATION WITH t-SNE (IMPROVED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Computing t-SNE embedding with stable parameters...\")\n",
    "N = len(features_pca)\n",
    "# Use stable t-SNE parameters for reproducible and better results\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    init='pca',  # PCA initialization for better stability\n",
    "    learning_rate='auto',  # Adaptive learning rate\n",
    "    perplexity=min(50, max(5, int(N * 0.01))),  # Dynamic perplexity based on sample size\n",
    "    max_iter=1500,  # More iterations for convergence\n",
    "    random_state=RANDOM_STATE,\n",
    "    metric='euclidean',\n",
    "    early_exaggeration=12.0\n",
    ")\n",
    "\n",
    "tsne_features = tsne.fit_transform(features_pca)\n",
    "print(f\"t-SNE completed with perplexity={tsne.perplexity}\")\n",
    "\n",
    "# Create visualization with fixed discrete colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# === K-MEANS PLOT WITH DISCRETE COLORS ===\n",
    "# Remap K-Means labels to 0..C-1 for consistent coloring\n",
    "unique_kmeans = sorted(set(cluster_labels))\n",
    "kmeans_label_map = {old: new for new, old in enumerate(unique_kmeans)}\n",
    "kmeans_colors = np.array([kmeans_label_map[label] for label in cluster_labels])\n",
    "\n",
    "# Create discrete colormap for K-Means\n",
    "n_kmeans_clusters = len(unique_kmeans)\n",
    "kmeans_cmap = matplotlib.colors.ListedColormap(plt.cm.tab10(np.linspace(0, 1, n_kmeans_clusters)))\n",
    "kmeans_norm = matplotlib.colors.BoundaryNorm(\n",
    "    boundaries=np.arange(-0.5, n_kmeans_clusters, 1), \n",
    "    ncolors=n_kmeans_clusters\n",
    ")\n",
    "\n",
    "scatter1 = axes[0].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=kmeans_colors,\n",
    "    cmap=kmeans_cmap,\n",
    "    norm=kmeans_norm,\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[0].set_title(f'K-Means Clustering (k={optimal_k})\\nt-SNE Visualization')\n",
    "axes[0].set_xlabel('t-SNE Component 1')\n",
    "axes[0].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "# Discrete colorbar for K-Means\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0], ticks=range(n_kmeans_clusters))\n",
    "cbar1.set_ticklabels([f'Cluster {unique_kmeans[i]}' for i in range(n_kmeans_clusters)])\n",
    "cbar1.set_label('K-Means Clusters')\n",
    "\n",
    "# === DBSCAN PLOT WITH NOISE IN GREY ===\n",
    "dbscan_colors = best_dbscan['labels'].copy()\n",
    "unique_dbscan = sorted(set(dbscan_colors))\n",
    "n_dbscan_clusters = len(unique_dbscan) - (1 if -1 in unique_dbscan else 0)\n",
    "\n",
    "# Create color mapping: noise (-1) -> 0 (grey), clusters -> 1,2,3...\n",
    "if -1 in unique_dbscan:\n",
    "    # Noise gets grey, clusters get tab10 colors\n",
    "    colors = ['#808080']  # Grey for noise\n",
    "    colors.extend(plt.cm.tab10(np.linspace(0, 1, n_dbscan_clusters)))\n",
    "    # Map -1 to 0, others to 1,2,3...\n",
    "    dbscan_label_map = {-1: 0}\n",
    "    cluster_idx = 1\n",
    "    for label in unique_dbscan:\n",
    "        if label != -1:\n",
    "            dbscan_label_map[label] = cluster_idx\n",
    "            cluster_idx += 1\n",
    "    total_colors = len(unique_dbscan)\n",
    "else:\n",
    "    # No noise, just use tab10 for all clusters\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_dbscan_clusters))\n",
    "    dbscan_label_map = {label: idx for idx, label in enumerate(unique_dbscan)}\n",
    "    total_colors = n_dbscan_clusters\n",
    "\n",
    "dbscan_colors_mapped = np.array([dbscan_label_map[label] for label in dbscan_colors])\n",
    "\n",
    "# Create discrete colormap for DBSCAN\n",
    "dbscan_cmap = matplotlib.colors.ListedColormap(colors)\n",
    "dbscan_norm = matplotlib.colors.BoundaryNorm(\n",
    "    boundaries=np.arange(-0.5, total_colors, 1), \n",
    "    ncolors=total_colors\n",
    ")\n",
    "\n",
    "scatter2 = axes[1].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=dbscan_colors_mapped,\n",
    "    cmap=dbscan_cmap,\n",
    "    norm=dbscan_norm,\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[1].set_title(f'DBSCAN Clustering (eps={best_dbscan[\"eps\"]})\\nt-SNE Visualization\\n{n_dbscan_clusters} clusters, {best_dbscan[\"n_noise\"]} noise points')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "# Discrete colorbar for DBSCAN\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[1], ticks=range(total_colors))\n",
    "if -1 in unique_dbscan:\n",
    "    tick_labels = ['Noise'] + [f'Cluster {i}' for i in range(n_dbscan_clusters)]\n",
    "else:\n",
    "    tick_labels = [f'Cluster {unique_dbscan[i]}' for i in range(n_dbscan_clusters)]\n",
    "cbar2.set_ticklabels(tick_labels)\n",
    "cbar2.set_label('DBSCAN Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/clustering_tsne_visualization_fixed.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization improvements:\")\n",
    "print(f\"- Used stable t-SNE parameters (init='pca', learning_rate='auto')\")\n",
    "print(f\"- Discrete colormaps with BoundaryNorm for clean cluster separation\")\n",
    "print(f\"- Proper legend labels for both K-Means and DBSCAN\")\n",
    "print(f\"- Noise points clearly marked in grey for DBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed364d",
   "metadata": {},
   "source": [
    "### 9.1 K-Nearest Neighbor Grafik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f769519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Distance Plot for DBSCAN Parameter Guidance\n",
    "print(\"=\" * 80)\n",
    "print(\"K-DISTANCE PLOT FOR DBSCAN EPS SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use the min_samples from best DBSCAN result\n",
    "k = best_dbscan['min_samples']\n",
    "print(f\"Computing {k}-distance plot for eps guidance...\")\n",
    "\n",
    "# Fit NearestNeighbors on PCA features\n",
    "neighbors = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "neighbors.fit(features_pca)\n",
    "\n",
    "# Get distances to k-th nearest neighbor for each point\n",
    "distances, indices = neighbors.kneighbors(features_pca)\n",
    "k_distances = distances[:, k-1]  # k-th distance (0-indexed)\n",
    "\n",
    "# Sort distances in descending order\n",
    "k_distances_sorted = np.sort(k_distances)[::-1]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(k_distances_sorted)), k_distances_sorted, 'b-', linewidth=1)\n",
    "plt.axhline(y=best_dbscan['eps'], color='red', linestyle='--', \n",
    "           label=f'Best eps = {best_dbscan[\"eps\"]}')\n",
    "\n",
    "# Add some reference lines for other tested eps values\n",
    "for eps in [0.3, 0.5, 0.7, 1.0]:\n",
    "    if eps != best_dbscan['eps']:\n",
    "        plt.axhline(y=eps, color='grey', linestyle=':', alpha=0.5, \n",
    "                   label=f'eps = {eps}')\n",
    "\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'{k}-th Nearest Neighbor Distance')\n",
    "plt.title(f'K-Distance Plot (k={k}) for DBSCAN eps Selection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the \"elbow\" region\n",
    "elbow_start = int(len(k_distances_sorted) * 0.05)\n",
    "elbow_end = int(len(k_distances_sorted) * 0.20)\n",
    "plt.axvspan(elbow_start, elbow_end, alpha=0.2, color='yellow', \n",
    "           label='Typical elbow region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/dbscan_k_distance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nK-Distance Plot Analysis:\")\n",
    "print(f\"- Red line shows selected eps = {best_dbscan['eps']}\")\n",
    "print(f\"- Look for 'elbow' in the curve to find optimal eps\")\n",
    "print(f\"- Steep increase indicates good density separation\")\n",
    "print(f\"- Current eps captures {np.sum(k_distances_sorted >= best_dbscan['eps'])} points as potential core points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6cb52",
   "metadata": {},
   "source": [
    "### 9.2 Interaktive Grafik zur explorativen Analyse der Cluster \n",
    "- Ohne Verwendung neuer Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interactive Image Browser for t-SNE Results\n",
    "## This version works with the standard inline matplotlib backend\n",
    "#\n",
    "#import numpy as np\n",
    "#from pathlib import Path\n",
    "#from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "#from IPython.display import display, clear_output\n",
    "#import ipywidgets as widgets\n",
    "#from ipywidgets import interact, interactive, fixed\n",
    "#\n",
    "## --- Prepare data from your notebook ---\n",
    "#X2d = tsne_features                  # shape (N,2)\n",
    "#paths = np.array([f['filepath'] for f in image_files])   # length N\n",
    "#labels_k = np.array(cluster_labels)  # K-Means labels\n",
    "#labels_db = np.array(best_dbscan['labels'])  # DBSCAN labels (may include -1 for noise)\n",
    "#\n",
    "#def create_cluster_browser(X, labels, paths, title, clustering_type=\"kmeans\"):\n",
    "#    \"\"\"Create an interactive cluster browser with dropdown selection\"\"\"\n",
    "#    \n",
    "#    # Create dropdown for point selection\n",
    "#    n_points = len(X)\n",
    "#    point_options = [(f\"Punkt {i} - Cluster {labels[i]}\", i) for i in range(n_points)]\n",
    "#    \n",
    "#    # Create widgets\n",
    "#    point_selector = widgets.Dropdown(\n",
    "#        options=point_options,\n",
    "#        value=0,\n",
    "#        description='Punkt wählen:',\n",
    "#        style={'description_width': 'initial'}\n",
    "#    )\n",
    "#    \n",
    "#    cluster_filter = widgets.Dropdown(\n",
    "#        options=[('Alle Cluster', -999)] + [(f'Cluster {c}', c) for c in np.unique(labels)],\n",
    "#        value=-999,\n",
    "#        description='Cluster Filter:',\n",
    "#        style={'description_width': 'initial'}\n",
    "#    )\n",
    "#    \n",
    "#    # Output areas\n",
    "#    plot_output = widgets.Output()\n",
    "#    image_output = widgets.Output()\n",
    "#    info_output = widgets.Output()\n",
    "#    \n",
    "#    def update_point_options(cluster_choice):\n",
    "#        \"\"\"Update available points based on cluster selection\"\"\"\n",
    "#        if cluster_choice == -999:  # All clusters\n",
    "#            filtered_indices = list(range(n_points))\n",
    "#        else:\n",
    "#            filtered_indices = np.where(labels == cluster_choice)[0].tolist()\n",
    "#        \n",
    "#        new_options = [(f\"Punkt {i} - Cluster {labels[i]}\", i) for i in filtered_indices]\n",
    "#        point_selector.options = new_options\n",
    "#        if new_options:\n",
    "#            point_selector.value = new_options[0][1]\n",
    "#    \n",
    "#    def show_plot_and_image(selected_point, cluster_choice):\n",
    "#        \"\"\"Display the scatter plot and selected image\"\"\"\n",
    "#        \n",
    "#        # Clear outputs\n",
    "#        plot_output.clear_output(wait=True)\n",
    "#        image_output.clear_output(wait=True)\n",
    "#        info_output.clear_output(wait=True)\n",
    "#        \n",
    "#        with plot_output:\n",
    "#            # Create scatter plot\n",
    "#            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#            \n",
    "#            # Prepare colors for clusters\n",
    "#            unique_labels = np.unique(labels)\n",
    "#            colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "#            color_map = {}\n",
    "#            \n",
    "#            for i, label in enumerate(unique_labels):\n",
    "#                if label == -1:  # Noise points for DBSCAN\n",
    "#                    color_map[label] = 'gray'\n",
    "#                else:\n",
    "#                    color_map[label] = colors[i]\n",
    "#            \n",
    "#            # Plot all points\n",
    "#            for label in unique_labels:\n",
    "#                mask = labels == label\n",
    "#                label_name = 'Noise' if label == -1 else f'Cluster {label}'\n",
    "#                ax.scatter(\n",
    "#                    X[mask, 0], X[mask, 1], \n",
    "#                    c=[color_map[label]], \n",
    "#                    label=label_name,\n",
    "#                    s=50, alpha=0.6\n",
    "#                )\n",
    "#            \n",
    "#            # Highlight selected point\n",
    "#            ax.scatter(\n",
    "#                X[selected_point, 0], X[selected_point, 1], \n",
    "#                c='red', s=200, marker='o', \n",
    "#                facecolors='none', edgecolors='red', linewidths=3,\n",
    "#                label='Ausgewählter Punkt'\n",
    "#            )\n",
    "#            \n",
    "#            ax.set_title(f'{title}\\nAusgewählter Punkt: {selected_point}, Cluster: {labels[selected_point]}')\n",
    "#            ax.set_xlabel('t-SNE Component 1')\n",
    "#            ax.set_ylabel('t-SNE Component 2')\n",
    "#            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#            ax.grid(True, alpha=0.3)\n",
    "#            \n",
    "#            plt.tight_layout()\n",
    "#            plt.show()\n",
    "#        \n",
    "#        with image_output:\n",
    "#            # Show the selected image\n",
    "#            try:\n",
    "#                img_path = paths[selected_point]\n",
    "#                img = Image.open(img_path)\n",
    "#                \n",
    "#                fig_img, ax_img = plt.subplots(figsize=(8, 6))\n",
    "#                ax_img.imshow(img)\n",
    "#                ax_img.set_title(f'{Path(img_path).name}\\n{clustering_type.upper()} Cluster: {labels[selected_point]}')\n",
    "#                ax_img.axis('off')\n",
    "#                plt.tight_layout()\n",
    "#                plt.show()\n",
    "#                \n",
    "#            except Exception as e:\n",
    "#                print(f\"Fehler beim Laden des Bildes: {str(e)}\")\n",
    "#        \n",
    "#        with info_output:\n",
    "#            # Show detailed information\n",
    "#            img_path = paths[selected_point]\n",
    "#            print(f\"📍 Punkt Index: {selected_point}\")\n",
    "#            print(f\"🏷️  Cluster: {labels[selected_point]}\")\n",
    "#            print(f\"📁 Dateiname: {Path(img_path).name}\")\n",
    "#            print(f\"📂 Vollständiger Pfad: {img_path}\")\n",
    "#            print(f\"📊 t-SNE Koordinaten: ({X[selected_point, 0]:.3f}, {X[selected_point, 1]:.3f})\")\n",
    "#    \n",
    "#    # Connect the cluster filter to point options update\n",
    "#    def on_cluster_change(change):\n",
    "#        if change['type'] == 'change' and change['name'] == 'value':\n",
    "#            update_point_options(change['new'])\n",
    "#    \n",
    "#    cluster_filter.observe(on_cluster_change)\n",
    "#    \n",
    "#    # Create interactive interface\n",
    "#    interactive_plot = interactive(\n",
    "#        show_plot_and_image,\n",
    "#        selected_point=point_selector,\n",
    "#        cluster_choice=fixed(cluster_filter.value)\n",
    "#    )\n",
    "#    \n",
    "#    # Layout\n",
    "#    controls = widgets.VBox([\n",
    "#        widgets.HTML(f\"<h3>{title}</h3>\"),\n",
    "#        widgets.HTML(\"<b>Wählen Sie einen Punkt aus, um das entsprechende Bild zu sehen:</b>\"),\n",
    "#        cluster_filter,\n",
    "#        point_selector\n",
    "#    ])\n",
    "#    \n",
    "#    plot_area = widgets.VBox([\n",
    "#        widgets.HTML(\"<b>Streudiagramm:</b>\"),\n",
    "#        plot_output\n",
    "#    ])\n",
    "#    \n",
    "#    image_area = widgets.VBox([\n",
    "#        widgets.HTML(\"<b>Bildvorschau:</b>\"),\n",
    "#        image_output,\n",
    "#        info_output\n",
    "#    ])\n",
    "#    \n",
    "#    # Display everything\n",
    "#    display(widgets.VBox([\n",
    "#        controls,\n",
    "#        widgets.HBox([plot_area, image_area])\n",
    "#    ]))\n",
    "#    \n",
    "#    # Show initial plot\n",
    "#    show_plot_and_image(0, -999)\n",
    "#    \n",
    "#    return interactive_plot\n",
    "#\n",
    "## Create browsers for both clustering methods\n",
    "#print(\"Erstelle interaktive Cluster-Browser...\")\n",
    "#print(\"📝 Anweisungen:\")\n",
    "#print(\"   1. Wählen Sie optional einen Cluster aus dem Filter\")\n",
    "#print(\"   2. Wählen Sie einen Punkt aus der Dropdown-Liste\")\n",
    "#print(\"   3. Das entsprechende Bild wird automatisch angezeigt\")\n",
    "#print()\n",
    "#\n",
    "## K-Means browser\n",
    "#print(\"🔍 K-Means Clustering Browser:\")\n",
    "#kmeans_browser = create_cluster_browser(\n",
    "#    X2d, labels_k, paths, \n",
    "#    title=f\"K-Means Clustering (k={optimal_k})\",\n",
    "#    clustering_type=\"kmeans\"\n",
    "#)\n",
    "#\n",
    "#print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "#\n",
    "## DBSCAN browser\n",
    "#print(\"🔍 DBSCAN Clustering Browser:\")\n",
    "#dbscan_browser = create_cluster_browser(\n",
    "#    X2d, labels_db, paths,\n",
    "#    title=f\"DBSCAN Clustering (eps={best_dbscan['eps']:.3f}, min_samples={best_dbscan.get('min_samples', 'N/A')})\",\n",
    "#    clustering_type=\"dbscan\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c838f",
   "metadata": {},
   "source": [
    "### 10. Cluster-Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8eea9e",
   "metadata": {},
   "source": [
    "#### 10.1 Speicher-Helfer definieren (Ordner/Bildkopie/Metadaten)\n",
    "- `create_cluster_directories(base_path, method_name, cluster_labels, dbscan_labels=None)`: legt unter `base_path/method_name/` pro Cluster einen Ordner an (`cluster_<id>`); bei DBSCAN landen Noise-Punkte in `noise/`. Rückgabe: Dict `{cluster_id: Path}`.\n",
    "- `copy_images_to_clusters(image_files, cluster_dirs, cluster_labels, method_name='kmeans', dbscan_labels=None)`: kopiert die Bilder in ihre Cluster-Ordner (überspringt bereits identische Dateien). Gibt Anzahl Kopien und Fehler zurück.\n",
    "- `save_cluster_metadata(cluster_dirs, image_files, cluster_labels, method_name='kmeans', dbscan_labels=None)`: schreibt je Cluster eine `cluster_metadata.json` mit `cluster_id`, `method`, `total_images`, `tenant_distribution` und `images[filename, tenant, sid, original_name]`.\n",
    "- Hinweise: Ordner-Anlage ist idempotent; `cluster_id` wird für JSON sauber in `int` konvertiert; DBSCAN-Noise (`-1`) wird separat in `noise/` geführt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_directories(base_path, method_name, cluster_labels, dbscan_labels=None):\n",
    "    \"\"\"Create directory structure for clustered images.\"\"\"\n",
    "    method_path = Path(base_path) / method_name\n",
    "    method_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        unique_clusters = sorted(set(cluster_labels))\n",
    "    else:  # dbscan\n",
    "        unique_clusters = sorted(set(dbscan_labels))\n",
    "        # Handle noise points (-1) separately\n",
    "        if -1 in unique_clusters:\n",
    "            unique_clusters = [c for c in unique_clusters if c != -1] + [-1]\n",
    "    \n",
    "    cluster_dirs = {}\n",
    "    for cluster_id in unique_clusters:\n",
    "        if cluster_id == -1:\n",
    "            cluster_dir = method_path / 'noise'\n",
    "        else:\n",
    "            cluster_dir = method_path / f'cluster_{cluster_id}'\n",
    "        cluster_dir.mkdir(exist_ok=True)\n",
    "        cluster_dirs[cluster_id] = cluster_dir\n",
    "    \n",
    "    return cluster_dirs\n",
    "\n",
    "def copy_images_to_clusters(image_files, cluster_dirs, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Copy images to their respective cluster directories.\"\"\"\n",
    "    print(f\"\\nCopying images to {method_name.upper()} cluster directories...\")\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        labels_to_use = cluster_labels\n",
    "    else:  # dbscan\n",
    "        labels_to_use = dbscan_labels\n",
    "    \n",
    "    copied_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        try:\n",
    "            cluster_id = labels_to_use[i]\n",
    "            source_path = Path(img_file['filepath'])\n",
    "            target_dir = cluster_dirs[cluster_id]\n",
    "            target_path = target_dir / source_path.name\n",
    "            \n",
    "            # Copy file if it doesn't exist or is different\n",
    "            if not target_path.exists() or target_path.stat().st_size != source_path.stat().st_size:\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                copied_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {img_file['filename']}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print(f\"Successfully copied {copied_count} images\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors: {error_count}\")\n",
    "    \n",
    "    return copied_count, error_count\n",
    "\n",
    "def save_cluster_metadata(cluster_dirs, image_files, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Save metadata for each cluster.\"\"\"\n",
    "    print(f\"\\nSaving {method_name.upper()} cluster metadata...\")\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        labels_to_use = cluster_labels\n",
    "    else:  # dbscan\n",
    "        labels_to_use = dbscan_labels\n",
    "    \n",
    "    for cluster_id, cluster_dir in cluster_dirs.items():\n",
    "        # Get images for this cluster\n",
    "        cluster_images = []\n",
    "        for i, img_file in enumerate(image_files):\n",
    "            if int(labels_to_use[i]) == cluster_id:  # Convert numpy int32 to Python int\n",
    "                cluster_images.append({\n",
    "                    'filename': img_file['filename'],\n",
    "                    'tenant': img_file['tenant'],\n",
    "                    'sid': img_file['sid'],\n",
    "                    'original_name': img_file['original_name']\n",
    "                })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        tenant_counts = Counter([img['tenant'] for img in cluster_images])\n",
    "        \n",
    "        metadata = {\n",
    "            'cluster_id': int(cluster_id) if isinstance(cluster_id, (np.integer, np.int32, np.int64)) else cluster_id,  # Ensure cluster_id is JSON serializable\n",
    "            'method': method_name,\n",
    "            'total_images': len(cluster_images),\n",
    "            'tenant_distribution': dict(tenant_counts),\n",
    "            'images': cluster_images\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = cluster_dir / 'cluster_metadata.json'\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Metadata saved for {len(cluster_dirs)} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92707b",
   "metadata": {},
   "source": [
    "#### 10.2 K-Means-Cluster speichern (Ausführung)\n",
    "- Legt `CLUSTERS_PATH=./results/clustered_images` an und erzeugt darin die K-Means-Ordnerstruktur (`kmeans/cluster_<id>`).\n",
    "- Gibt pro Cluster die Bildanzahl aus (Berechnung über `cluster_labels`).\n",
    "- Kopiert die Bilder in die jeweiligen Cluster-Ordner (überspringt bereits identische Dateien) und zählt Kopien/Fehler.\n",
    "- Schreibt je Cluster eine `cluster_metadata.json` (Größe, Tenant-Verteilung, Bildliste) in den jeweiligen Ordner.\n",
    "- Abschluss: Meldung mit Zielpfad `./results/clustered_images/kmeans`.\n",
    "- Hinweis: Vorgang ist idempotent; für große Datenmengen ggf. Symlinks statt Kopien erwägen, um Speicher zu sparen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9463811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-Means clustering results to timestamped directories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING K-MEANS CLUSTERS TO TIMESTAMPED DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Using timestamped cluster directory: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Create K-Means cluster directories\n",
    "kmeans_dirs = create_cluster_directories(CLUSTERS_PATH, 'kmeans', cluster_labels)\n",
    "\n",
    "print(f\"Created K-Means cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(kmeans_dirs.items()):\n",
    "    cluster_size = sum(1 for label in cluster_labels if label == cluster_id)\n",
    "    print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images to K-Means clusters\n",
    "kmeans_copied, kmeans_errors = copy_images_to_clusters(\n",
    "    image_files, kmeans_dirs, cluster_labels, 'kmeans'\n",
    ")\n",
    "\n",
    "# Save K-Means cluster metadata\n",
    "save_cluster_metadata(kmeans_dirs, image_files, cluster_labels, 'kmeans')\n",
    "\n",
    "print(f\"\\nK-Means clustering results saved to: {Path(CLUSTERS_PATH) / 'kmeans'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebeb588",
   "metadata": {},
   "source": [
    "#### 10.3 DBSCAN-Cluster speichern (Ausführung)\n",
    "- Ergänzt `image_files` um `dbscan_cluster` aus `best_dbscan['labels']` (inkl. Noise = `-1`).\n",
    "- Erstellt Ordnerstruktur unter `CLUSTERS_PATH/dbscan/` mit `cluster_<id>/` und `noise/` (für Label `-1`).\n",
    "- Gibt pro DBSCAN-Cluster die Bildanzahl aus (Noise separat ausgewiesen).\n",
    "- Kopiert alle Bilder in die entsprechenden **DBSCAN-Ordner** (überspringt identische Zieldateien).\n",
    "- Schreibt je Cluster eine **cluster_metadata.json** (Total, Tenant-Verteilung, Bildliste).\n",
    "- Abschluss: bestätigt Zielpfad `./results/clustered_images/dbscan`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DBSCAN clustering results to timestamped directories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING DBSCAN CLUSTERS TO TIMESTAMPED DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Using timestamped cluster directory: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Add DBSCAN cluster labels to image files for consistency\n",
    "dbscan_labels = best_dbscan['labels']\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['dbscan_cluster'] = int(dbscan_labels[i])\n",
    "\n",
    "# Create DBSCAN cluster directories\n",
    "dbscan_dirs = create_cluster_directories(CLUSTERS_PATH, 'dbscan', None, dbscan_labels)\n",
    "\n",
    "print(f\"Created DBSCAN cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(dbscan_dirs.items()):\n",
    "    cluster_size = sum(1 for label in dbscan_labels if label == cluster_id)\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  {cluster_dir.name} (noise): {cluster_size} images\")\n",
    "    else:\n",
    "        print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images to DBSCAN clusters\n",
    "dbscan_copied, dbscan_errors = copy_images_to_clusters(\n",
    "    image_files, dbscan_dirs, None, 'dbscan', dbscan_labels\n",
    ")\n",
    "\n",
    "# Save DBSCAN cluster metadata\n",
    "save_cluster_metadata(dbscan_dirs, image_files, None, 'dbscan', dbscan_labels)\n",
    "\n",
    "print(f\"\\nDBSCAN clustering results saved to: {Path(CLUSTERS_PATH) / 'dbscan'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afe61f",
   "metadata": {},
   "source": [
    "### 11. Zusammenfassungsreport erzeugen & Verzeichnisübersicht\n",
    "- Erstellt `cluster_summary` mit:\n",
    "  - `dataset_info`: Quellpfad, Anzahl verarbeiteter Bilder, Zeitstempel.\n",
    "  - `kmeans_clustering`: Methode, `optimal_k`, bester Silhouette-Wert, Kopier-/Fehlerzahlen, Clusterverteilung.\n",
    "  - `dbscan_clustering`: Methode, `eps`, `min_samples`, #Cluster, #Noise, Silhouette, Kopier-/Fehlerzahlen, Clusterverteilung.\n",
    "  - `directory_structure`: absolute Pfade zu `base_path`, `kmeans_path`, `dbscan_path`.\n",
    "- Speichert die Zusammenfassung als `clustering_summary.json` im `CLUSTERS_PATH`.\n",
    "- Konsolenübersicht:\n",
    "  - Gesamtanzahl verarbeiteter Bilder, Anzahl angelegter K-Means/DBSCAN-Cluster, Kopierstatistik.\n",
    "  - Baumansicht der angelegten Verzeichnisstruktur einschließlich Clustergrößen.\n",
    "  - Auflistung der relevanten Ausgabedateien (Summary, je Cluster `cluster_metadata.json`, Analyseplots unter `RESULTS_PATH`).\n",
    "- Hinweis (kleine Korrektur): In `dbscan_clustering` sollte `min_samples` aus `best_dbscan['min_samples']` übernommen werden, nicht aus einer evtl. veralteten Variablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report for timestamped clustered images\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMESTAMPED CLUSTER ORGANIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary with timestamp info\n",
    "cluster_summary = {\n",
    "    'run_info': {\n",
    "        'timestamp': TIMESTAMP,\n",
    "        'analysis_results_path': str(Path(RESULTS_PATH).resolve()),\n",
    "        'clustered_images_path': str(Path(CLUSTERS_PATH).resolve())\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'source_dataset': str(Path(DATASET_PATH).resolve()),\n",
    "        'total_images_processed': len(image_files),\n",
    "        'clustering_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    },\n",
    "    'kmeans_clustering': {\n",
    "        'method': 'K-Means (MiniBatch)',\n",
    "        'optimal_k': optimal_k,\n",
    "        'silhouette_score': float(silhouette_scores[best_silhouette_idx]),\n",
    "        'images_copied': kmeans_copied,\n",
    "        'copy_errors': kmeans_errors,\n",
    "        'cluster_distribution': {\n",
    "            str(cluster_id): int(count) for cluster_id, count in Counter(cluster_labels).items()\n",
    "        }\n",
    "    },\n",
    "    'dbscan_clustering': {\n",
    "        'method': 'DBSCAN',\n",
    "        'eps': best_dbscan['eps'],\n",
    "        'min_samples': best_dbscan['min_samples'],\n",
    "        'n_clusters': best_dbscan['n_clusters'],\n",
    "        'n_noise': best_dbscan['n_noise'],\n",
    "        'silhouette_score': float(best_dbscan['silhouette_score']),\n",
    "        'images_copied': dbscan_copied,\n",
    "        'copy_errors': dbscan_errors,\n",
    "        'cluster_distribution': {\n",
    "            str(cluster_id): int(count) for cluster_id, count in Counter(dbscan_labels).items()\n",
    "        }\n",
    "    },\n",
    "    'directory_structure': {\n",
    "        'base_path': str(Path(CLUSTERS_PATH).resolve()),\n",
    "        'kmeans_path': str(Path(CLUSTERS_PATH, 'kmeans').resolve()),\n",
    "        'dbscan_path': str(Path(CLUSTERS_PATH, 'dbscan').resolve())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save cluster summary in both locations\n",
    "summary_file_clusters = Path(CLUSTERS_PATH) / 'clustering_summary.json'\n",
    "summary_file_analysis = Path(RESULTS_PATH) / 'clustering_summary.json'\n",
    "\n",
    "with open(summary_file_clusters, 'w') as f:\n",
    "    json.dump(cluster_summary, f, indent=2)\n",
    "with open(summary_file_analysis, 'w') as f:\n",
    "    json.dump(cluster_summary, f, indent=2)\n",
    "\n",
    "print(f\"Timestamped cluster organization completed successfully!\")\n",
    "print(f\"\\nRun timestamp: {TIMESTAMP}\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"- Total images processed: {len(image_files)}\")\n",
    "print(f\"- K-Means clusters created: {len(kmeans_dirs)}\")\n",
    "print(f\"- DBSCAN clusters created: {len(dbscan_dirs)}\")\n",
    "print(f\"- Images copied (K-Means): {kmeans_copied}\")\n",
    "print(f\"- Images copied (DBSCAN): {dbscan_copied}\")\n",
    "\n",
    "print(f\"\\nTimestamped directory structure created:\")\n",
    "print(f\"results/\")\n",
    "print(f\"├── clustering_analysis_{TIMESTAMP}/\")\n",
    "print(f\"│   ├── run_metadata.json\")\n",
    "print(f\"│   ├── clustering_report.json\")\n",
    "print(f\"│   ├── clustering_summary.json\")\n",
    "print(f\"│   └── *.png (analysis plots)\")\n",
    "print(f\"└── clustered_images_{TIMESTAMP}/\")\n",
    "print(f\"    ├── clustering_summary.json\")\n",
    "print(f\"    ├── kmeans/\")\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    cluster_size = sum(1 for label in cluster_labels if label == cluster_id)\n",
    "    print(f\"    │   ├── cluster_{cluster_id}/ ({cluster_size} images)\")\n",
    "print(f\"    └── dbscan/\")\n",
    "for cluster_id in sorted(set(dbscan_labels)):\n",
    "    cluster_size = sum(1 for label in dbscan_labels if label == cluster_id)\n",
    "    if cluster_id == -1:\n",
    "        print(f\"        ├── noise/ ({cluster_size} images)\")\n",
    "    else:\n",
    "        print(f\"        ├── cluster_{cluster_id}/ ({cluster_size} images)\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"- Cluster summary: {summary_file_clusters}\")\n",
    "print(f\"- Analysis summary: {summary_file_analysis}\")\n",
    "print(f\"- Individual cluster metadata: cluster_metadata.json in each cluster directory\")\n",
    "print(f\"- Analysis results: {RESULTS_PATH}/\")\n",
    "print(f\"- Clustered images: {CLUSTERS_PATH}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde73ad7",
   "metadata": {},
   "source": [
    "### 12. Tenant-Cluster-Analyse (Heatmap & Detailstatistik)\n",
    "- Baut eine Tenant×Cluster-Matrix auf (zeilenweise normalisiert) und berechnet **Prozentanteile je Tenant** pro Cluster.\n",
    "- Visualisiert die Verteilung als **Heatmap** mit Beschriftung (%.1f) und speichert sie als `tenant_cluster_heatmap.png` unter `RESULTS_PATH`.\n",
    "- Gibt zusätzlich eine **Detailtabelle** in der Konsole aus: für jeden Tenant Gesamtzahl + Prozent/Anzahl pro Cluster.\n",
    "- Interpretation: Zeilen summieren zu ~100 % → zeigt **Präferenz/Schieflage** eines Tenants über Cluster; bei kleinen Tenants Vorsicht (Varianz).\n",
    "- Optional: Ergänze eine zweite Heatmap auf **Rohzählungen** oder einen **Stacked Bar Chart** je Tenant für absolute Vergleichbarkeit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tenant distribution across clusters\n",
    "print(\"=\" * 80)\n",
    "print(\"TENANT DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create tenant-cluster matrix\n",
    "tenant_cluster_matrix = defaultdict(lambda: defaultdict(int))\n",
    "total_by_tenant = defaultdict(int)\n",
    "\n",
    "for img_file in image_files:\n",
    "    tenant = img_file['tenant']\n",
    "    cluster = img_file['cluster']\n",
    "    tenant_cluster_matrix[tenant][cluster] += 1\n",
    "    total_by_tenant[tenant] += 1\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "tenants = sorted(total_by_tenant.keys())\n",
    "clusters = sorted(set(cluster_labels))\n",
    "\n",
    "matrix_data = []\n",
    "for tenant in tenants:\n",
    "    row = []\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        row.append(percentage)\n",
    "    matrix_data.append(row)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = np.array(matrix_data)\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    xticklabels=[f'Cluster {c}' for c in clusters],\n",
    "    yticklabels=tenants,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Percentage of Tenant Images'}\n",
    ")\n",
    "plt.title('Tenant Distribution Across Clusters (%)')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Tenants')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/tenant_cluster_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Tenant-Cluster Distribution:\")\n",
    "for tenant in tenants:\n",
    "    print(f\"\\n{tenant} ({total_by_tenant[tenant]} images):\")\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        if count > 0:\n",
    "            print(f\"  Cluster {cluster}: {count} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965da81",
   "metadata": {},
   "source": [
    "### 13. Clustering-Report (JSON) erstellen & Kernergebnisse ausgeben\n",
    "- Erstellt `clustering_report.json` unter `RESULTS_PATH` mit:\n",
    "  - `dataset_info`: `total_images`, `feature_dimension`, `pca_components`, `pca_explained_variance`, `tenant_distribution`.\n",
    "  - `kmeans_results`: `optimal_k`, bester `silhouette_score`, `cluster_sizes` und Liste der `evaluation_results` (k, inertia, silhouette).\n",
    "  - `dbscan_results`: `best_eps`, `n_clusters`, `n_noise`, `silhouette_score`.\n",
    "  - `tenant_cluster_analysis`: je Tenant `total_images` und `cluster_distribution`.\n",
    "- Persistiert den Report (indent=2) und gibt eine kompakte **Zusammenfassung** in der Konsole aus:\n",
    "  - Optimales k (K-Means), bester Silhouette-Score.\n",
    "  - DBSCAN-Ergebnis: #Cluster, #Noise.\n",
    "  - PCA: erklärte Gesamtvarianz (%).\n",
    "  - Clustergrößen (K-Means) inkl. Prozentanteile.\n",
    "- Hinweise:\n",
    "  - Falls `image_files` nach dem Valid-Filter reduziert wurde, kann `tenant_distribution` (aus der frühen Ladephase) abweichen. Optional neu berechnen auf Basis der gefilterten `image_files`.\n",
    "  - Optional `best_dbscan['min_samples']` zusätzlich in `dbscan_results` aufnehmen, um die Parameterwahl vollständig zu dokumentieren.\n",
    "  - Visualisierungen (`*.png`) und Clusterbeispiele (`cluster_*_examples.png`) liegen parallel im `RESULTS_PATH`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive clustering report with timestamp info\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMESTAMPED CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compile results with timestamp information\n",
    "clustering_report = {\n",
    "    'run_info': {\n",
    "        'timestamp': TIMESTAMP,\n",
    "        'run_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'analysis_results_path': str(Path(RESULTS_PATH).resolve()),\n",
    "        'clustered_images_path': str(Path(CLUSTERS_PATH).resolve())\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_images': int(len(image_files)),\n",
    "        'feature_dimension': int(FEATURE_DIM),\n",
    "        'pca_components': int(PCA_COMPONENTS),\n",
    "        'pca_explained_variance': float(pca.explained_variance_ratio_.sum()),\n",
    "        'tenant_distribution': {k: int(v) for k, v in tenant_distribution.items()}\n",
    "    },\n",
    "    'kmeans_results': {\n",
    "        'optimal_k': int(optimal_k),\n",
    "        'silhouette_score': float(silhouette_scores[best_silhouette_idx]),\n",
    "        'cluster_sizes': {int(k): int(v) for k, v in Counter(cluster_labels).items()},\n",
    "        'evaluation_results': [\n",
    "            {\n",
    "                'k': int(r['k']),\n",
    "                'inertia': float(r['inertia']),\n",
    "                'silhouette_score': float(r['silhouette_score'])\n",
    "            } for r in kmeans_results\n",
    "        ]\n",
    "    },\n",
    "    'dbscan_results': {\n",
    "        'best_eps': float(best_dbscan['eps']),\n",
    "        'min_samples': int(best_dbscan['min_samples']),\n",
    "        'n_clusters': int(best_dbscan['n_clusters']),\n",
    "        'n_noise': int(best_dbscan['n_noise']),\n",
    "        'silhouette_score': float(best_dbscan['silhouette_score'])\n",
    "    },\n",
    "    'tenant_cluster_analysis': {\n",
    "        tenant: {\n",
    "            'total_images': int(total_by_tenant[tenant]),\n",
    "            'cluster_distribution': {int(k): int(v) for k, v in tenant_cluster_matrix[tenant].items()}\n",
    "        } for tenant in tenants\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to timestamped directory\n",
    "with open(f\"{RESULTS_PATH}/clustering_report.json\", 'w') as f:\n",
    "    json.dump(clustering_report, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Analysis completed successfully!\")\n",
    "print(f\"\\nRun Information:\")\n",
    "print(f\"- Timestamp: {TIMESTAMP}\")\n",
    "print(f\"- Analysis results saved to: {RESULTS_PATH}\")\n",
    "print(f\"- Clustered images saved to: {CLUSTERS_PATH}\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"- Optimal number of clusters (K-Means): {optimal_k}\")\n",
    "print(f\"- Best silhouette score: {silhouette_scores[best_silhouette_idx]:.4f}\")\n",
    "print(f\"- DBSCAN found {best_dbscan['n_clusters']} clusters with {best_dbscan['n_noise']} noise points\")\n",
    "print(f\"- PCA captured {pca.explained_variance_ratio_.sum():.1%} of the variance\")\n",
    "\n",
    "print(f\"\\nCluster sizes (K-Means):\")\n",
    "cluster_sizes = Counter(cluster_labels)\n",
    "for cluster_id, size in sorted(cluster_sizes.items()):\n",
    "    percentage = (size / len(cluster_labels)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {size} images ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTimestamped results saved to: {RESULTS_PATH}\")\n",
    "print(f\"- Run metadata: run_metadata.json\")\n",
    "print(f\"- Clustering report: clustering_report.json\")\n",
    "print(f\"- Clustering summary: clustering_summary.json\")\n",
    "print(f\"- Visualizations: *.png files\")\n",
    "print(f\"- Cluster examples: cluster_*_examples.png\")\n",
    "\n",
    "print(f\"\\nTo compare different runs, check the various timestamped directories in ./results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21146e25",
   "metadata": {},
   "source": [
    "### 14. Interpretation & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation and next steps for timestamped analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"INTERPRETATION AND NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Analysis run timestamp: {TIMESTAMP}\")\n",
    "print(f\"Results saved in timestamped directories for comparison with future runs.\")\n",
    "print()\n",
    "\n",
    "print(\"Based on the cluster analysis, consider the following interpretations:\")\n",
    "print()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"Cluster Characteristics Analysis:\")\n",
    "for cluster_id in sorted(cluster_sizes.keys()):\n",
    "    cluster_images = [img for img in image_files if img['cluster'] == cluster_id]\n",
    "    cluster_tenants = [img['tenant'] for img in cluster_images]\n",
    "    tenant_counts = Counter(cluster_tenants)\n",
    "    dominant_tenant = tenant_counts.most_common(1)[0]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_images)} images):\")\n",
    "    print(f\"  Dominant tenant: {dominant_tenant[0]} ({dominant_tenant[1]}/{len(cluster_images)} images, {dominant_tenant[1]/len(cluster_images)*100:.1f}%)\")\n",
    "    \n",
    "    if len(tenant_counts) == 1:\n",
    "        print(f\"  Interpretation: Tenant-specific characteristics ({dominant_tenant[0]})\")\n",
    "    elif dominant_tenant[1] / len(cluster_images) > 0.7:\n",
    "        print(f\"  Interpretation: Primarily {dominant_tenant[0]} with some similarities to other tenants\")\n",
    "    else:\n",
    "        print(f\"  Interpretation: Mixed tenant cluster - likely represents common infrastructure features\")\n",
    "\n",
    "print(\"\\nPotential classification tasks based on clustering:\")\n",
    "print(\"1. Tenant Classification: Classify images by transportation company\")\n",
    "print(\"2. Infrastructure Type: Distinguish between different track types (e.g., embedded vs. ballasted)\")\n",
    "print(\"3. Environmental Conditions: Classify by lighting, weather, or time of day\")\n",
    "print(\"4. Urban vs. Rural: Distinguish between city and countryside rail infrastructure\")\n",
    "print()\n",
    "print(\"Recommended next steps:\")\n",
    "print(\"1. Manually inspect cluster examples to understand what visual features drive the clustering\")\n",
    "print(\"2. Create labels based on identified patterns (e.g., 'gravel_track', 'asphalt_embedded', etc.)\")\n",
    "print(\"3. Use these labels to train supervised classifiers\")\n",
    "print(\"4. Consider data augmentation strategies for underrepresented clusters\")\n",
    "print(\"5. Evaluate whether clustering captures meaningful domain-specific patterns\")\n",
    "print(\"6. Compare results across different runs using the timestamped directories\")\n",
    "print(\"7. Track how clustering results change with different parameters or datasets\")\n",
    "\n",
    "print(f\"\\nFor future comparisons:\")\n",
    "print(f\"- This run's results: {RESULTS_PATH}\")\n",
    "print(f\"- Clustered images: {CLUSTERS_PATH}\")\n",
    "print(f\"- Look for patterns across different timestamps to validate clustering stability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
