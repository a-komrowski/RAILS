{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db966c2",
   "metadata": {},
   "source": [
    "# Explorative Cluster-Analyse der Gleisbilder\n",
    "\n",
    "Diese Notebook führt eine unüberwachte Cluster-Analyse der Center-Bilder durch, um ohne Vorwissen Strukturen und Muster in den Daten zu entdecken.\n",
    "\n",
    "**Ziele:**\n",
    "- Feature-Extraktion mit vortrainiertem CNN (ResNet50)\n",
    "- K-Means und DBSCAN Clustering\n",
    "- Evaluierung der Cluster-Qualität\n",
    "- Visualisierung und Interpretation der Ergebnisse\n",
    "\n",
    "**Erwartete Erkenntnisse:**\n",
    "- Unterschiedliche Untergrundtypen (Schotter vs. Asphalt)\n",
    "- Verschiedene Beleuchtungsbedingungen\n",
    "- Stadtspezifische Unterschiede\n",
    "- Anomalien oder seltene Situationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import shutil\n",
    "# Force TensorFlow to use CPU only - MUST BE SET BEFORE IMPORTING TENSORFLOW\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# Deep Learning Libraries - Import AFTER setting environment variables\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Running on CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_PATH = \"./datasets/clustering_sample_5000\"  # Path to sampled dataset\n",
    "RESULTS_PATH = \"./results/clustering_analysis\"      # Output path for results\n",
    "BATCH_SIZE = 32                                     # Batch size for feature extraction\n",
    "IMG_SIZE = (224, 224)                              # Input size for ResNet50\n",
    "FEATURE_DIM = 2048                                 # ResNet50 feature dimension\n",
    "N_CLUSTERS_RANGE = range(3, 16)                    # Range of k values to test\n",
    "PCA_COMPONENTS = 50                                # PCA components for dimensionality reduction\n",
    "RANDOM_STATE = 42                                  # Random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Results path: {RESULTS_PATH}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Dataset path {DATASET_PATH} does not exist!\")\n",
    "    print(\"Available dataset directories:\")\n",
    "    datasets_dir = Path(\"./datasets\")\n",
    "    if datasets_dir.exists():\n",
    "        for subdir in datasets_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                img_count = len([f for f in subdir.iterdir() if f.suffix == '.png'])\n",
    "                print(f\"  {subdir.name}: {img_count} images\")\n",
    "else:\n",
    "    img_count = len([f for f in Path(DATASET_PATH).iterdir() if f.suffix == '.png'])\n",
    "    print(f\"Found {img_count} images in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c45d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename: str):\n",
    "    \"\"\"Parse filename to extract tenant, SID, and original filename.\"\"\"\n",
    "    if not filename.endswith('_C.png'):\n",
    "        return None, None, None\n",
    "    \n",
    "    name_without_ext = filename[:-6]  # Remove '_C.png'\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        tenant = parts[0]\n",
    "        sid = parts[1]\n",
    "        original_filename = '_'.join(parts[2:])\n",
    "        return tenant, sid, original_filename\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "def load_dataset_info(dataset_path: str):\n",
    "    \"\"\"Load and analyze dataset information.\"\"\"\n",
    "    image_files = []\n",
    "    tenant_distribution = defaultdict(int)\n",
    "    \n",
    "    for file_path in Path(dataset_path).glob('*_C.png'):\n",
    "        filename = file_path.name\n",
    "        tenant, sid, original_name = parse_filename(filename)\n",
    "        \n",
    "        if tenant:\n",
    "            image_files.append({\n",
    "                'filepath': str(file_path),\n",
    "                'filename': filename,\n",
    "                'tenant': tenant,\n",
    "                'sid': sid,\n",
    "                'original_name': original_name\n",
    "            })\n",
    "            tenant_distribution[tenant] += 1\n",
    "    \n",
    "    return image_files, dict(tenant_distribution)\n",
    "\n",
    "# Load dataset information\n",
    "print(\"Loading dataset information...\")\n",
    "image_files, tenant_distribution = load_dataset_info(DATASET_PATH)\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_files)}\")\n",
    "print(f\"Number of tenants: {len(tenant_distribution)}\")\n",
    "print(\"\\nTenant distribution:\")\n",
    "for tenant, count in sorted(tenant_distribution.items()):\n",
    "    percentage = (count / len(image_files)) * 100\n",
    "    print(f\"  {tenant}: {count} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path: str, target_size: tuple = IMG_SIZE):\n",
    "    \"\"\"Load and preprocess image for ResNet50.\"\"\"\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=target_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_feature_extractor():\n",
    "    \"\"\"Create ResNet50 feature extractor (without top classification layer).\"\"\"\n",
    "    print(\"Loading ResNet50 model...\")\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "        pooling='avg'  # Global average pooling\n",
    "    )\n",
    "    \n",
    "    # The model output will be (batch_size, 2048) features\n",
    "    print(f\"Feature extractor output shape: {base_model.output_shape}\")\n",
    "    return base_model\n",
    "\n",
    "def extract_features_batch(model, image_paths: list, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Extract features from images in batches.\"\"\"\n",
    "    features = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    print(f\"Extracting features from {len(image_paths)} images...\")\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        batch_valid_paths = []\n",
    "        \n",
    "        # Load batch images\n",
    "        for img_path in batch_paths:\n",
    "            img_array = load_and_preprocess_image(img_path)\n",
    "            if img_array is not None:\n",
    "                batch_images.append(img_array[0])  # Remove batch dimension\n",
    "                batch_valid_paths.append(img_path)\n",
    "        \n",
    "        if batch_images:\n",
    "            # Convert to numpy array and predict\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_features = model.predict(batch_images, verbose=0)\n",
    "            \n",
    "            features.extend(batch_features)\n",
    "            valid_paths.extend(batch_valid_paths)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Processed {min(i + batch_size, len(image_paths))}/{len(image_paths)} images\")\n",
    "    \n",
    "    return np.array(features), valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "# Extract features using ResNet50\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = create_feature_extractor()\n",
    "\n",
    "# Extract image paths\n",
    "image_paths = [item['filepath'] for item in image_files]\n",
    "\n",
    "# Extract features\n",
    "start_time = time.time()\n",
    "features, valid_paths = extract_features_batch(feature_extractor, image_paths)\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFeature extraction completed in {extraction_time:.2f} seconds\")\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "print(f\"Valid images: {len(valid_paths)}/{len(image_paths)}\")\n",
    "\n",
    "# Update image_files to only include valid images\n",
    "valid_image_files = []\n",
    "for img_file in image_files:\n",
    "    if img_file['filepath'] in valid_paths:\n",
    "        valid_image_files.append(img_file)\n",
    "\n",
    "image_files = valid_image_files\n",
    "print(f\"Updated image files: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "print(\"Normalizing features...\")\n",
    "features_normalized = normalize(features, norm='l2', axis=1)\n",
    "\n",
    "print(f\"Original feature statistics:\")\n",
    "print(f\"  Mean: {features.mean():.4f}\")\n",
    "print(f\"  Std: {features.std():.4f}\")\n",
    "print(f\"  Min: {features.min():.4f}\")\n",
    "print(f\"  Max: {features.max():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized feature statistics:\")\n",
    "print(f\"  Mean: {features_normalized.mean():.4f}\")\n",
    "print(f\"  Std: {features_normalized.std():.4f}\")\n",
    "print(f\"  L2 norm (first sample): {np.linalg.norm(features_normalized[0]):.4f}\")\n",
    "\n",
    "# Optional: Apply PCA for dimensionality reduction\n",
    "print(f\"\\nApplying PCA to reduce dimensions from {FEATURE_DIM} to {PCA_COMPONENTS}...\")\n",
    "pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
    "features_pca = pca.fit_transform(features_normalized)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"PCA features shape: {features_pca.shape}\")\n",
    "\n",
    "# We'll use both normalized full features and PCA features for comparison\n",
    "features_for_clustering = features_normalized  # Start with full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans_clusters(features, k_range, use_minibatch=True):\n",
    "    \"\"\"Evaluate K-Means clustering for different k values.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating K-Means for k in {list(k_range)}...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"  Testing k={k}...\")\n",
    "        \n",
    "        # Use MiniBatchKMeans for efficiency with large datasets\n",
    "        if use_minibatch:\n",
    "            kmeans = MiniBatchKMeans(\n",
    "                n_clusters=k,\n",
    "                random_state=RANDOM_STATE,\n",
    "                batch_size=100,\n",
    "                n_init=10\n",
    "            )\n",
    "        else:\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=k,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_init=10\n",
    "            )\n",
    "        \n",
    "        # Fit and predict\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "        \n",
    "        # Cluster sizes\n",
    "        cluster_sizes = Counter(cluster_labels)\n",
    "        min_cluster_size = min(cluster_sizes.values())\n",
    "        max_cluster_size = max(cluster_sizes.values())\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'min_cluster_size': min_cluster_size,\n",
    "            'max_cluster_size': max_cluster_size,\n",
    "            'cluster_sizes': dict(cluster_sizes),\n",
    "            'model': kmeans\n",
    "        })\n",
    "        \n",
    "        print(f\"    Inertia: {inertia:.2f}, Silhouette: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate K-Means clustering\n",
    "print(\"=\" * 80)\n",
    "print(\"K-MEANS CLUSTERING EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "kmeans_results = evaluate_kmeans_clusters(features_for_clustering, N_CLUSTERS_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c81cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot K-Means evaluation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('K-Means Clustering Evaluation', fontsize=16)\n",
    "\n",
    "# Extract data for plotting\n",
    "k_values = [r['k'] for r in kmeans_results]\n",
    "inertias = [r['inertia'] for r in kmeans_results]\n",
    "silhouette_scores = [r['silhouette_score'] for r in kmeans_results]\n",
    "min_cluster_sizes = [r['min_cluster_size'] for r in kmeans_results]\n",
    "max_cluster_sizes = [r['max_cluster_size'] for r in kmeans_results]\n",
    "\n",
    "# Elbow plot\n",
    "axes[0, 0].plot(k_values, inertias, 'bo-')\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 0].set_ylabel('Inertia')\n",
    "axes[0, 0].set_title('Elbow Method')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Silhouette score plot\n",
    "axes[0, 1].plot(k_values, silhouette_scores, 'ro-')\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 1].set_ylabel('Silhouette Score')\n",
    "axes[0, 1].set_title('Silhouette Analysis')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Cluster size distribution\n",
    "axes[1, 0].plot(k_values, min_cluster_sizes, 'go-', label='Min cluster size')\n",
    "axes[1, 0].plot(k_values, max_cluster_sizes, 'mo-', label='Max cluster size')\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 0].set_ylabel('Cluster Size')\n",
    "axes[1, 0].set_title('Cluster Size Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Combined metrics (normalized)\n",
    "norm_inertias = np.array(inertias) / max(inertias)\n",
    "norm_silhouettes = np.array(silhouette_scores) / max(silhouette_scores)\n",
    "axes[1, 1].plot(k_values, 1 - norm_inertias, 'b-', label='1 - Normalized Inertia')\n",
    "axes[1, 1].plot(k_values, norm_silhouettes, 'r-', label='Normalized Silhouette')\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 1].set_ylabel('Normalized Score')\n",
    "axes[1, 1].set_title('Combined Metrics')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/kmeans_evaluation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "best_silhouette_idx = np.argmax(silhouette_scores)\n",
    "optimal_k = k_values[best_silhouette_idx]\n",
    "print(f\"\\nOptimal k based on silhouette score: {optimal_k}\")\n",
    "print(f\"Best silhouette score: {silhouette_scores[best_silhouette_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final K-Means clustering with optimal k\n",
    "print(f\"\\nPerforming final K-Means clustering with k={optimal_k}...\")\n",
    "\n",
    "final_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=RANDOM_STATE,\n",
    "    batch_size=100,\n",
    "    n_init=20  # More initializations for final model\n",
    ")\n",
    "\n",
    "cluster_labels = final_kmeans.fit_predict(features_for_clustering)\n",
    "\n",
    "# Add cluster labels to image files\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['cluster'] = int(cluster_labels[i])\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(f\"\\nCluster Analysis:\")\n",
    "cluster_stats = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for img_file in image_files:\n",
    "    cluster = img_file['cluster']\n",
    "    tenant = img_file['tenant']\n",
    "    cluster_stats[cluster]['total'] += 1\n",
    "    cluster_stats[cluster][tenant] += 1\n",
    "\n",
    "for cluster_id in sorted(cluster_stats.keys()):\n",
    "    stats = cluster_stats[cluster_id]\n",
    "    total = stats['total']\n",
    "    print(f\"\\nCluster {cluster_id}: {total} images\")\n",
    "    \n",
    "    # Show tenant distribution in this cluster\n",
    "    tenant_counts = {k: v for k, v in stats.items() if k != 'total'}\n",
    "    for tenant, count in sorted(tenant_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {tenant}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_examples(image_files, cluster_id, n_examples=6):\n",
    "    \"\"\"Display example images from a specific cluster.\"\"\"\n",
    "    cluster_images = [img for img in image_files if img['cluster'] == cluster_id]\n",
    "    \n",
    "    if not cluster_images:\n",
    "        print(f\"No images found for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    # Randomly sample examples\n",
    "    examples = np.random.choice(cluster_images, min(n_examples, len(cluster_images)), replace=False)\n",
    "    \n",
    "    # Create subplot\n",
    "    cols = 3\n",
    "    rows = (len(examples) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(f'Cluster {cluster_id} Examples ({len(cluster_images)} total images)', fontsize=16)\n",
    "    \n",
    "    for i, img_info in enumerate(examples):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Load and display image\n",
    "        try:\n",
    "            img = Image.open(img_info['filepath'])\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(f\"{img_info['tenant']}_{img_info['sid']}\", fontsize=10)\n",
    "            axes[row, col].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error loading\\n{img_info['filename']}\", \n",
    "                               ha='center', va='center', transform=axes[row, col].transAxes)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(examples), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_PATH}/cluster_{cluster_id}_examples.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Display examples for each cluster\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    display_cluster_examples(image_files, cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use PCA features for DBSCAN (better performance in lower dimensions)\n",
    "print(\"Applying DBSCAN on PCA-reduced features...\")\n",
    "\n",
    "# Try different eps values\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]\n",
    "min_samples = 5\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    print(f\"\\nTesting DBSCAN with eps={eps}, min_samples={min_samples}...\")\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(features_pca)\n",
    "    \n",
    "    # Count clusters and noise points\n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    # Calculate silhouette score (only if we have more than 1 cluster)\n",
    "    if n_clusters > 1:\n",
    "        # Filter out noise points for silhouette calculation\n",
    "        mask = dbscan_labels != -1\n",
    "        if mask.sum() > 1:  # Need at least 2 points\n",
    "            silhouette_avg = silhouette_score(features_pca[mask], dbscan_labels[mask])\n",
    "        else:\n",
    "            silhouette_avg = -1\n",
    "    else:\n",
    "        silhouette_avg = -1\n",
    "    \n",
    "    dbscan_results.append({\n",
    "        'eps': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette_score': silhouette_avg,\n",
    "        'labels': dbscan_labels\n",
    "    })\n",
    "    \n",
    "    print(f\"  Clusters: {n_clusters}, Noise points: {n_noise}, Silhouette: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Select best DBSCAN result\n",
    "valid_dbscan = [r for r in dbscan_results if r['silhouette_score'] > 0]\n",
    "if valid_dbscan:\n",
    "    best_dbscan = max(valid_dbscan, key=lambda x: x['silhouette_score'])\n",
    "    print(f\"\\nBest DBSCAN: eps={best_dbscan['eps']}, {best_dbscan['n_clusters']} clusters, {best_dbscan['n_noise']} noise points\")\n",
    "else:\n",
    "    print(\"\\nNo valid DBSCAN results found. Consider adjusting parameters.\")\n",
    "    best_dbscan = dbscan_results[0]  # Use first result as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results with t-SNE\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER VISUALIZATION WITH t-SNE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Computing t-SNE embedding...\")\n",
    "# Use PCA features for t-SNE (faster and often better results)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    perplexity=min(30, len(features_pca) - 1),\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "tsne_features = tsne.fit_transform(features_pca)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot K-Means results\n",
    "scatter1 = axes[0].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='tab10',\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[0].set_title(f'K-Means Clustering (k={optimal_k})\\nt-SNE Visualization')\n",
    "axes[0].set_xlabel('t-SNE Component 1')\n",
    "axes[0].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# Plot DBSCAN results\n",
    "scatter2 = axes[1].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=best_dbscan['labels'],\n",
    "    cmap='tab10',\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[1].set_title(f'DBSCAN Clustering (eps={best_dbscan[\"eps\"]})\\nt-SNE Visualization')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/clustering_tsne_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_directories(base_path, method_name, cluster_labels, dbscan_labels=None):\n",
    "    \"\"\"Create directory structure for clustered images.\"\"\"\n",
    "    method_path = Path(base_path) / method_name\n",
    "    method_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        unique_clusters = sorted(set(cluster_labels))\n",
    "    else:  # dbscan\n",
    "        unique_clusters = sorted(set(dbscan_labels))\n",
    "        # Handle noise points (-1) separately\n",
    "        if -1 in unique_clusters:\n",
    "            unique_clusters = [c for c in unique_clusters if c != -1] + [-1]\n",
    "    \n",
    "    cluster_dirs = {}\n",
    "    for cluster_id in unique_clusters:\n",
    "        if cluster_id == -1:\n",
    "            cluster_dir = method_path / 'noise'\n",
    "        else:\n",
    "            cluster_dir = method_path / f'cluster_{cluster_id}'\n",
    "        cluster_dir.mkdir(exist_ok=True)\n",
    "        cluster_dirs[cluster_id] = cluster_dir\n",
    "    \n",
    "    return cluster_dirs\n",
    "\n",
    "def copy_images_to_clusters(image_files, cluster_dirs, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Copy images to their respective cluster directories.\"\"\"\n",
    "    print(f\"\\nCopying images to {method_name.upper()} cluster directories...\")\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        labels_to_use = cluster_labels\n",
    "    else:  # dbscan\n",
    "        labels_to_use = dbscan_labels\n",
    "    \n",
    "    copied_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        try:\n",
    "            cluster_id = labels_to_use[i]\n",
    "            source_path = Path(img_file['filepath'])\n",
    "            target_dir = cluster_dirs[cluster_id]\n",
    "            target_path = target_dir / source_path.name\n",
    "            \n",
    "            # Copy file if it doesn't exist or is different\n",
    "            if not target_path.exists() or target_path.stat().st_size != source_path.stat().st_size:\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                copied_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {img_file['filename']}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print(f\"Successfully copied {copied_count} images\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors: {error_count}\")\n",
    "    \n",
    "    return copied_count, error_count\n",
    "\n",
    "def save_cluster_metadata(cluster_dirs, image_files, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Save metadata for each cluster.\"\"\"\n",
    "    print(f\"\\nSaving {method_name.upper()} cluster metadata...\")\n",
    "    \n",
    "    if method_name == 'kmeans':\n",
    "        labels_to_use = cluster_labels\n",
    "    else:  # dbscan\n",
    "        labels_to_use = dbscan_labels\n",
    "    \n",
    "    for cluster_id, cluster_dir in cluster_dirs.items():\n",
    "        # Get images for this cluster\n",
    "        cluster_images = []\n",
    "        for i, img_file in enumerate(image_files):\n",
    "            if int(labels_to_use[i]) == cluster_id:  # Convert numpy int32 to Python int\n",
    "                cluster_images.append({\n",
    "                    'filename': img_file['filename'],\n",
    "                    'tenant': img_file['tenant'],\n",
    "                    'sid': img_file['sid'],\n",
    "                    'original_name': img_file['original_name']\n",
    "                })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        tenant_counts = Counter([img['tenant'] for img in cluster_images])\n",
    "        \n",
    "        metadata = {\n",
    "            'cluster_id': int(cluster_id) if isinstance(cluster_id, (np.integer, np.int32, np.int64)) else cluster_id,  # Ensure cluster_id is JSON serializable\n",
    "            'method': method_name,\n",
    "            'total_images': len(cluster_images),\n",
    "            'tenant_distribution': dict(tenant_counts),\n",
    "            'images': cluster_images\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_file = cluster_dir / 'cluster_metadata.json'\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Metadata saved for {len(cluster_dirs)} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9463811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-Means clustering results to directories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING K-MEANS CLUSTERS TO DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create base directory for clustered images\n",
    "CLUSTERS_PATH = \"./results/clustered_images\"\n",
    "Path(CLUSTERS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create K-Means cluster directories\n",
    "kmeans_dirs = create_cluster_directories(CLUSTERS_PATH, 'kmeans', cluster_labels)\n",
    "\n",
    "print(f\"Created K-Means cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(kmeans_dirs.items()):\n",
    "    cluster_size = sum(1 for label in cluster_labels if label == cluster_id)\n",
    "    print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images to K-Means clusters\n",
    "kmeans_copied, kmeans_errors = copy_images_to_clusters(\n",
    "    image_files, kmeans_dirs, cluster_labels, 'kmeans'\n",
    ")\n",
    "\n",
    "# Save K-Means cluster metadata\n",
    "save_cluster_metadata(kmeans_dirs, image_files, cluster_labels, 'kmeans')\n",
    "\n",
    "print(f\"\\nK-Means clustering results saved to: {Path(CLUSTERS_PATH) / 'kmeans'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DBSCAN clustering results to directories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING DBSCAN CLUSTERS TO DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add DBSCAN cluster labels to image files for consistency\n",
    "dbscan_labels = best_dbscan['labels']\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['dbscan_cluster'] = int(dbscan_labels[i])\n",
    "\n",
    "# Create DBSCAN cluster directories\n",
    "dbscan_dirs = create_cluster_directories(CLUSTERS_PATH, 'dbscan', None, dbscan_labels)\n",
    "\n",
    "print(f\"Created DBSCAN cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(dbscan_dirs.items()):\n",
    "    cluster_size = sum(1 for label in dbscan_labels if label == cluster_id)\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  {cluster_dir.name} (noise): {cluster_size} images\")\n",
    "    else:\n",
    "        print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images to DBSCAN clusters\n",
    "dbscan_copied, dbscan_errors = copy_images_to_clusters(\n",
    "    image_files, dbscan_dirs, None, 'dbscan', dbscan_labels\n",
    ")\n",
    "\n",
    "# Save DBSCAN cluster metadata\n",
    "save_cluster_metadata(dbscan_dirs, image_files, None, 'dbscan', dbscan_labels)\n",
    "\n",
    "print(f\"\\nDBSCAN clustering results saved to: {Path(CLUSTERS_PATH) / 'dbscan'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report for clustered images\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER ORGANIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary\n",
    "cluster_summary = {\n",
    "    'dataset_info': {\n",
    "        'source_dataset': str(Path(DATASET_PATH).resolve()),\n",
    "        'total_images_processed': len(image_files),\n",
    "        'clustering_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    },\n",
    "    'kmeans_clustering': {\n",
    "        'method': 'K-Means (MiniBatch)',\n",
    "        'optimal_k': optimal_k,\n",
    "        'silhouette_score': float(silhouette_scores[best_silhouette_idx]),\n",
    "        'images_copied': kmeans_copied,\n",
    "        'copy_errors': kmeans_errors,\n",
    "        'cluster_distribution': {\n",
    "            str(cluster_id): int(count) for cluster_id, count in Counter(cluster_labels).items()\n",
    "        }\n",
    "    },\n",
    "    'dbscan_clustering': {\n",
    "        'method': 'DBSCAN',\n",
    "        'eps': best_dbscan['eps'],\n",
    "        'min_samples': min_samples,\n",
    "        'n_clusters': best_dbscan['n_clusters'],\n",
    "        'n_noise': best_dbscan['n_noise'],\n",
    "        'silhouette_score': float(best_dbscan['silhouette_score']),\n",
    "        'images_copied': dbscan_copied,\n",
    "        'copy_errors': dbscan_errors,\n",
    "        'cluster_distribution': {\n",
    "            str(cluster_id): int(count) for cluster_id, count in Counter(dbscan_labels).items()\n",
    "        }\n",
    "    },\n",
    "    'directory_structure': {\n",
    "        'base_path': str(Path(CLUSTERS_PATH).resolve()),\n",
    "        'kmeans_path': str(Path(CLUSTERS_PATH, 'kmeans').resolve()),\n",
    "        'dbscan_path': str(Path(CLUSTERS_PATH, 'dbscan').resolve())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save cluster summary\n",
    "summary_file = Path(CLUSTERS_PATH) / 'clustering_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(cluster_summary, f, indent=2)\n",
    "\n",
    "print(f\"Cluster organization completed successfully!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Total images processed: {len(image_files)}\")\n",
    "print(f\"- K-Means clusters created: {len(kmeans_dirs)}\")\n",
    "print(f\"- DBSCAN clusters created: {len(dbscan_dirs)}\")\n",
    "print(f\"- Images copied (K-Means): {kmeans_copied}\")\n",
    "print(f\"- Images copied (DBSCAN): {dbscan_copied}\")\n",
    "\n",
    "print(f\"\\nDirectory structure created:\")\n",
    "print(f\"{CLUSTERS_PATH}/\")\n",
    "print(f\"├── kmeans/\")\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    cluster_size = sum(1 for label in cluster_labels if label == cluster_id)\n",
    "    print(f\"│   ├── cluster_{cluster_id}/ ({cluster_size} images)\")\n",
    "print(f\"└── dbscan/\")\n",
    "for cluster_id in sorted(set(dbscan_labels)):\n",
    "    cluster_size = sum(1 for label in dbscan_labels if label == cluster_id)\n",
    "    if cluster_id == -1:\n",
    "        print(f\"    ├── noise/ ({cluster_size} images)\")\n",
    "    else:\n",
    "        print(f\"    ├── cluster_{cluster_id}/ ({cluster_size} images)\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"- Cluster summary: {summary_file}\")\n",
    "print(f\"- Individual cluster metadata: cluster_metadata.json in each cluster directory\")\n",
    "print(f\"- Analysis results: {RESULTS_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tenant distribution across clusters\n",
    "print(\"=\" * 80)\n",
    "print(\"TENANT DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create tenant-cluster matrix\n",
    "tenant_cluster_matrix = defaultdict(lambda: defaultdict(int))\n",
    "total_by_tenant = defaultdict(int)\n",
    "\n",
    "for img_file in image_files:\n",
    "    tenant = img_file['tenant']\n",
    "    cluster = img_file['cluster']\n",
    "    tenant_cluster_matrix[tenant][cluster] += 1\n",
    "    total_by_tenant[tenant] += 1\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "tenants = sorted(total_by_tenant.keys())\n",
    "clusters = sorted(set(cluster_labels))\n",
    "\n",
    "matrix_data = []\n",
    "for tenant in tenants:\n",
    "    row = []\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        row.append(percentage)\n",
    "    matrix_data.append(row)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = np.array(matrix_data)\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    xticklabels=[f'Cluster {c}' for c in clusters],\n",
    "    yticklabels=tenants,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Percentage of Tenant Images'}\n",
    ")\n",
    "plt.title('Tenant Distribution Across Clusters (%)')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Tenants')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/tenant_cluster_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Tenant-Cluster Distribution:\")\n",
    "for tenant in tenants:\n",
    "    print(f\"\\n{tenant} ({total_by_tenant[tenant]} images):\")\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        if count > 0:\n",
    "            print(f\"  Cluster {cluster}: {count} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive clustering report\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compile results\n",
    "clustering_report = {\n",
    "    'dataset_info': {\n",
    "        'total_images': len(image_files),\n",
    "        'feature_dimension': FEATURE_DIM,\n",
    "        'pca_components': PCA_COMPONENTS,\n",
    "        'pca_explained_variance': float(pca.explained_variance_ratio_.sum()),\n",
    "        'tenant_distribution': tenant_distribution\n",
    "    },\n",
    "    'kmeans_results': {\n",
    "        'optimal_k': optimal_k,\n",
    "        'silhouette_score': float(silhouette_scores[best_silhouette_idx]),\n",
    "        'cluster_sizes': dict(Counter(cluster_labels)),\n",
    "        'evaluation_results': [\n",
    "            {\n",
    "                'k': r['k'],\n",
    "                'inertia': float(r['inertia']),\n",
    "                'silhouette_score': float(r['silhouette_score'])\n",
    "            } for r in kmeans_results\n",
    "        ]\n",
    "    },\n",
    "    'dbscan_results': {\n",
    "        'best_eps': best_dbscan['eps'],\n",
    "        'n_clusters': best_dbscan['n_clusters'],\n",
    "        'n_noise': best_dbscan['n_noise'],\n",
    "        'silhouette_score': float(best_dbscan['silhouette_score'])\n",
    "    },\n",
    "    'tenant_cluster_analysis': {\n",
    "        tenant: {\n",
    "            'total_images': total_by_tenant[tenant],\n",
    "            'cluster_distribution': dict(tenant_cluster_matrix[tenant])\n",
    "        } for tenant in tenants\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(f\"{RESULTS_PATH}/clustering_report.json\", 'w') as f:\n",
    "    json.dump(clustering_report, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Analysis completed successfully!\")\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"- Optimal number of clusters (K-Means): {optimal_k}\")\n",
    "print(f\"- Best silhouette score: {silhouette_scores[best_silhouette_idx]:.4f}\")\n",
    "print(f\"- DBSCAN found {best_dbscan['n_clusters']} clusters with {best_dbscan['n_noise']} noise points\")\n",
    "print(f\"- PCA captured {pca.explained_variance_ratio_.sum():.1%} of the variance\")\n",
    "\n",
    "print(f\"\\nCluster sizes (K-Means):\")\n",
    "cluster_sizes = Counter(cluster_labels)\n",
    "for cluster_id, size in sorted(cluster_sizes.items()):\n",
    "    percentage = (size / len(cluster_labels)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {size} images ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_PATH}\")\n",
    "print(f\"- Clustering report: clustering_report.json\")\n",
    "print(f\"- Visualizations: *.png files\")\n",
    "print(f\"- Cluster examples: cluster_*_examples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation and next steps\n",
    "print(\"=\" * 80)\n",
    "print(\"INTERPRETATION AND NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Based on the cluster analysis, consider the following interpretations:\")\n",
    "print()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"Cluster Characteristics Analysis:\")\n",
    "for cluster_id in sorted(cluster_sizes.keys()):\n",
    "    cluster_images = [img for img in image_files if img['cluster'] == cluster_id]\n",
    "    cluster_tenants = [img['tenant'] for img in cluster_images]\n",
    "    tenant_counts = Counter(cluster_tenants)\n",
    "    dominant_tenant = tenant_counts.most_common(1)[0]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_images)} images):\")\n",
    "    print(f\"  Dominant tenant: {dominant_tenant[0]} ({dominant_tenant[1]}/{len(cluster_images)} images, {dominant_tenant[1]/len(cluster_images)*100:.1f}%)\")\n",
    "    \n",
    "    if len(tenant_counts) == 1:\n",
    "        print(f\"  Interpretation: Tenant-specific characteristics ({dominant_tenant[0]})\")\n",
    "    elif dominant_tenant[1] / len(cluster_images) > 0.7:\n",
    "        print(f\"  Interpretation: Primarily {dominant_tenant[0]} with some similarities to other tenants\")\n",
    "    else:\n",
    "        print(f\"  Interpretation: Mixed tenant cluster - likely represents common infrastructure features\")\n",
    "\n",
    "print(\"\\nPotential classification tasks based on clustering:\")\n",
    "print(\"1. Tenant Classification: Classify images by transportation company\")\n",
    "print(\"2. Infrastructure Type: Distinguish between different track types (e.g., embedded vs. ballasted)\")\n",
    "print(\"3. Environmental Conditions: Classify by lighting, weather, or time of day\")\n",
    "print(\"4. Urban vs. Rural: Distinguish between city and countryside rail infrastructure\")\n",
    "print()\n",
    "print(\"Recommended next steps:\")\n",
    "print(\"1. Manually inspect cluster examples to understand what visual features drive the clustering\")\n",
    "print(\"2. Create labels based on identified patterns (e.g., 'gravel_track', 'asphalt_embedded', etc.)\")\n",
    "print(\"3. Use these labels to train supervised classifiers\")\n",
    "print(\"4. Consider data augmentation strategies for underrepresented clusters\")\n",
    "print(\"5. Evaluate whether clustering captures meaningful domain-specific patterns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
