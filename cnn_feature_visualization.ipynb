{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8bf0ea",
   "metadata": {},
   "source": [
    "# CNN Feature Map Visualization\n",
    "\n",
    "This notebook provides deep learning feature visualization for understanding how VGG16 and ResNet50 process railway images.\n",
    "\n",
    "## Analysis Goals:\n",
    "- **Feature Map Visualization**: Display CNN layer activations from VGG16 and ResNet50\n",
    "- **Layer Comparison**: Compare early, middle, and late layer responses\n",
    "- **Model Comparison**: Understand differences between VGG16 and ResNet50 feature extraction\n",
    "- **Single Image Focus**: Deep analysis of one image at a time\n",
    "\n",
    "## Key Questions:\n",
    "1. What visual features do different CNN layers detect in railway images?\n",
    "2. How do VGG16 and ResNet50 differ in their feature extraction?\n",
    "3. Which layers are most important for railway infrastructure recognition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a08df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = (224, 224)  # Standard input size for VGG16 and ResNet50\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ============================================================================\n",
    "# MODIFY THIS PATH TO ANALYZE DIFFERENT IMAGES\n",
    "# ============================================================================\n",
    "EXAMPLE_IMAGE_PATH = \"./datasets/clustering_sample_1000/ava_102_0000003090_C.png\"\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Target image size: {IMG_SIZE}\")\n",
    "print(f\"Example image path: {EXAMPLE_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path: str, target_size: tuple = IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Load and resize image for CNN analysis.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        target_size: Target size for resizing (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image: Resized image ready for CNN processing\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    \n",
    "    # Load and resize image\n",
    "    img_pil = Image.open(image_path)\n",
    "    img_resized = img_pil.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba25ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMapVisualizer:\n",
    "    \"\"\"Visualize feature maps from different CNN layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.layer_models = {}\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load pre-trained VGG16 and ResNet50 models.\"\"\"\n",
    "        print(\"Loading pre-trained models...\")\n",
    "        \n",
    "        # Load VGG16\n",
    "        self.models['vgg16'] = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        # Load ResNet50\n",
    "        self.models['resnet50'] = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        \n",
    "        print(\"Models loaded successfully!\")\n",
    "        print(f\"VGG16 layers: {len(self.models['vgg16'].layers)}\")\n",
    "        print(f\"ResNet50 layers: {len(self.models['resnet50'].layers)}\")\n",
    "        \n",
    "    def get_layer_outputs(self, model_name, layer_names):\n",
    "        \"\"\"\n",
    "        Create models that output specific layer activations.\n",
    "        \n",
    "        Args:\n",
    "            model_name: 'vgg16' or 'resnet50'\n",
    "            layer_names: List of layer names to extract\n",
    "        \"\"\"\n",
    "        base_model = self.models[model_name]\n",
    "        \n",
    "        # Get the outputs of specified layers\n",
    "        layer_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "        \n",
    "        # Create a model that returns these outputs\n",
    "        activation_model = Model(inputs=base_model.input, outputs=layer_outputs)\n",
    "        \n",
    "        self.layer_models[f\"{model_name}_layers\"] = activation_model\n",
    "        return activation_model\n",
    "    \n",
    "    def visualize_feature_maps(self, img, model_name, layer_names, max_filters=16):\n",
    "        \"\"\"\n",
    "        Visualize feature maps from specific layers.\n",
    "        \n",
    "        Args:\n",
    "            img: Input PIL image\n",
    "            model_name: 'vgg16' or 'resnet50'\n",
    "            layer_names: List of layer names to visualize\n",
    "            max_filters: Maximum number of filters to show per layer\n",
    "        \"\"\"\n",
    "        # Get preprocessing function\n",
    "        preprocess_func = vgg16_preprocess if model_name == 'vgg16' else resnet50_preprocess\n",
    "        \n",
    "        # Prepare image for model\n",
    "        img_array = image.img_to_array(img.resize((224, 224)))\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_func(img_array)\n",
    "        \n",
    "        # Get layer outputs\n",
    "        activation_model = self.get_layer_outputs(model_name, layer_names)\n",
    "        activations = activation_model.predict(img_array, verbose=0)\n",
    "        \n",
    "        # Visualize each layer\n",
    "        for layer_idx, (layer_name, activation) in enumerate(zip(layer_names, activations)):\n",
    "            self._plot_feature_maps(activation, layer_name, max_filters)\n",
    "            \n",
    "        return activations\n",
    "    \n",
    "    def _plot_feature_maps(self, activation, layer_name, max_filters=16):\n",
    "        \"\"\"Plot feature maps for a single layer.\"\"\"\n",
    "        # Get dimensions\n",
    "        n_features = activation.shape[-1]\n",
    "        size = activation.shape[1]\n",
    "        \n",
    "        # Limit number of filters to display\n",
    "        n_cols = 8\n",
    "        n_filters = min(max_filters, n_features)\n",
    "        n_rows = (n_filters + n_cols - 1) // n_cols\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i in range(n_filters):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            \n",
    "            # Display feature map\n",
    "            feature_map = activation[0, :, :, i]\n",
    "            axes[row, col].imshow(feature_map, cmap='viridis')\n",
    "            axes[row, col].set_title(f'Filter {i}', fontsize=8)\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_filters, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'{layer_name} - Feature Maps (Shape: {activation.shape})', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_model_features(self, img):\n",
    "        \"\"\"\n",
    "        Compare features extracted by VGG16 and ResNet50.\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPARING CNN MODEL FEATURES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # VGG16 key layers\n",
    "        vgg16_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "        \n",
    "        # ResNet50 key layers\n",
    "        resnet50_layers = ['conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out']\n",
    "        \n",
    "        print(\"\\nVisualizing VGG16 Feature Maps...\")\n",
    "        vgg16_activations = self.visualize_feature_maps(img, 'vgg16', vgg16_layers)\n",
    "        \n",
    "        print(\"\\nVisualizing ResNet50 Feature Maps...\")\n",
    "        resnet50_activations = self.visualize_feature_maps(img, 'resnet50', resnet50_layers)\n",
    "        \n",
    "        return vgg16_activations, resnet50_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image path to analyze\n",
    "image_path = EXAMPLE_IMAGE_PATH\n",
    "\n",
    "print(f\"Analyzing image: {image_path}\")\n",
    "print(f\"Image exists: {os.path.exists(image_path)}\")\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(\"\\nImage not found! Please check the path or choose from available images:\")\n",
    "    # Try to list available images\n",
    "    sample_dir = Path(\"./datasets/clustering_sample_1000\")\n",
    "    if sample_dir.exists():\n",
    "        sample_images = list(sample_dir.glob(\"*_C.png\"))[:10]  # Show first 10\n",
    "        print(\"\\nAvailable sample images:\")\n",
    "        for img in sample_images:\n",
    "            print(f\"  {img}\")\n",
    "        if len(sample_images) > 0:\n",
    "            image_path = str(sample_images[0])\n",
    "            print(f\"\\nUsing first available image: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image for CNN feature visualization\n",
    "print(\"Loading image for CNN feature visualization...\")\n",
    "\n",
    "# Load image\n",
    "img = load_image(image_path)\n",
    "\n",
    "print(f\"Image loaded and resized to: {img.size}\")\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Input Image: {Path(image_path).name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load CNN models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INITIALIZING CNN MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "visualizer = FeatureMapVisualizer()\n",
    "visualizer.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN feature maps\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CNN FEATURE MAP VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare model features\n",
    "vgg16_activations, resnet50_activations = visualizer.compare_model_features(img)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nAnalyzed image: {Path(image_path).name}\")\n",
    "print(f\"VGG16 feature maps generated from {len(vgg16_activations)} layers\")\n",
    "print(f\"ResNet50 feature maps generated from {len(resnet50_activations)} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed6518",
   "metadata": {},
   "source": [
    "## Understanding CNN Feature Maps\n",
    "\n",
    "### What Each Layer Detects:\n",
    "\n",
    "**Early Layers (conv1, block1):**\n",
    "- **Basic edges**: Horizontal, vertical, diagonal lines\n",
    "- **Simple textures**: Basic patterns and gradients\n",
    "- **Color transitions**: Boundaries between different regions\n",
    "- **Railway context**: Rail edges, tie boundaries, ballast textures\n",
    "\n",
    "**Middle Layers (conv3, block3):**\n",
    "- **Complex patterns**: Repeated structures like railway ties\n",
    "- **Shape recognition**: Geometric forms, curves, intersections\n",
    "- **Texture combinations**: Complex surface patterns\n",
    "- **Railway context**: Track geometry, infrastructure patterns\n",
    "\n",
    "**Deep Layers (conv5, block5):**\n",
    "- **High-level features**: Complete objects and structures\n",
    "- **Semantic understanding**: Infrastructure types, conditions\n",
    "- **Context recognition**: Urban vs rural, maintenance levels\n",
    "- **Railway context**: Track types, environmental conditions\n",
    "\n",
    "### Interpreting Filter Responses:\n",
    "\n",
    "**Bright regions in feature maps** = Strong feature detection\n",
    "**Dark regions in feature maps** = Weak/no feature detection\n",
    "**Scattered bright spots** = Feature detected in multiple locations\n",
    "**Uniform activation** = Feature present throughout the image\n",
    "\n",
    "### Model Differences:\n",
    "\n",
    "**VGG16:**\n",
    "- Simpler architecture with clear hierarchical feature learning\n",
    "- Good for understanding basic feature progression\n",
    "- More interpretable layer-by-layer feature evolution\n",
    "\n",
    "**ResNet50:**\n",
    "- Skip connections allow for more complex feature combinations\n",
    "- Better at capturing both low-level and high-level features\n",
    "- More robust feature extraction due to residual learning\n",
    "\n",
    "### Why This Matters for Clustering:\n",
    "\n",
    "1. **Feature Quality**: Understanding what features are detected helps choose optimal layers\n",
    "2. **Model Choice**: VGG16 vs ResNet50 may cluster images differently based on their feature extraction\n",
    "3. **Layer Selection**: Earlier vs later layers capture different aspects of the image\n",
    "\n",
    "### To Analyze Different Images:\n",
    "\n",
    "Simply modify the `EXAMPLE_IMAGE_PATH` variable in the configuration cell and re-run the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
