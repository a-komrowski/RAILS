{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3df4de",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised CNN Training – Trackbed Surface Classification (ASPHALT, BALLAST, GRAS, STONE, ERROR)\n",
    "\n",
    "**Goal.** Train and evaluate a supervised convolutional neural network (Transfer Learning with ResNet50V2) to classify railway trackbed images into **five single-label classes**: `ASPHALT`, `BALLAST`, `GRAS`, `STONE`, `ERROR`.  \n",
    "This notebook implements a **complete, self-contained ML pipeline**:\n",
    "\n",
    "1. **Data I/O** from TFRecord (parsing, decoding, normalization)\n",
    "2. **Model** definition (pretrained ResNet50V2 backbone + small classifier head)\n",
    "3. **Training** with the _best config_ (see below)\n",
    "4. **Evaluation** with detailed metrics (confusion matrix, per-class precision/recall/F1, macro/weighted scores)\n",
    "5. **Visualization** (training curves, confusion matrix, per-class bars, class distribution)\n",
    "6. **Artifacts** (saved model + CSV/JSON results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95295a66",
   "metadata": {},
   "source": [
    "\n",
    "## Quick start\n",
    "\n",
    "1. Set the two TFRecord paths below:\n",
    "   - `TRAIN_TFRECORD_PATH` — training set (single-file TFRecord)\n",
    "   - `EVAL_TFRECORD_PATH` — evaluation/hold-out set (single-file TFRecord)\n",
    "\n",
    "2. Optionally adjust `OUTPUT_DIR` and `MODEL_NAME`.\n",
    "\n",
    "3. Run all cells (GPU recommended).\n",
    "\n",
    "**Assumptions about TFRecords** (as created by our earlier pipeline):\n",
    "- Each example contains: `image_filename` (string), `image_raw` (bytes), `height` (int64), `width` (int64), `depth` (int64), `label` (int64 class index), `class_name` (string).\n",
    "- Labels are 0–4, mapping to: `ASPHALT`, `BALLAST`, `GRAS`, `STONE`, `ERROR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration (edit these) ==============================================\n",
    "\n",
    "# TFRecord inputs\n",
    "TRAIN_TFRECORD_PATH = \"/media/andi/ssd2/dev/code/Overseer2/data/inputs/MultiLabel_TB_small_08-25.tfrecord\"   # <-- EDIT\n",
    "EVAL_TFRECORD_PATH  = \"/media/andi/ssd2/dev/code/Overseer2/data/inputs/MultiLabel_TB_Evaluation_08-25.tfrecord\"    # <-- EDIT\n",
    "\n",
    "# Output directory + model name\n",
    "OUTPUT_DIR = \"./outputs_trackbed\"                         # artifacts will be saved here\n",
    "MODEL_NAME = \"PT_MultiLabelResNetS_Trackbed\"\n",
    "\n",
    "# Label mapping and image shape (matches our ResNet50V2 setup)\n",
    "CLASSES = [\"ASPHALT\", \"BALLAST\", \"GRAS\", \"STONE\", \"ERROR\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH  = 224\n",
    "IMG_DEPTH  = 3\n",
    "\n",
    "# \"Best\" training configuration (as agreed/established externally)\n",
    "BEST_CONFIG = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 30,\n",
    "    # You can tweak these without changing the core \"best\" idea\n",
    "    \"val_fraction\": 0.2,         # reserve a small fraction of TRAIN for validation\n",
    "    \"early_stopping_patience\": 6 # be gentle to avoid over/under-fitting\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b069115",
   "metadata": {},
   "source": [
    "\n",
    "## Environment & reproducibility\n",
    "\n",
    "We initialize TensorFlow, set seeds for determinism (as much as is practical on GPU),\n",
    "and enable GPU memory growth to avoid OOM on first allocation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e633b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, random, math, itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reproducibility (best-effort on GPU)\n",
    "SEED = 123\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# GPU memory growth (optional but recommended)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Enabled memory growth for {len(gpus)} GPU(s).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not set memory growth: {e}\")\n",
    "\n",
    "# Create output dir\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Output dir:\", os.path.abspath(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8d657",
   "metadata": {},
   "source": [
    "\n",
    "## Data pipeline (TFRecord → `tf.data.Dataset`)\n",
    "\n",
    "We mirror the schema used in `create_trackbed_tfrecord.ipynb`.  \n",
    "Loading steps:\n",
    "\n",
    "1. Parse features from each TFRecord example.\n",
    "2. Decode raw bytes → image tensor; ensure 3 channels; **resize to 224×224**.\n",
    "3. One‑hot encode labels (5 classes).\n",
    "4. Normalize to `[0,1]`.\n",
    "5. Shuffle + split `TRAIN` into **train/validation** (by `val_fraction`).  \n",
    "   The `EVAL` TFRecord is loaded as‑is for final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature schema (must match TFRecord writer)\n",
    "FEATURE_DESC = {\n",
    "    'image_filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw':      tf.io.FixedLenFeature([], tf.string),\n",
    "    'height':         tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width':          tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth':          tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label':          tf.io.FixedLenFeature([], tf.int64),\n",
    "    'class_name':     tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_tfrecord(proto):\n",
    "    \"\"\"Parse a single Example proto.\"\"\"\n",
    "    return tf.io.parse_single_example(proto, FEATURE_DESC)\n",
    "\n",
    "def _decode_and_preprocess(feat_dict):\n",
    "    \"\"\"Decode bytes → image; enforce 3 channels; resize to 224x224; one‑hot label.\"\"\"\n",
    "    img = tf.io.decode_raw(feat_dict['image_raw'], tf.uint8)\n",
    "    h   = tf.cast(feat_dict['height'], tf.int32)\n",
    "    w   = tf.cast(feat_dict['width'],  tf.int32)\n",
    "    d   = tf.cast(feat_dict['depth'],  tf.int32)\n",
    "    img = tf.reshape(img, [h, w, d])\n",
    "\n",
    "    # If single-channel, convert to RGB for pretrained models\n",
    "    def to_rgb(x):\n",
    "        return tf.image.grayscale_to_rgb(x)\n",
    "\n",
    "    img = tf.cond(tf.equal(d, 1), lambda: to_rgb(img), lambda: img)\n",
    "\n",
    "    # Resize to model input\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # normalize\n",
    "\n",
    "    label_index = tf.cast(feat_dict['label'], tf.int32)\n",
    "    label_1h    = tf.one_hot(label_index, depth=NUM_CLASSES)\n",
    "    return img, label_1h\n",
    "\n",
    "def _decode_with_filename(feat_dict):\n",
    "    \"\"\"Variant that also returns the original filename for evaluation/analysis.\"\"\"\n",
    "    img, label_1h = _decode_and_preprocess(feat_dict)\n",
    "    return img, label_1h, feat_dict['image_filename']\n",
    "\n",
    "def _count_records(tfrecord_path):\n",
    "    \"\"\"Count number of examples in a single-file TFRecord.\"\"\"\n",
    "    return sum(1 for _ in tf.data.TFRecordDataset(tfrecord_path))\n",
    "\n",
    "def load_train_val_ds(tfrecord_path, batch_size, val_fraction=0.1, shuffle_multiplier=20):\n",
    "    \"\"\"Create train/val datasets from a single TFRecord file by a deterministic split.\"\"\"\n",
    "    n_total = _count_records(tfrecord_path)\n",
    "    n_val   = max(1, int(round(n_total * float(val_fraction))))\n",
    "    n_train = max(1, n_total - n_val)\n",
    "    print(f\"Found {n_total} samples → train: {n_train}, val: {n_val}\")\n",
    "\n",
    "    raw = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    raw = raw.map(_parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # We perform a simple split by 'take/skip' (repeatable as long as the file order doesn't change).\n",
    "    # For stronger randomness across epochs, you could shuffle before splitting,\n",
    "    # but then report the exact split seed in your paper.\n",
    "    train_raw = raw.take(n_train)\n",
    "    val_raw   = raw.skip(n_train)\n",
    "\n",
    "    # Build train ds\n",
    "    train_ds = (train_raw\n",
    "                .shuffle(buffer_size=batch_size*shuffle_multiplier, seed=SEED, reshuffle_each_iteration=True)\n",
    "                .map(_decode_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    # Build val ds\n",
    "    val_ds = (val_raw\n",
    "              .map(_decode_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    return train_ds, val_ds, n_train, n_val\n",
    "\n",
    "def load_eval_ds(tfrecord_path, batch_size):\n",
    "    raw = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    raw = raw.map(_parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds  = (raw\n",
    "           .map(_decode_with_filename, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "           .batch(batch_size)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4497d7d",
   "metadata": {},
   "source": [
    "\n",
    "## Model\n",
    "\n",
    "We reuse the architecture from our previous experiments: **ResNet50V2** (ImageNet pretrained, frozen) + a lightweight MLP head.  \n",
    "Loss is **categorical cross‑entropy** (single-label, 5-way softmax). Metrics include **categorical accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "def build_pt_multilabel_resnets_trackbed(initial_lr=1e-4, loss_fn='categorical_crossentropy'):\n",
    "    # Input\n",
    "    inp = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH))\n",
    "\n",
    "    # Pretrained backbone (frozen)\n",
    "    base = ResNet50V2(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH), pooling='avg')\n",
    "    base.trainable = False\n",
    "\n",
    "    x = base(inp)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    out = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    # Exponential decay on LR (as in our reference)\n",
    "    lr_sched = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=float(initial_lr),\n",
    "        decay_steps=10_000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    opt = Adam(learning_rate=lr_sched)\n",
    "    model.compile(optimizer=opt, loss=loss_fn, metrics=[tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy')])\n",
    "    return model\n",
    "\n",
    "model = build_pt_multilabel_resnets_trackbed(initial_lr=BEST_CONFIG[\"learning_rate\"], loss_fn='categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7313c",
   "metadata": {},
   "source": [
    "\n",
    "## Training (single **best** configuration)\n",
    "\n",
    "We **only** train the configuration we previously identified as best:  \n",
    "`learning_rate=1e-4`, `batch_size=32`, `num_epochs=30`.\n",
    "\n",
    "We reserve `val_fraction` (default 10%) of the training TFRecord for validation.  \n",
    "Callbacks: `ModelCheckpoint` (best `val_loss`), `EarlyStopping` (patience configurable), and `CSVLogger`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "BATCH_SIZE = int(BEST_CONFIG[\"batch_size\"])\n",
    "EPOCHS     = int(BEST_CONFIG[\"num_epochs\"])\n",
    "VAL_FRAC   = float(BEST_CONFIG[\"val_fraction\"])\n",
    "\n",
    "# Load datasets\n",
    "train_ds, val_ds, n_train, n_val = load_train_val_ds(TRAIN_TFRECORD_PATH, batch_size=BATCH_SIZE, val_fraction=VAL_FRAC)\n",
    "eval_ds = load_eval_ds(EVAL_TFRECORD_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Callbacks & paths\n",
    "timestamp   = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir     = Path(OUTPUT_DIR) / f\"{MODEL_NAME}__{timestamp}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH  = str(run_dir / f\"{MODEL_NAME}.keras\")\n",
    "LOG_CSV     = str(run_dir / \"training_log.csv\")\n",
    "CFG_JSON    = str(run_dir / \"config.json\")\n",
    "CLASSES_JSON= str(run_dir / \"classes.json\")\n",
    "\n",
    "# Save config & classes for reproducibility\n",
    "with open(CFG_JSON, \"w\") as f:\n",
    "    json.dump(BEST_CONFIG, f, indent=2)\n",
    "with open(CLASSES_JSON, \"w\") as f:\n",
    "    json.dump(CLASSES, f, indent=2)\n",
    "\n",
    "cbs = [\n",
    "    ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=int(BEST_CONFIG[\"early_stopping_patience\"]), restore_best_weights=True, verbose=1),\n",
    "    CSVLogger(LOG_CSV)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nBest model saved to:\", MODEL_PATH)\n",
    "print(\"Logs saved to:\", LOG_CSV)\n",
    "print(\"Train/Val sizes:\", n_train, n_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d228ab",
   "metadata": {},
   "source": [
    "\n",
    "## Training curves\n",
    "\n",
    "We visualize loss and categorical accuracy across epochs for both training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ebc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot: Loss\n",
    "plt.figure()\n",
    "plt.plot(history.history.get('loss', []), label='train_loss')\n",
    "plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot: Categorical Accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history.get('categorical_accuracy', []), label='train_cat_acc')\n",
    "plt.plot(history.history.get('val_categorical_accuracy', []), label='val_cat_acc')\n",
    "plt.title('Categorical Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56236f01",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation on the hold‑out EVAL set\n",
    "\n",
    "We compute:\n",
    "- Overall **confusion matrix**\n",
    "- Per‑class **Precision**, **Recall (Sensitivity)**, **Specificity**, **F1‑Score**, **Accuracy**\n",
    "- **Type I/II** error rates\n",
    "- **Macro** (unweighted) and **Weighted** averages\n",
    "\n",
    "We also save:\n",
    "- `evaluation_summary_*.csv` — metrics table\n",
    "- `confusion_matrix_*.csv` — raw counts\n",
    "- `false_inferences_*.json` — filenames of FP/FN + misclassifications (with predicted vs. true class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure we load the best-saved model from disk (even if EarlyStopping restored in-memory weights)\n",
    "best_model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "# Predict across the eval set\n",
    "all_true_idx = []\n",
    "all_pred_idx = []\n",
    "false_dict = {\n",
    "    'false_positive': {c: [] for c in CLASSES},\n",
    "    'false_negative': {c: [] for c in CLASSES},\n",
    "    'misclassified': []\n",
    "}\n",
    "\n",
    "for batch_imgs, batch_trues, batch_fns in eval_ds:\n",
    "    probs = best_model.predict(batch_imgs, verbose=0)\n",
    "    pred_idx = np.argmax(probs, axis=1)\n",
    "    true_idx = np.argmax(batch_trues.numpy(), axis=1)\n",
    "\n",
    "    all_pred_idx.extend(list(pred_idx))\n",
    "    all_true_idx.extend(list(true_idx))\n",
    "\n",
    "    # Track errors with filenames + confidence\n",
    "    for ti, pi, fn, p in zip(true_idx, pred_idx, batch_fns.numpy(), probs):\n",
    "        filename = fn.decode('utf-8') if isinstance(fn, (bytes, bytearray)) else str(fn)\n",
    "        conf = float(np.max(p))\n",
    "        if ti != pi:\n",
    "            false_dict['misclassified'].append({\n",
    "                'filename': filename,\n",
    "                'true_class': CLASSES[int(ti)],\n",
    "                'predicted_class': CLASSES[int(pi)],\n",
    "                'confidence': conf\n",
    "            })\n",
    "        # Per-class FP/FN views\n",
    "        for ci in range(NUM_CLASSES):\n",
    "            # Binary view for class ci\n",
    "            true_bin = (ti == ci)\n",
    "            pred_bin = (pi == ci)\n",
    "            if (not true_bin) and pred_bin:\n",
    "                false_dict['false_positive'][CLASSES[ci]].append(f\"{filename}_{conf:.3f}\")\n",
    "            if true_bin and (not pred_bin):\n",
    "                false_dict['false_negative'][CLASSES[ci]].append(f\"{filename}_{conf:.3f}\")\n",
    "\n",
    "all_true_idx = np.array(all_true_idx, dtype=int)\n",
    "all_pred_idx = np.array(all_pred_idx, dtype=int)\n",
    "\n",
    "# Overall confusion matrix (NUM_CLASSES x NUM_CLASSES)\n",
    "overall_conf = confusion_matrix(all_true_idx, all_pred_idx, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "# Build per-class binary TP/FP/FN/TN\n",
    "binary_conf = np.zeros((NUM_CLASSES, 4), dtype=float)  # TP, FP, FN, TN\n",
    "for ci in range(NUM_CLASSES):\n",
    "    # For class ci: positive if true==ci / predicted==ci\n",
    "    tp = np.sum((all_true_idx == ci) & (all_pred_idx == ci))\n",
    "    fp = np.sum((all_true_idx != ci) & (all_pred_idx == ci))\n",
    "    fn = np.sum((all_true_idx == ci) & (all_pred_idx != ci))\n",
    "    tn = np.sum((all_true_idx != ci) & (all_pred_idx != ci))\n",
    "    binary_conf[ci] = [tp, fp, fn, tn]\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return (a / b) if b != 0 else 0.0\n",
    "\n",
    "# Compute metrics\n",
    "tps, fps, fns, tns = binary_conf[:,0], binary_conf[:,1], binary_conf[:,2], binary_conf[:,3]\n",
    "accuracies   = np.array([_safe_div(tp+tn, tp+fp+fn+tn) for tp,fp,fn,tn in binary_conf])\n",
    "recalls      = np.array([_safe_div(tp, tp+fn) for tp,fn in zip(tps,fns)])              # aka sensitivity\n",
    "specificity  = np.array([_safe_div(tn, tn+fp) for tn,fp in zip(tns,fps)])\n",
    "typeI_err    = np.array([_safe_div(fp, fp+tn) for fp,tn in zip(fps,tns)])\n",
    "typeII_err   = np.array([_safe_div(fn, tp+fn) for tp,fn in zip(tps,fns)])\n",
    "precisions   = np.array([_safe_div(tp, tp+fp) for tp,fp in zip(tps,fps)])\n",
    "f1_scores    = np.array([_safe_div(2*p*r, p+r) for p,r in zip(precisions,recalls)])\n",
    "\n",
    "overall_acc  = _safe_div(np.trace(overall_conf), np.sum(overall_conf))\n",
    "\n",
    "macro_precision = float(np.mean(precisions)) if len(precisions) else 0.0\n",
    "macro_recall    = float(np.mean(recalls))    if len(recalls)    else 0.0\n",
    "macro_f1        = float(np.mean(f1_scores))  if len(f1_scores)  else 0.0\n",
    "\n",
    "supports = tps + fns\n",
    "total    = np.sum(supports) if np.sum(supports) > 0 else 1.0\n",
    "weighted_precision = float(np.sum(precisions * supports) / total)\n",
    "weighted_recall    = float(np.sum(recalls * supports)    / total)\n",
    "weighted_f1        = float(np.sum(f1_scores * supports)  / total)\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": list(map(float, accuracies)) + [float(overall_acc)],\n",
    "    \"Precision\": list(map(float, precisions)) + [float(macro_precision)],\n",
    "    \"Recall\": list(map(float, recalls)) + [float(macro_recall)],\n",
    "    \"Specificity\": list(map(float, specificity)) + [float(np.mean(specificity) if len(specificity) else 0.0)],\n",
    "    \"F1-Score\": list(map(float, f1_scores)) + [float(macro_f1)],\n",
    "    \"Type I Error\": list(map(float, typeI_err)) + [float(np.mean(typeI_err) if len(typeI_err) else 0.0)],\n",
    "    \"Type II Error\": list(map(float, typeII_err)) + [float(np.mean(typeII_err) if len(typeII_err) else 0.0)],\n",
    "    \"Weighted Precision\": [weighted_precision],\n",
    "    \"Weighted Recall\": [weighted_recall],\n",
    "    \"Weighted F1-Score\": [weighted_f1],\n",
    "}\n",
    "\n",
    "# Save CSV metrics and confusion matrix + false inferences JSON\n",
    "eval_csv = str(Path(run_dir) / f\"evaluation_summary_{MODEL_NAME}.csv\")\n",
    "conf_csv = str(Path(run_dir) / f\"confusion_matrix_{MODEL_NAME}.csv\")\n",
    "false_json = str(Path(run_dir) / f\"false_inferences_{MODEL_NAME}.json\")\n",
    "\n",
    "with open(eval_csv, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Metric\", \"Class\", \"Value\"])\n",
    "    # per-class\n",
    "    for name, vals in metrics.items():\n",
    "        if name.startswith(\"Weighted\"):\n",
    "            continue\n",
    "        for idx, v in enumerate(vals[:-1]):\n",
    "            writer.writerow([name, CLASSES[idx], v])\n",
    "    # macro/overall\n",
    "    writer.writerow([])\n",
    "    writer.writerow([\"Metric\", \"Overall/Average\", \"Value\"])\n",
    "    for name, vals in metrics.items():\n",
    "        if name.startswith(\"Weighted\"):\n",
    "            writer.writerow([name, \"Weighted\", vals[0]])\n",
    "        else:\n",
    "            writer.writerow([name, \"Macro Average\", vals[-1]])\n",
    "\n",
    "np.savetxt(conf_csv, overall_conf, delimiter=',', fmt='%d', header=','.join(CLASSES), comments='')\n",
    "\n",
    "with open(false_json, 'w') as jf:\n",
    "    json.dump(false_dict, jf, indent=2)\n",
    "\n",
    "print(\"Saved:\", eval_csv)\n",
    "print(\"Saved:\", conf_csv)\n",
    "print(\"Saved:\", false_json)\n",
    "\n",
    "# --- Visualizations -----------------------------------------------------------\n",
    "\n",
    "# (1) Confusion matrix heatmap\n",
    "plt.figure()\n",
    "plt.imshow(overall_conf)\n",
    "plt.title('Confusion Matrix (counts)')\n",
    "plt.xticks(ticks=range(NUM_CLASSES), labels=CLASSES, rotation=45, ha='right')\n",
    "plt.yticks(ticks=range(NUM_CLASSES), labels=CLASSES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        plt.text(j, i, str(overall_conf[i, j]), ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (2) Per-class F1 bar chart\n",
    "plt.figure()\n",
    "plt.bar(CLASSES, f1_scores)\n",
    "plt.title('F1-Score by Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (3) Class distribution (support)\n",
    "supports_counts = overall_conf.sum(axis=1)\n",
    "plt.figure()\n",
    "plt.bar(CLASSES, supports_counts)\n",
    "plt.title('Evaluation Set Class Distribution (True labels)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (4) False Positives vs False Negatives per class\n",
    "fp_counts = [len(false_dict['false_positive'][c]) for c in CLASSES]\n",
    "fn_counts = [len(false_dict['false_negative'][c]) for c in CLASSES]\n",
    "\n",
    "plt.figure()\n",
    "x = np.arange(NUM_CLASSES)\n",
    "plt.bar(x - 0.2, fp_counts, width=0.4, label='False Positives')\n",
    "plt.bar(x + 0.2, fn_counts, width=0.4, label='False Negatives')\n",
    "plt.xticks(x, CLASSES, rotation=30, ha='right')\n",
    "plt.title('FP vs FN per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== EVAL SUMMARY ===\")\n",
    "print(f\"Overall accuracy: {overall_acc:.4f}\")\n",
    "print(f\"Macro F1-Score:  {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:     {weighted_f1:.4f}\")\n",
    "print(f\"Total samples:    {int(np.sum(overall_conf))}\")\n",
    "print(f\"Total misclass.:  {len(false_dict['misclassified'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddc136",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Qualitative peek\n",
    "\n",
    "If you want to visualize a few predictions with their true labels and confidences, run the next cell.  \n",
    "This draws directly from the `eval_ds` tensors (not from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_eval_samples(ds, model, k=6):\n",
    "    imgs_shown = 0\n",
    "    for imgs, labels, fns in ds:\n",
    "        probs = model.predict(imgs, verbose=0)\n",
    "        pred_idx = np.argmax(probs, axis=1)\n",
    "        true_idx = np.argmax(labels.numpy(), axis=1)\n",
    "\n",
    "        b = imgs.shape[0]\n",
    "        rows = int(math.ceil(min(k, b) / 3))\n",
    "        cols = 3 if k >= 3 else min(k, b)\n",
    "\n",
    "        plt.figure(figsize=(cols*3, rows*3))\n",
    "        for i in range(min(k, b)):\n",
    "            ax = plt.subplot(rows, cols, i+1)\n",
    "            ax.imshow(imgs[i].numpy())\n",
    "            pidx = int(pred_idx[i])\n",
    "            tidx = int(true_idx[i])\n",
    "            conf = float(np.max(probs[i]))\n",
    "            ax.set_title(f\"pred: {CLASSES[pidx]} ({conf:.2f})\\ntrue: {CLASSES[tidx]}\")\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        imgs_shown += min(k, b)\n",
    "        if imgs_shown >= k:\n",
    "            break\n",
    "\n",
    "# Uncomment to preview a few eval samples (set k as needed)\n",
    "show_eval_samples(eval_ds, best_model, k=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7834cb1",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & reproducibility\n",
    "\n",
    "- All **hyperparameters** and **class names** are saved alongside the trained model in the run folder.\n",
    "- The **best model** (by `val_loss`) is saved in Keras format: `<OUTPUT_DIR>/<MODEL_NAME>__<timestamp>/<MODEL_NAME>.keras`.\n",
    "- Metrics and confusion matrix are available as CSV files; false inferences are stored as JSON (with filenames and confidences).\n",
    "\n",
    "**Next steps (optional):**\n",
    "- Unfreeze upper ResNet stages for fine‑tuning (small LR).\n",
    "- Class‑balanced sampling / focal loss if the dataset is imbalanced.\n",
    "- Integrate cross‑validation across different TFRecord shards (if available).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
