{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db966c2",
   "metadata": {},
   "source": [
    "# RAILS — Railbed Analysis & Image Learning System\n",
    "\n",
    "\"RAILS“ ist ein fiktiver Projektname und steht in keiner Verbindung zu bestehenden Produkten, Marken oder Organisationen, etwaige Namensähnlichkeiten sind zufällig und nicht beabsichtigt.\n",
    "\n",
    "Dieses Notebook behandelt die Analyse und das Clustering von Schienenuntergrund-Bilddaten im Rahmen des Master Moduls *„Maschinelles Lernen“*. Grundlage sind rund 50.000 Graustufenbilder aus Messfahrten mehrerer europäischer Städte, die verschiedene Oberbautypen wie Schotter, Asphalt oder Rasen zeigen.  \n",
    " \n",
    "Ziel ist es, mit Hilfe von **Clustering- und Klassifikationsverfahren** Muster und Strukturen in den Daten zu erkennen und die Leistungsfähigkeit unterschiedlicher Ansätze zu vergleichen. Dazu werden die Bilder vorverarbeitet, mithilfe eines vortrainierten CNN in Embeddings überführt und anschließend mit Methoden wie KMeans und DBSCAN gruppiert.\n",
    " \n",
    "Das Notebook ist so strukturiert, dass es den gesamten Workflow von der technischen Vorbereitung bis zur Auswertung abbildet:  \n",
    "- Zunächst erfolgt das **Setup** der Umgebung sowie die **Konfiguration** der Pfade und Parameter.  \n",
    "- Anschließend werden **Datensatzstruktur und Metadaten** untersucht, bevor die **Feature-Extraktion** mit einem CNN durchgeführt wird.  \n",
    "- Die resultierenden Features werden **normalisiert und mittels PCA reduziert**, um sie für Clustering-Algorithmen vorzubereiten.  \n",
    "- Im Kernteil werden **KMeans** und **DBSCAN** angewendet, systematisch evaluiert und visuell miteinander verglichen.  \n",
    "- Schließlich werden die **Cluster-Ergebnisse gespeichert**, Reports erzeugt und eine detaillierte **Tenant-Analyse** erstellt.  \n",
    " \n",
    "Im Fokus steht die praktische Anwendung moderner Machine-Learning-Techniken auf reale Daten sowie eine transparente Dokumentation der Ergebnisse mit Metriken und Visualisierungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8d82b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f2c149",
   "metadata": {},
   "source": [
    "# Inhaltsverzeichnis\n",
    "- [Einleitung](#rails--railbed-analysis--image-learning-system)\n",
    "- [1. Setup: Importe & Umgebung](#1-setup-importe--umgebung)\n",
    "- [2. Konfiguration: Pfade, Modellparameter](#2-konfiguration-pfade-modellparameter)\n",
    "- [3. Ergebnisse-Verzeichnis & Datensatz-Check](#3-ergebnisse-verzeichnis--datensatz-check)\n",
    "- [4. Dateiname-Parsing, Dataset-Metadaten & Tenant-Verteilung](#4-dateiname-parsing-dataset-metadaten--tenant-verteilung)\n",
    "- [5. Feature-Extraktion](#5-feature-extraktion)\n",
    "- [5.1 Feature-Extraktion vorbereiten: Funktionen & Pipeline](#51-feature-extraktion-vorbereiten-funktionen--pipeline)\n",
    "- [5.2 Feature-Extraktion ausführen (Timing & Valid-Filter)](#52-feature-extraktion-ausführen-timing--valid-filter)\n",
    "- [6. Feature-Nachbearbeitung (Normalisierung & PCA → Vorbereitung fürs Clustering)](#6-feature-nachbearbeitung-normalisierung--pca--vorbereitung-fürs-clustering)\n",
    "- [7. K-Means](#7-k-means)\n",
    "- [7.1 K-Means & Clusteranalyse](#71-k-means--clusteranalyse)\n",
    "- [7.2 K-Means-Clusterbeispiele: Visualisierung von Beispielbildern](#72-k-means-clusterbeispiele-visualisierung-von-beispielbildern)\n",
    "- [8. DBSCAN](#8-dbscan)\n",
    "- [8.1 DBSCAN: Clustering & Kennzahlen](#81-dbscan-clustering--kennzahlen)\n",
    "- [8.2 DBSCAN-Clusterbeispiele: Visualisierung von Beispielbildern](#82-dbscan-clusterbeispiele-visualisierung-von-beispielbildern)\n",
    "- [9. Reporting und Visualisierung](#9-reporting-und-visualisierung)\n",
    "- [9.1 Visualisierung: K-Means & DBSCAN (Side-by-Side) mit t-SNE](#91-visualisierung-k-means--dbscan-side-by-side-mit-t-sne)\n",
    "- [10. Cluster-Ergebnisse speichern](#10-cluster-ergebnisse-speichern)\n",
    "- [10.1 Speicher-Helfer definieren (Ordner/Bildkopie/Metadaten)](#101-speicher-helfer-definieren-ordnerbildkopiemetadaten)\n",
    "- [10.2 K-Means-Cluster speichern (Ausführung)](#102-k-means-cluster-speichern-ausführung)\n",
    "- [10.3 DBSCAN-Cluster speichern (Ausführung)](#103-dbscan-cluster-speichern-ausführung)\n",
    "- [11. Tenant-Cluster-Analyse (Heatmap & Detailstatistik)](#11-tenant-cluster-analyse-heatmap--detailstatistik)\n",
    "- [12. Clustering-Report (JSON) erstellen & Kernergebnisse ausgeben](#12-clustering-report-json-erstellen--kernergebnisse-ausgeben)\n",
    "- [13. Ergebnisse der Clustering Phase](#13-ergebnisse-der-clustering-phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa00d48",
   "metadata": {},
   "source": [
    "## 1. Setup: Importe & Umgebung\n",
    "\n",
    "**Was:**  \n",
    "- Import zentraler Bibliotheken (Datenhandling, Visualisierung, ML/DL).  \n",
    "\n",
    "**Warum:**  \n",
    "- Konsistente, reproduzierbare Läufe und Zentralisierung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# Deep Learning Libraries - Import AFTER setting environment variables\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Image Processing\n",
    "from img_preprocessing import ImagePreprocessor\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c1b32",
   "metadata": {},
   "source": [
    "## 2. Konfiguration: Pfade, Modellparameter\n",
    "\n",
    "**Was:**  \n",
    "- Anlegen lauf-zeitgestempelter Ergebnisordner (`RESULTS_PATH`, `CLUSTERS_PATH`).  \n",
    "- Definition zentraler Parameter für Feature-Extraktion und Clustering.  \n",
    "- Ausgabe eines kompakten **Konfigurations-Snapshots** zur Nachvollziehbarkeit.\n",
    "\n",
    "**Warum:**  \n",
    "- **`TIMESTAMP`**: Versioniert jeden Run → kein Überschreiben, einfache Nachverfolgung & Vergleichbarkeit.  \n",
    "- **`DATASET_PATH=\"./datasets/clustering_sample_10000\"`**: Arbeitsdatensatz; kann später skaliert werden.  \n",
    "- **`BASE_RESULTS_PATH` / `RESULTS_PATH` / `CLUSTERS_PATH`**: Saubere Trennung von Analyseartefakten und exportierten Clusterbildern.  \n",
    "- **`BATCH_SIZE=32`**: Batchgröße **für die Feature-Extraktion** (unabhängig von MiniBatchKMeans).  \n",
    "- **`IMG_SIZE=(224, 224)`**: Passendes Eingabeformat für ResNet50 (ImageNet).  \n",
    "- **`FEATURE_DIM=1024`**: Resultiert aus **ResNet50 `conv4_block6_out` + GlobalAveragePooling**.  \n",
    "- **`PCA_COMPONENTS=50`**: Dimensionsreduktion (schnelleres, stabileres Clustering).  \n",
    "- **`RANDOM_STATE=42`**: Reproduzierbarkeit für PCA/t-SNE/K-Means.  \n",
    "- **`N_CLUSTERS_RANGE=range(3, 16)`**: Historischer Rest aus der Explorationsphase; **in der Endfassung nicht genutzt**.\n",
    "\n",
    "**Finale Parameter (gesetzt aus der Voranalyse):**  \n",
    "- **K-Means:** `k={BEST_K}`, `n_init={BEST_INIT}`, `random_state={RANDOM_STATE}`.  \n",
    "- **DBSCAN:** `eps={BEST_EPS}`, `min_samples={BEST_MIN_SAMPLES}`.  \n",
    "- **Feature Extractor:** `ResNet50 (conv4_block6_out)`  \n",
    "- **Preprocessing:** `L2 normalization + PCA`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "DATASET_PATH = \"./datasets/clustering_sample_10000\"\n",
    "BASE_RESULTS_PATH = f\"./results_{TIMESTAMP}\"\n",
    "RESULTS_PATH = f\"{BASE_RESULTS_PATH}/clustering_analysis\"\n",
    "CLUSTERS_PATH = f\"{BASE_RESULTS_PATH}/clustered_images\"\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "FEATURE_DIM = 1024 #Chosen because of ResNet50 layer conv4_block6_out\n",
    "N_CLUSTERS_RANGE = range(3, 16)\n",
    "PCA_COMPONENTS = 50\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Finalized clustering parameters (optimized through experimental phase)\n",
    "BEST_K = 5  # K-Means: highest silhouette score\n",
    "BEST_INIT = 20 # K-Means: number of initializations to avoid local minima\n",
    "\n",
    "\n",
    "BEST_EPS = 0.5  # DBSCAN: balanced score\n",
    "BEST_MIN_SAMPLES = 10  # DBSCAN: optimal density parameter\n",
    "\n",
    "# Finalized Feature Extraction Model\n",
    "FEATURE_EXTRACTOR = 'ResNet50 (conv4_block6_out)'\n",
    "\n",
    "#Finalized Preprocessing\n",
    "PREPROCESSING = 'L2 normalization + PCA'\n",
    "\n",
    "# Configuration snapshot\n",
    "config_snapshot = {\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'dataset_path': DATASET_PATH,\n",
    "    'feature_extractor': 'ResNet50 (conv4_block6_out)',\n",
    "    'preprocessing': 'L2 normalization + PCA',\n",
    "    'pca_components': PCA_COMPONENTS,\n",
    "    'kmeans_k': BEST_K,\n",
    "    'dbscan_eps': BEST_EPS,\n",
    "    'dbscan_min_samples': BEST_MIN_SAMPLES,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "print(f\"Run timestamp: {TIMESTAMP}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")\n",
    "print(f\"Clustered images will be saved to: {CLUSTERS_PATH}\")\n",
    "print(\"\\nFinalized parameters:\")\n",
    "for key, value in config_snapshot.items():\n",
    "    if key != 'timestamp':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"Results will be saved to:\")\n",
    "print(f\"  Analysis results: {RESULTS_PATH}\")\n",
    "print(f\"  Clustered images: {CLUSTERS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051fce4",
   "metadata": {},
   "source": [
    "## 3. Ergebnisse-Verzeichnis & Datensatz-Check\n",
    "\n",
    "**Was:**  \n",
    "- Anlage laufzeitgestempelter Ergebnisordner (Analyse, Clusterbilder), Ausgabe der Pfade.  \n",
    "- Sanity-Check des Datensatzpfads; Fallback: Auflistung vorhandener Datasets mit Bildanzahl.\n",
    "- Zählen aller `.png`-Dateien im gewählten Dataset.  \n",
    "- Persistieren einer `run_metadata.json` (Pfad- und Konfig-Snapshot).\n",
    "\n",
    "**Warum:**  \n",
    "- **Ordnererstellung** verhindert Fehler bei wiederholten Läufen.  \n",
    "- **Pfad-/Existenz-Check** reduziert Fehlersuche bei falschen Datasetangaben.  \n",
    "- **Bildzählung** schafft Transparenz (Basis für spätere Prozentangaben, Laufzeitabschätzungen).  \n",
    "- **`run_metadata.json`** dokumentiert Pfade & Konfiguration (Reproduzierbarkeit, Vergleichbarkeit zwischen Runs).\n",
    "\n",
    "**Outputs:**  \n",
    "- Verzeichnisse: `RESULTS_PATH`, `CLUSTERS_PATH`  \n",
    "- Datei: `RESULTS_PATH/run_metadata.json`  \n",
    "- Konsolen-Logs mit Pfaden und Bildanzahl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamped result roots (idempotent: no error if folders already exist)\n",
    "Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(CLUSTERS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created timestamped directories:\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Analysis results path: {RESULTS_PATH}\")\n",
    "print(f\"Clustered images path: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Check dataset existence; if missing, enumerate available subfolders under ./datasets\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Dataset path {DATASET_PATH} does not exist!\")\n",
    "    print(\"Available dataset directories:\")\n",
    "    datasets_dir = Path(\"./datasets\")\n",
    "    if datasets_dir.exists():\n",
    "        for subdir in datasets_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                # Count only files ending with '.png' (case-sensitive)\n",
    "                img_count = len([f for f in subdir.iterdir() if f.suffix == '.png'])\n",
    "                print(f\"  {subdir.name}: {img_count} images\")\n",
    "else:\n",
    "    # Count .png files in the selected dataset (flat folder expected)\n",
    "    img_count = len([f for f in Path(DATASET_PATH).iterdir() if f.suffix == '.png'])\n",
    "    print(f\"Found {img_count} images in dataset\")\n",
    "\n",
    "# Build a metadata snapshot for this run (paths, counts, and core configuration)\n",
    "run_metadata = {\n",
    "    'timestamp': TIMESTAMP,  # run identifier\n",
    "    'dataset_path': str(Path(DATASET_PATH).resolve()),\n",
    "    'total_images_found': img_count if os.path.exists(DATASET_PATH) else 0,\n",
    "    'analysis_results_path': str(Path(RESULTS_PATH).resolve()),\n",
    "    'clustered_images_path': str(Path(CLUSTERS_PATH).resolve()),\n",
    "    'configuration': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'img_size': IMG_SIZE,\n",
    "        'feature_dim': FEATURE_DIM,\n",
    "        'n_clusters_range': list(N_CLUSTERS_RANGE),\n",
    "        'pca_components': PCA_COMPONENTS,\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Persist metadata to JSON for reproducibility/audit\n",
    "with open(f\"{RESULTS_PATH}/run_metadata.json\", 'w') as f:\n",
    "    json.dump(run_metadata, f, indent=2)\n",
    "    \n",
    "print(f\"\\nRun metadata saved to: {RESULTS_PATH}/run_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838965c4",
   "metadata": {},
   "source": [
    "## 4. Dateiname-Parsing, Dataset-Metadaten & Tenant-Verteilung\n",
    "\n",
    "**Was:**  \n",
    "- Parsen der Dateinamen mit Suffix `_C.png`, um **tenant**, **SID** und **original_filename** zu extrahieren.  \n",
    "- Einlesen des Datasets zu einer **Dateiliste** und Aggregation der **Tenant-Verteilung**.\n",
    "- Ausgabe von **Gesamtzahl**, **#Tenants** und **Prozentanteilen** je Tenant.\n",
    "\n",
    "**Warum:**  \n",
    "- Die Metadaten aus dem Dateinamen erlauben **Slicing** (z. B. Auswertung pro Tenant) und **Qualitätskontrollen** (Ungleichgewichte erkennen).  \n",
    "- Die Fokussierung auf `_C.png` stellt sicher, dass nur **konforme, bereinigte** Bildvarianten verarbeitet werden (einheitliche Pipeline).\n",
    "\n",
    "**Outputs:**  \n",
    "- `image_files` (Liste mit Pfad + extrahierten Metadaten)  \n",
    "- `tenant_distribution` (Dict mit Zählungen)  \n",
    "- Konsolen-Logs (Totals, #Tenants, Verteilung in %)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c45d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename: str):\n",
    "    \"\"\"Parse filename to extract tenant, SID, and original filename.\"\"\"\n",
    "    # Expect filenames ending with '_C.png' (case-sensitive)\n",
    "    # Pattern convention: {tenant}_{sid}_{original}_C.png\n",
    "    if not filename.endswith('_C.png'):\n",
    "        return None, None, None\n",
    "    \n",
    "    name_without_ext = filename[:-6]  # Remove '_C.png' (6 characters)\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    # Require at least 3 parts: tenant, sid, and the remaining original name\n",
    "    if len(parts) >= 3:\n",
    "        tenant = parts[0]\n",
    "        sid = parts[1]\n",
    "        original_filename = '_'.join(parts[2:])\n",
    "        return tenant, sid, original_filename\n",
    "    \n",
    "    # Fallback if naming does not match the expected convention\n",
    "    return None, None, None\n",
    "\n",
    "def load_dataset_info(dataset_path: str):\n",
    "    \"\"\"Load and analyze dataset information.\"\"\"\n",
    "    # Build file index and aggregate tenant counts based on parsed filenames\n",
    "    image_files = []\n",
    "    tenant_distribution = defaultdict(int)\n",
    "    \n",
    "    # Non-recursive, case-sensitive pattern: only matches '*_C.png' in the given folder\n",
    "    for file_path in Path(dataset_path).glob('*_C.png'):\n",
    "        filename = file_path.name\n",
    "        tenant, sid, original_name = parse_filename(filename)\n",
    "        \n",
    "        # Keep only files that match the naming convention\n",
    "        if tenant:\n",
    "            image_files.append({\n",
    "                'filepath': str(file_path),\n",
    "                'filename': filename,\n",
    "                'tenant': tenant,\n",
    "                'sid': sid,\n",
    "                'original_name': original_name\n",
    "            })\n",
    "            tenant_distribution[tenant] += 1\n",
    "    \n",
    "    return image_files, dict(tenant_distribution)\n",
    "\n",
    "# Load dataset information (flat folder expected)\n",
    "print(\"Loading dataset information...\")\n",
    "image_files, tenant_distribution = load_dataset_info(DATASET_PATH)\n",
    "\n",
    "# Summary (assumes >=1 valid image; add guard if needed)\n",
    "print(f\"\\nTotal images: {len(image_files)}\")\n",
    "print(f\"Number of tenants: {len(tenant_distribution)}\")\n",
    "print(\"\\nTenant distribution:\")\n",
    "for tenant, count in sorted(tenant_distribution.items()):\n",
    "    percentage = (count / len(image_files)) * 100\n",
    "    print(f\"  {tenant}: {count} images ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad44ed",
   "metadata": {},
   "source": [
    "### 5. Feature-Extraktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f40cd",
   "metadata": {},
   "source": [
    "### 5.1 Feature-Extraktion vorbereiten: Funktionen & Pipeline\n",
    "\n",
    "**Was:**  \n",
    "- Hilfsfunktionen zum Laden/Preprocessen einzelner Bilder, Aufbau des **ResNet50**-Encoders bis **`conv4_block6_out`** (mit **GlobalAveragePooling**) sowie **batchweises** Ableiten der Feature-Embeddings.  \n",
    "- Rückgabe: Feature-Matrix **(N × 1024)** und Liste der erfolgreich verarbeiteten Pfade.\n",
    "\n",
    "**Warum:**  \n",
    "- Klare Trennung von **I/O**, **Modellkonstruktion** und **Batch-Inferenz** vereinfacht Debugging und ermöglicht späteren Encodertausch ohne Code-Duplikate.\n",
    "- Feature-Extraktion: **ResNet50 `conv4_block6_out` + GlobalAveragePooling** → **1024-dimensionale** Vektoren (finale Basis für L2 + PCA). \n",
    "\n",
    "**Outputs:**  \n",
    "- **Keine Dateien** – nur In-Memory: `features` (N×1024) und `valid_paths` für Folgeschritte (Normalisierung, PCA, Clustering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path: str, target_size: tuple = IMG_SIZE):\n",
    "    \"\"\"Load and preprocess image for ResNet50.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        # insert preprocessing here\n",
    "        # for exmaple:\n",
    "        #img = ImagePreprocessor.method_3_gamma_correction(img)\n",
    "        img_array = image.img_to_array(img)    \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_feature_extractor():\n",
    "    \"\"\"Create ResNet50 feature extractor up to 'conv4_block6_out' with GAP.\"\"\"\n",
    "    print(\"Loading ResNet50 model (conv4_block6_out + GAP)...\")\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "    )\n",
    "    x = base_model.get_layer('conv4_block6_out').output\n",
    "    x = GlobalAveragePooling2D(name='gap_conv4')(x)  # -> (None, 1024)\n",
    "    model = Model(inputs=base_model.input, outputs=x, name='resnet50_conv4_gap')\n",
    "    print(f\"Feature extractor output shape: {model.output_shape}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_features_batch(model, image_paths: list, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Extract features from images in batches.\"\"\"\n",
    "    features = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    print(f\"Extracting features from {len(image_paths)} images...\")\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        batch_valid_paths = []\n",
    "        \n",
    "        # Load batch images\n",
    "        for img_path in batch_paths:\n",
    "            img_array = load_and_preprocess_image(img_path)\n",
    "            if img_array is not None:\n",
    "                batch_images.append(img_array[0])  # Remove batch dimension\n",
    "                batch_valid_paths.append(img_path)\n",
    "        \n",
    "        if batch_images:\n",
    "            # Convert to numpy array and predict\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_features = model.predict(batch_images, verbose=0)\n",
    "            \n",
    "            features.extend(batch_features)\n",
    "            valid_paths.extend(batch_valid_paths)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Processed {min(i + batch_size, len(image_paths))}/{len(image_paths)} images\")\n",
    "    \n",
    "    return np.array(features), valid_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd2971",
   "metadata": {},
   "source": [
    "### 5.2 Feature-Extraktion ausführen (Timing & Valid-Filter)\n",
    "\n",
    "**Was:**  \n",
    "- Initialisieren des **ResNet50**-Feature-Extractors bis **`conv4_block6_out`** (mit GlobalAveragePooling), Sammeln der `image_paths` und **batchweise** Extraktion der Embeddings.  \n",
    "- Messung der Laufzeit und Ausgabe zentraler Kennzahlen (Shape, Anzahl gültiger Bilder).  \n",
    "- Filtern nicht ladbarer Bilder und Aktualisieren von `image_files` auf die valide Teilmenge.\n",
    "\n",
    "**Warum:**  \n",
    "- Die Laufzeitmessung unterstützt die Abschätzung von Skalierung und geeigneten Batchgrößen.  \n",
    "- Der Valid-Filter stellt sicher, dass nachfolgende Schritte (Normalisierung, PCA, Clustering) nur konsistente Daten erhalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using ResNet50\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = create_feature_extractor()  # builds ResNet50 backbone \n",
    "\n",
    "# Extract image paths\n",
    "image_paths = [item['filepath'] for item in image_files]  # preserve order for alignment with features\n",
    "\n",
    "# Extract features\n",
    "start_time = time.time()\n",
    "features, valid_paths = extract_features_batch(feature_extractor, image_paths)  # batched inference\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nFeature extraction completed in {extraction_time:.2f} seconds\")\n",
    "print(f\"Extracted features shape: {features.shape}\")  # expect (N_valid, 1024)\n",
    "print(f\"Valid images: {len(valid_paths)}/{len(image_paths)}\")\n",
    "\n",
    "# Update image_files to only include valid images\n",
    "valid_image_files = []\n",
    "for img_file in image_files:\n",
    "    if img_file['filepath'] in valid_paths:  # keep only successfully processed paths\n",
    "        valid_image_files.append(img_file)\n",
    "\n",
    "image_files = valid_image_files  # downstream steps use the filtered metadata list\n",
    "print(f\"Updated image files: {len(image_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e92e21",
   "metadata": {},
   "source": [
    "## 6. Feature-Nachbearbeitung (Normalisierung & PCA → Vorbereitung fürs Clustering)\n",
    "\n",
    "**Was:**  \n",
    "- L2-Normierung der Embeddings (zeilenweise) und optionale PCA-Reduktion auf `PCA_COMPONENTS`.  \n",
    "- Ausgeben von Kennzahlen vor/nach Normalisierung sowie erklärter Gesamtvarianz der PCA.\n",
    "\n",
    "**Warum:**  \n",
    "- L2-Normierung macht Distanzen vergleichbar und stabilisiert Metriken.  \n",
    "- PCA reduziert Dimensionen, Rauschen und Rechenzeit. kann die Trennbarkeit für K-Means/DBSCAN verbessern.\n",
    "\n",
    "**Besonderheiten:**  \n",
    "- PCA-Fit erfolgt auf **normalisierten** Features; `random_state` für Reproduzierbarkeit gesetzt.  \n",
    "- Default für das Clustering sind die **voll normalisierten** Features. PCA-Features optional testen/umschalten.\n",
    "\n",
    "**Outputs:**  \n",
    "- `features_normalized`, `features_pca`\n",
    "- Konsolen-Stats (vor/nach Normalisierung, PCA-Varianz, Shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to unit length per sample (stabilizes distances for K-Means/DBSCAN)\n",
    "print(\"Normalizing features...\")\n",
    "features_normalized = normalize(features, norm='l2', axis=1)\n",
    "\n",
    "# Basic sanity statistics before/after normalization\n",
    "print(f\"Original feature statistics:\")\n",
    "print(f\"  Mean: {features.mean():.4f}\")\n",
    "print(f\"  Std: {features.std():.4f}\")\n",
    "print(f\"  Min: {features.min():.4f}\")\n",
    "print(f\"  Max: {features.max():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized feature statistics:\")\n",
    "print(f\"  Mean: {features_normalized.mean():.4f}\")\n",
    "print(f\"  Std: {features_normalized.std():.4f}\")\n",
    "print(f\"  L2 norm (first sample): {np.linalg.norm(features_normalized[0]):.4f}\")\n",
    "\n",
    "# Optional dimensionality reduction (denoising + speed-up for clustering/TSNE)\n",
    "print(f\"\\nApplying PCA to reduce dimensions from {FEATURE_DIM} to {PCA_COMPONENTS}...\")\n",
    "pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
    "features_pca = pca.fit_transform(features_normalized)\n",
    "\n",
    "# Report explained variance (sum) and resulting shape\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"PCA features shape: {features_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5e076",
   "metadata": {},
   "source": [
    "## 7. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381f980",
   "metadata": {},
   "source": [
    "### 7.1 K-Means & Clusteranalyse\n",
    "\n",
    "**Was:**  \n",
    "- **MiniBatchKMeans** mit **festen Parametern** (`k={BEST_K}`, `n_init={BEST_INIT}`, `random_state={RANDOM_STATE}`) auf den **PCA-reduzierten** und **L2-normalisierten** Features.  \n",
    "- Ableitung der **Clusterlabels** (`cluster_labels`) und Anreicherung von `image_files` um das Feld **`cluster`**.  \n",
    "- Kompakte **Qualitätsmetriken**  \n",
    "- Übersicht je Cluster inkl. **Tenant-Verteilung** (Anzahl & Prozent).\n",
    "\n",
    "**Warum:**  \n",
    "- Das finale K-Means-Modell liefert die **produktiven Labels** als Basis für Export, Visualisierung und Berichte.  \n",
    "- Tenant-Verteilungen unterstützen die **Interpretation** von Clustern und das Erkennen möglicher **Imbalances/Bias**.\n",
    "\n",
    "**Outputs:**  \n",
    "- In-Memory: `cluster_labels`, angereichertes `image_files['cluster']`.  \n",
    "- Konsole: *Inertia*, *Silhouette*, Clustergrößen und **Tenant-Rangfolge** pro Cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"K-MEANS (FINAL)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using finalized K-Means configuration: k={BEST_K}, n_init={BEST_INIT}, random_state={RANDOM_STATE}\")\n",
    "\n",
    "final_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=BEST_K,\n",
    "    random_state=RANDOM_STATE,\n",
    "    batch_size=100,\n",
    "    n_init=BEST_INIT\n",
    ")\n",
    "\n",
    "cluster_labels = final_kmeans.fit_predict(features_pca)\n",
    "\n",
    "# Evaluate clustering quality\n",
    "kmeans_inertia = final_kmeans.inertia_\n",
    "kmeans_silhouette = silhouette_score(features_pca, cluster_labels)\n",
    "print(f\"K-Means (k={BEST_K}) — Inertia: {kmeans_inertia:.2f}, Silhouette: {kmeans_silhouette:.4f}\")\n",
    "\n",
    "# Add cluster labels to image metadata\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['cluster'] = int(cluster_labels[i])\n",
    "\n",
    "# Cluster composition (Counts per cluster + Tenant distribution)\n",
    "print(f\"\\nCluster Analysis:\")\n",
    "cluster_stats = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for img_file in image_files:\n",
    "    cluster = img_file['cluster']\n",
    "    tenant = img_file['tenant']\n",
    "    cluster_stats[cluster]['total'] += 1\n",
    "    cluster_stats[cluster][tenant] += 1\n",
    "\n",
    "# Readable report per cluster\n",
    "for cluster_id in sorted(cluster_stats.keys()):\n",
    "    stats = cluster_stats[cluster_id]\n",
    "    total = stats['total']\n",
    "    print(f\"\\nCluster {cluster_id}: {total} images\")\n",
    "    \n",
    "    tenant_counts = {k: v for k, v in stats.items() if k != 'total'}\n",
    "    for tenant, count in sorted(tenant_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {tenant}: {count} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fe6c4",
   "metadata": {},
   "source": [
    "### 7.2 K-Means-Clusterbeispiele: Visualisierung von Beispielbildern\n",
    "\n",
    "**Was:**  \n",
    "- Zeigt pro Cluster eine Stichprobe zufälliger Bilder in einem Bildgrid (3 Spalten, mehrere Zeilen).  \n",
    "- Nutzt die finalen K-Means-Labels (`image_files[i]['cluster']`) als Grundlage.\n",
    "\n",
    "**Warum:**  \n",
    "- Schnelle, visuelle Qualitätsprüfung der Clusterinhalte (Plausibilität, Ausreißer, Muster).\n",
    "\n",
    "**Besonderheiten:**  \n",
    "- Zufallsstichprobe ≠ „repräsentativste“ Beispiele. \n",
    "- Fehlerhafte oder fehlende Dateien werden textuell im Grid gekennzeichnet.  \n",
    "- Ergebnisse werden als `cluster_<id>_examples.png` gespeichert.\n",
    "\n",
    "**Outputs:**\n",
    "- PNG-Bildtafeln je Cluster unter `RESULTS_PATH`, Konsolenlogs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_examples(image_files, cluster_id, n_examples=6):\n",
    "    \"\"\"Display example images from a specific cluster.\"\"\"\n",
    "    # Collect all images that belong to the requested cluster\n",
    "    cluster_images = [img for img in image_files if img['cluster'] == cluster_id]\n",
    "    \n",
    "    if not cluster_images:\n",
    "        print(f\"No images found for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    # Randomly sample examples (set np.random.seed(...) earlier for reproducibility)\n",
    "    examples = np.random.choice(cluster_images, min(n_examples, len(cluster_images)), replace=False)\n",
    "    \n",
    "    # Create subplot grid (3 columns; rows computed from sample size)\n",
    "    cols = 3\n",
    "    rows = (len(examples) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(f'Cluster {cluster_id} Examples ({len(cluster_images)} total images)', fontsize=16)\n",
    "    \n",
    "    for i, img_info in enumerate(examples):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Load and display image; on failure, render a textual placeholder\n",
    "        try:\n",
    "            img = Image.open(img_info['filepath'])\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(f\"{img_info['tenant']}_{img_info['sid']}\", fontsize=10)\n",
    "            axes[row, col].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error loading\\n{img_info['filename']}\", \n",
    "                               ha='center', va='center', transform=axes[row, col].transAxes)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide any empty subplot cells (if sample size not divisible by #cols)\n",
    "    for i in range(len(examples), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Save a PNG panel per cluster to RESULTS_PATH\n",
    "    plt.savefig(f\"{RESULTS_PATH}/cluster_{cluster_id}_examples.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Render example grids for all discovered clusters ---\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    display_cluster_examples(image_files, cluster_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f56f7",
   "metadata": {},
   "source": [
    "## 8. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777aff56",
   "metadata": {},
   "source": [
    "### 8.1 DBSCAN: Clustering & Kennzahlen\n",
    "\n",
    "**Was:**  \n",
    "- Dichtebasiertes Clustering mit **DBSCAN** auf den **PCA-reduzierten** und **L2-normalisierten** Features.  \n",
    "- Verwendung **fester Parameter**: `eps={BEST_EPS}`, `min_samples={BEST_MIN_SAMPLES}`.  \n",
    "- Berechnung kompakter Metriken: **Anzahl Cluster** (ohne Noise), **Noise-Anteil** und **Silhouette** (*nur auf Nicht-Noise-Punkten*).  \n",
    "- Anreicherung der Metadaten: Schreiben der DBSCAN-Labels in `image_files['dbscan']`.\n",
    "\n",
    "**Warum:**  \n",
    "- DBSCAN erkennt **nicht-kugelige Strukturen** und markiert **Ausreißer** (`-1`) als Noise.  \n",
    "- Feste Parameter sichern **Reproduzierbarkeit** der Endfassung (keine Grid-Suche/Sweeps).\n",
    "\n",
    "**Outputs:**  \n",
    "- In-Memory: `dbscan_labels`, angereichertes `image_files['dbscan']`.  \n",
    "- Konsole: Anzahl Cluster (ohne Noise), Noise-Quote, Silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN (FINAL)\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN (FINAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying DBSCAN on PCA-reduced features...\")\n",
    "print(f\"Using finalized DBSCAN configuration: eps={BEST_EPS}, min_samples={BEST_MIN_SAMPLES}\")\n",
    "\n",
    "# Final DBSCAN run with finalized parameters\n",
    "dbscan = DBSCAN(eps=BEST_EPS, min_samples=BEST_MIN_SAMPLES)\n",
    "dbscan_labels = dbscan.fit_predict(features_pca)\n",
    "\n",
    "# Basic metrics\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)  # exclude noise label (-1)\n",
    "n_noise = int((dbscan_labels == -1).sum())\n",
    "noise_ratio = n_noise / len(dbscan_labels) if len(dbscan_labels) > 0 else 0.0\n",
    "\n",
    "# Silhouette score on non-noise points (only if more than 1 cluster)\n",
    "if n_clusters > 1:\n",
    "    mask = dbscan_labels != -1\n",
    "    if mask.sum() > 1:\n",
    "        dbscan_silhouette = silhouette_score(features_pca[mask], dbscan_labels[mask])\n",
    "    else:\n",
    "        dbscan_silhouette = -1.0\n",
    "else:\n",
    "    dbscan_silhouette = -1.0\n",
    "\n",
    "print(f\"DBSCAN — Clusters: {n_clusters}, Noise: {n_noise} ({noise_ratio*100:.1f}%), \"\n",
    "      f\"Silhouette: {dbscan_silhouette:.4f}\")\n",
    "\n",
    "# (Optional) attach DBSCAN labels to metadata without overwriting K-Means labels\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['dbscan'] = int(dbscan_labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b276230",
   "metadata": {},
   "source": [
    "### 8.2 DBSCAN-Clusterbeispiele: Visualisierung von Beispielbildern\n",
    "\n",
    "**Was:**  \n",
    "- Visualisierung von Beispielbildern je **DBSCAN-Label** (inkl. **Noise = `-1`**).  \n",
    "- Für jedes Label werden bis zu `n_examples` Bilder zufällig ausgewählt und in einem Grid angezeigt; pro Label wird zudem ein PNG-Panel gespeichert.\n",
    "\n",
    "**Warum:**  \n",
    "- Visuelle Stichproben erleichtern die **Interpretation** der DBSCAN-Cluster (Motivähnlichkeiten, Ausreißer) und eine erste **Qualitätsprüfung** der Ergebnisse.\n",
    "\n",
    "**Outputs:**  \n",
    "- PNG-Bildtafeln je Cluster unter `RESULTS_PATH`, Konsolenlogs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN cluster examples: visualize sample images per DBSCAN label (-1 = noise)\n",
    "\n",
    "def display_dbscan_cluster_examples(image_files, cluster_id, n_examples=6):\n",
    "    \"\"\"Display example images for a given DBSCAN label (cluster_id; -1 denotes noise).\"\"\"\n",
    "    # Collect all images that belong to the requested DBSCAN label\n",
    "    cluster_images = [img for img in image_files if img.get('dbscan') == cluster_id]\n",
    "    \n",
    "    if not cluster_images:\n",
    "        print(f\"No images found for DBSCAN label {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    # Randomly sample examples (set a global seed earlier if you need reproducibility)\n",
    "    examples = np.random.choice(cluster_images, min(n_examples, len(cluster_images)), replace=False)\n",
    "    \n",
    "    # Create subplot grid (3 columns; rows computed from sample size)\n",
    "    cols = 3\n",
    "    rows = (len(examples) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    title_label = \"Noise (-1)\" if cluster_id == -1 else f\"Cluster {cluster_id}\"\n",
    "    fig.suptitle(f'DBSCAN {title_label} Examples ({len(cluster_images)} total images)', fontsize=16)\n",
    "    \n",
    "    for i, img_info in enumerate(examples):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Load and display image; on failure, render a textual placeholder\n",
    "        try:\n",
    "            img = Image.open(img_info['filepath'])\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].set_title(f\"{img_info['tenant']}_{img_info['sid']}\", fontsize=10)\n",
    "            axes[row, col].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[row, col].text(0.5, 0.5, f\"Error loading\\n{img_info.get('filename','<unknown>')}\", \n",
    "                                ha='center', va='center', transform=axes[row, col].transAxes)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide any empty subplot cells (if sample size not divisible by #cols)\n",
    "    for i in range(len(examples), rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Save a PNG panel per DBSCAN label to RESULTS_PATH\n",
    "    outfile = f\"{RESULTS_PATH}/dbscan_{'noise' if cluster_id == -1 else f'cluster_{cluster_id}'}_examples.png\"\n",
    "    plt.savefig(outfile, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Render example grids for all discovered DBSCAN labels (including noise, -1)\n",
    "print(\"=\" * 80)\n",
    "print(\"DBSCAN CLUSTER EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(set(dbscan_labels)):\n",
    "    display_dbscan_cluster_examples(image_files, cluster_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0668b09",
   "metadata": {},
   "source": [
    "## 9. Reporting und Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425b015",
   "metadata": {},
   "source": [
    "### 9.1 Visualisierung: K-Means & DBSCAN (Side-by-Side) mit t-SNE\n",
    "\n",
    "**Was:**  \n",
    "- Berechnet ein 2D-t-SNE auf den **PCA-Features** (`features_pca`) mit stabilen Parametern (`init='pca'`, `learning_rate='auto'`, **dynamische Perplexity**).  \n",
    "- Erstellt eine **Side-by-Side**-Visualisierung: links K-Means, rechts DBSCAN.  \n",
    "- Verwendet **diskrete Farbkarten** mit `BoundaryNorm` und eindeutigen **Legenden/Colorbars**; konsistente Farbindizierung via Remapping der Labels (0…C-1).  \n",
    "- Bei DBSCAN werden **Noise-Punkte (`-1`)** explizit **grau** dargestellt, reguläre Cluster erhalten `tab10`-Farben.\n",
    "\n",
    "**Warum:**  \n",
    "- t-SNE reduziert die **50D-PCA-Features** auf **2D** und ermöglicht eine **intuitive** Sicht auf die Clusterstruktur.  \n",
    "- Der **direkte Vergleich** von K-Means und DBSCAN in einem Plot macht Unterschiede der Verfahren sichtbar.  \n",
    "- **Stabile Parameter** erhöhen die **Reproduzierbarkeit** der Layouts über Läufe hinweg.\n",
    "\n",
    "**Besonderheiten:**  \n",
    "- **Perplexity-Guard:** Faustregel ≈ **1 % von N**, auf **[5, 50]** geclippt und zusätzlich **\\< N** erzwungen (t-SNE-Anforderung).  \n",
    "\n",
    "**Outputs:**  \n",
    "- Anzeige des Side-by-Side-Plots.  \n",
    "- Speicherung unter **`{RESULTS_PATH}/clustering_tsne_visualization_fixed.png`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization of K-Means and DBSCAN results with fixed discrete colors\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER VISUALIZATION WITH t-SNE (FINAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Computing t-SNE embedding with stable parameters...\")\n",
    "N = len(features_pca)\n",
    "\n",
    "# Dynamic, valid perplexity:\n",
    "# - rule of thumb ~1% of N, clipped to [5, 50]\n",
    "# - MUST satisfy 1 < perplexity < N (t-SNE requirement)\n",
    "if N < 10:\n",
    "    perplexity = max(2, N // 2)  # keep < N for very small datasets\n",
    "else:\n",
    "    perplexity = min(50, max(5, int(N * 0.01)))\n",
    "    perplexity = min(perplexity, N - 1)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    init='pca',              # better stability\n",
    "    learning_rate='auto',    # adaptive LR\n",
    "    perplexity=perplexity,   # validated perplexity\n",
    "    max_iter=1500,           # more iterations for convergence\n",
    "    random_state=RANDOM_STATE,\n",
    "    metric='euclidean',\n",
    "    early_exaggeration=12.0\n",
    ")\n",
    "\n",
    "tsne_features = tsne.fit_transform(features_pca)\n",
    "print(f\"t-SNE completed with perplexity={perplexity} (N={N})\")\n",
    "\n",
    "# Create side-by-side visualization (K-Means left, DBSCAN right)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# === K-MEANS PLOT WITH DISCRETE COLORS ===\n",
    "# Remap K-Means labels to 0..C-1 for consistent color indexing\n",
    "unique_kmeans = sorted(set(cluster_labels))\n",
    "kmeans_label_map = {old: new for new, old in enumerate(unique_kmeans)}\n",
    "kmeans_colors = np.array([kmeans_label_map[label] for label in cluster_labels])\n",
    "\n",
    "n_kmeans_clusters = len(unique_kmeans)\n",
    "kmeans_cmap = matplotlib.colors.ListedColormap(plt.cm.tab10(np.linspace(0, 1, n_kmeans_clusters)))\n",
    "kmeans_norm = matplotlib.colors.BoundaryNorm(\n",
    "    boundaries=np.arange(-0.5, n_kmeans_clusters, 1),\n",
    "    ncolors=n_kmeans_clusters\n",
    ")\n",
    "\n",
    "scatter1 = axes[0].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=kmeans_colors,\n",
    "    cmap=kmeans_cmap,\n",
    "    norm=kmeans_norm,\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[0].set_title(f'K-Means Clustering (k={BEST_K})\\nt-SNE Visualization')\n",
    "axes[0].set_xlabel('t-SNE Component 1')\n",
    "axes[0].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0], ticks=range(n_kmeans_clusters))\n",
    "cbar1.set_ticklabels([f'Cluster {unique_kmeans[i]}' for i in range(n_kmeans_clusters)])\n",
    "cbar1.set_label('K-Means Clusters')\n",
    "\n",
    "# === DBSCAN PLOT WITH NOISE IN GREY ===\n",
    "# Use final DBSCAN labels directly (no grid search artifacts)\n",
    "dbscan_colors_raw = dbscan_labels.copy()\n",
    "unique_dbscan = sorted(set(dbscan_colors_raw))\n",
    "n_dbscan_clusters = len([lbl for lbl in unique_dbscan if lbl != -1])\n",
    "n_dbscan_noise = int(np.sum(dbscan_colors_raw == -1))\n",
    "\n",
    "# Map noise (-1) -> 0 (grey), clusters -> 1..K for discrete colormap indexing\n",
    "if -1 in unique_dbscan:\n",
    "    # Grey for noise then tab10 for clusters\n",
    "    colors = ['#808080']\n",
    "    colors.extend(plt.cm.tab10(np.linspace(0, 1, n_dbscan_clusters)))\n",
    "    dbscan_label_map = {-1: 0}\n",
    "    idx = 1\n",
    "    for lbl in unique_dbscan:\n",
    "        if lbl != -1:\n",
    "            dbscan_label_map[lbl] = idx\n",
    "            idx += 1\n",
    "    total_colors = len(unique_dbscan)\n",
    "else:\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_dbscan_clusters))\n",
    "    dbscan_label_map = {lbl: i for i, lbl in enumerate(unique_dbscan)}\n",
    "    total_colors = n_dbscan_clusters\n",
    "\n",
    "dbscan_colors = np.array([dbscan_label_map[lbl] for lbl in dbscan_colors_raw])\n",
    "\n",
    "dbscan_cmap = matplotlib.colors.ListedColormap(colors)\n",
    "dbscan_norm = matplotlib.colors.BoundaryNorm(\n",
    "    boundaries=np.arange(-0.5, total_colors, 1),\n",
    "    ncolors=total_colors\n",
    ")\n",
    "\n",
    "scatter2 = axes[1].scatter(\n",
    "    tsne_features[:, 0],\n",
    "    tsne_features[:, 1],\n",
    "    c=dbscan_colors,\n",
    "    cmap=dbscan_cmap,\n",
    "    norm=dbscan_norm,\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "axes[1].set_title(\n",
    "    f'DBSCAN Clustering (eps={BEST_EPS}, min_samples={BEST_MIN_SAMPLES})\\n'\n",
    "    f't-SNE Visualization\\n{n_dbscan_clusters} clusters, {n_dbscan_noise} noise points'\n",
    ")\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[1], ticks=range(total_colors))\n",
    "if -1 in unique_dbscan:\n",
    "    tick_labels = ['Noise'] + [f'Cluster {i}' for i in range(n_dbscan_clusters)]\n",
    "else:\n",
    "    tick_labels = [f'Cluster {unique_dbscan[i]}' for i in range(n_dbscan_clusters)]\n",
    "cbar2.set_ticklabels(tick_labels)\n",
    "cbar2.set_label('DBSCAN Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/clustering_tsne_visualization_fixed.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c838f",
   "metadata": {},
   "source": [
    "### 10. Cluster-Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8eea9e",
   "metadata": {},
   "source": [
    "#### 10.1 Speicher-Helfer definieren (Ordner/Bildkopie/Metadaten)\n",
    " \n",
    "**Was:**  \n",
    "- es werden drei Hilfsfunktionen für die Speicherung der Cluster-Ergebnisse definiert\n",
    "- `create_cluster_directories()`: Legt strukturierte Ordnerhierarchie an (`method_name/cluster_<id>/` bzw. `noise/` für DBSCAN)\n",
    "- `copy_images_to_clusters()`: Kopiert Originalbilder in ihre jeweiligen Cluster-Ordner mit Duplikatsvermeidung\n",
    "- `save_cluster_metadata()`: Erstellt detaillierte JSON-Metadaten pro Cluster mit Tenant-Statistiken und Bildlisten\n",
    " \n",
    "**Warum:**  \n",
    "- Clustering-Ergebnisse werden dauerhaft und strukturiert gespeichert für spätere Analyse\n",
    "- Bilder werden nach Clustern sortiert → vereinfacht manuelle Inspektion und Qualitätskontrolle der Clustering Verfahren\n",
    "- JSON-File dokumentiert Cluster-Zusammensetzung, Tenant-Verteilung und verwendete Methodik\n",
    "- Separierte Cluster ermöglichen gezielte Weiterverarbeitung einzelner Gruppen, hierdurch ist ein Labeln der Cluster einfach möglich, um eine spätere Klassifizierung von neuen Daten vornehmen zu können\n",
    " \n",
    "**Besonderheiten:**  \n",
    "- Ordner-Erstellung und Dateikopien überschreiben nicht bei identischen Dateien (Größenvergleich)\n",
    "- Noise-Punkte (`-1`) werden speziell im noise/-Ordner gesammelt, reguläre Cluster in `cluster_<id>/`\n",
    "- Unterstützt sowohl K-Means (`cluster_labels`) als auch DBSCAN (`dbscan_labels`) Parameter\n",
    " \n",
    "**Outputs:**  \n",
    "- Ordner mit kopierten Originalbildern im festgelegten Dateipfad: `{base_path}/{method_name}/cluster_<id>/`\n",
    "- Metadaten Dateien je Cluster mit Angabe der `cluster_id`, `total_images`, `tenant_distribution` und den Details der Bilder\n",
    "- Erstellung von Statistiken zur Anzahl der erfolgreichen Kopien, Anzahl der Fehler (wenn vorhanden) und die Größe der Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_directories(base_path, method_name, cluster_labels, dbscan_labels=None):\n",
    "    \"\"\"Create directory structure for clustered images (K-Means or DBSCAN).\n",
    "    - Ensures Python int keys for cluster IDs (guards against numpy int types).\n",
    "    - For DBSCAN, the noise label (-1) is placed into a 'noise' folder at the end.\n",
    "    \"\"\"\n",
    "    method_path = Path(base_path) / method_name\n",
    "    method_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Input validation\n",
    "    if method_name == 'dbscan' and dbscan_labels is None:\n",
    "        raise ValueError(\"DBSCAN selected but 'dbscan_labels' is None.\")\n",
    "    if method_name not in ('kmeans', 'dbscan'):\n",
    "        raise ValueError(f\"Unknown method_name '{method_name}'. Use 'kmeans' or 'dbscan'.\")\n",
    "\n",
    "    # Collect unique cluster IDs and coerce to Python int\n",
    "    if method_name == 'kmeans':\n",
    "        labels = cluster_labels\n",
    "    else:  # dbscan\n",
    "        labels = dbscan_labels\n",
    "\n",
    "    unique_clusters = sorted({int(c) for c in labels})\n",
    "    if method_name == 'dbscan' and -1 in unique_clusters:\n",
    "        # Put noise (-1) at the end\n",
    "        unique_clusters = [c for c in unique_clusters if c != -1] + [-1]\n",
    "\n",
    "    cluster_dirs = {}\n",
    "    for cluster_id in unique_clusters:\n",
    "        # Ensure folders exist; noise goes to a dedicated folder\n",
    "        if cluster_id == -1:\n",
    "            cluster_dir = method_path / 'noise'\n",
    "        else:\n",
    "            cluster_dir = method_path / f'cluster_{cluster_id}'\n",
    "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cluster_dirs[int(cluster_id)] = cluster_dir  # enforce Python int key\n",
    "\n",
    "    return cluster_dirs\n",
    "\n",
    "\n",
    "def copy_images_to_clusters(image_files, cluster_dirs, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Copy images into their respective cluster directories.\n",
    "    - Validates inputs and label lengths.\n",
    "    - Coerces cluster IDs to Python int for safe dict indexing.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCopying images to {method_name.upper()} cluster directories...\")\n",
    "\n",
    "    # Input validation\n",
    "    if method_name == 'dbscan' and dbscan_labels is None:\n",
    "        raise ValueError(\"DBSCAN selected but 'dbscan_labels' is None.\")\n",
    "    if method_name not in ('kmeans', 'dbscan'):\n",
    "        raise ValueError(f\"Unknown method_name '{method_name}'. Use 'kmeans' or 'dbscan'.\")\n",
    "\n",
    "    labels_to_use = cluster_labels if method_name == 'kmeans' else dbscan_labels\n",
    "    if len(labels_to_use) != len(image_files):\n",
    "        raise ValueError(\n",
    "            f\"Label count ({len(labels_to_use)}) does not match image_files count ({len(image_files)}).\"\n",
    "        )\n",
    "\n",
    "    copied_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for i, img_file in enumerate(image_files):\n",
    "        try:\n",
    "            cluster_id = int(labels_to_use[i])  # enforce Python int\n",
    "            source_path = Path(img_file['filepath'])\n",
    "            target_dir = cluster_dirs.get(cluster_id)\n",
    "            if target_dir is None:\n",
    "                raise KeyError(f\"No directory for cluster_id '{cluster_id}'. Did you call create_cluster_directories()?\")\n",
    "\n",
    "            target_path = target_dir / source_path.name\n",
    "\n",
    "            # Copy only if missing or size differs\n",
    "            if not target_path.exists() or target_path.stat().st_size != source_path.stat().st_size:\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                copied_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {img_file.get('filename', source_path.name if 'source_path' in locals() else '<unknown>')}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Successfully copied {copied_count} images\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Errors: {error_count}\")\n",
    "\n",
    "    return copied_count, error_count\n",
    "\n",
    "\n",
    "def save_cluster_metadata(cluster_dirs, image_files, cluster_labels, method_name='kmeans', dbscan_labels=None):\n",
    "    \"\"\"Save per-cluster metadata (JSON) into each cluster directory.\n",
    "    - Validates inputs and label lengths.\n",
    "    - Ensures JSON-serializable cluster IDs and preserves non-ASCII characters.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving {method_name.upper()} cluster metadata...\")\n",
    "\n",
    "    # Input validation\n",
    "    if method_name == 'dbscan' and dbscan_labels is None:\n",
    "        raise ValueError(\"DBSCAN selected but 'dbscan_labels' is None.\")\n",
    "    if method_name not in ('kmeans', 'dbscan'):\n",
    "        raise ValueError(f\"Unknown method_name '{method_name}'. Use 'kmeans' or 'dbscan'.\")\n",
    "\n",
    "    labels_to_use = cluster_labels if method_name == 'kmeans' else dbscan_labels\n",
    "    if len(labels_to_use) != len(image_files):\n",
    "        raise ValueError(\n",
    "            f\"Label count ({len(labels_to_use)}) does not match image_files count ({len(image_files)}).\"\n",
    "        )\n",
    "\n",
    "    for cluster_id, cluster_dir in cluster_dirs.items():\n",
    "        # Collect images for this cluster\n",
    "        cluster_images = []\n",
    "        for i, img_file in enumerate(image_files):\n",
    "            if int(labels_to_use[i]) == int(cluster_id):\n",
    "                cluster_images.append({\n",
    "                    'filename': img_file['filename'],\n",
    "                    'tenant': img_file['tenant'],\n",
    "                    'sid': img_file['sid'],\n",
    "                    'original_name': img_file['original_name']\n",
    "                })\n",
    "\n",
    "        # Basic stats\n",
    "        tenant_counts = Counter(img['tenant'] for img in cluster_images)\n",
    "\n",
    "        metadata = {\n",
    "            'cluster_id': int(cluster_id),\n",
    "            'method': method_name,\n",
    "            'total_images': len(cluster_images),\n",
    "            'tenant_distribution': dict(tenant_counts),\n",
    "            'images': cluster_images\n",
    "        }\n",
    "\n",
    "        # Write JSON (preserve non-ASCII, pretty print)\n",
    "        metadata_file = cluster_dir / 'cluster_metadata.json'\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Metadata saved for {len(cluster_dirs)} clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92707b",
   "metadata": {},
   "source": [
    "#### 10.2 K-Means-Cluster speichern (Ausführung)\n",
    " \n",
    "**Was:**  \n",
    "- Führt die K-Means-Cluster-Speicherung mit den in Abschnitt 10.1 definierten Hilfsfunktionen aus\n",
    "- erstellt je gefundenem Cluster einen Ordner in der definierten Ordnerstruktur\n",
    "- generiert die zuvor definierten Statistiken als JSON-Datei\n",
    " \n",
    "**Warum:**  \n",
    "- vereinfachte visuelle Kontrolle\n",
    "- persistente Archivierung zur späteren Analyse\n",
    "- gezielte Weiterverarbeitung\n",
    "- direkte Vergleiche mit vorherigen Läufen durch Anlage der Ordnerstruktur inklusive Timestamp\n",
    " \n",
    "**Besonderheiten:**  \n",
    "- Zeitstempel-basierte Pfade für eindeutige Versionierung\n",
    "- Ausgabe von detaillierten Statistiken und Fehler-Tracking\n",
    " \n",
    "**Outputs:**  \n",
    "- kopierte Originalbilder in der gewählten Ordnerstruktur `{CLUSTERS_PATH}/kmeans/cluster_<0..k-1>/`\n",
    "- Metadaten Datei `cluster_metadata.json` für jedes Cluster\n",
    "- Anzeige der Anzahl erstellter Ordner, Cluster-Größen und Kopierstatistiken (Erfolg/Fehler)\n",
    "- Nennung des Speicherorts als Bestätigung `./results_{TIMESTAMP}/clustered_images/kmeans/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9463811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-Means clustering results to timestamped directories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING K-MEANS CLUSTERS TO TIMESTAMPED DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Using timestamped cluster directory: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Create K-Means cluster directories\n",
    "kmeans_dirs = create_cluster_directories(CLUSTERS_PATH, 'kmeans', cluster_labels)\n",
    "\n",
    "print(f\"Created K-Means cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(kmeans_dirs.items()):\n",
    "    cluster_size = sum(1 for label in cluster_labels if label == cluster_id)\n",
    "    print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images to K-Means clusters\n",
    "kmeans_copied, kmeans_errors = copy_images_to_clusters(\n",
    "    image_files, kmeans_dirs, cluster_labels, 'kmeans'\n",
    ")\n",
    "\n",
    "# Save K-Means cluster metadata\n",
    "save_cluster_metadata(kmeans_dirs, image_files, cluster_labels, 'kmeans')\n",
    "\n",
    "print(f\"\\nK-Means clustering results saved to: {Path(CLUSTERS_PATH) / 'kmeans'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebeb588",
   "metadata": {},
   "source": [
    "#### 10.3 DBSCAN-Cluster speichern (Ausführung)\n",
    " \n",
    "**Was:**  \n",
    "- Führt die DBSCAN-Cluster-Speicherung mit den in Abschnitt 10.1 definierten Hilfsfunktionen aus\n",
    "- erstellt je gefundenem Cluster einen Ordner in der definierten Ordnerstruktur, zusätzlich wird ein Ordner für die noise Punkte eingeführt\n",
    "- generiert die zuvor definierten Statistiken als JSON-Datei\n",
    " \n",
    "**Warum:**  \n",
    "- Ermöglichung der Analyse anomaler Bilder zur Identifizierung von Ausreißern\n",
    "- manuelle Überprüfung der dichtebasierten Gruppierung\n",
    "- direkter Vergleich beider Clustering Ansätze möglich\n",
    " \n",
    "**Besonderheiten:**  \n",
    "- Nutzt die \"balanced score\" Parameterwahl (`silhouette * (1 - noise_ratio)`) statt reiner Silhouette-Optimierung\n",
    " \n",
    "**Outputs:**  \n",
    "- kopierte Originalbilder in der gewählten Ordnerstruktur `{CLUSTERS_PATH}/dbscan/cluster_<0..n>/` + `noise/`\n",
    "- Metadaten Datei `cluster_metadata.json` für jedes Cluster\n",
    "- Anzeige der Anzahl erstellter Ordner, Cluster-Größen und Kopierstatistiken (Erfolg/Fehler)\n",
    "- Nennung des Speicherorts als Bestätigung `./results_{TIMESTAMP}/clustered_images/dbscan/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DBSCAN clustering results to timestamped directories (FINAL)\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING DBSCAN CLUSTERS TO TIMESTAMPED DIRECTORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Using timestamped cluster directory: {CLUSTERS_PATH}\")\n",
    "\n",
    "# Attach final DBSCAN labels to image metadata (keep key name consistent with visualization helpers)\n",
    "for i, img_file in enumerate(image_files):\n",
    "    img_file['dbscan'] = int(dbscan_labels[i])\n",
    "\n",
    "# Create DBSCAN cluster directories (noise label -1 goes to 'noise' folder)\n",
    "dbscan_dirs = create_cluster_directories(CLUSTERS_PATH, 'dbscan', None, dbscan_labels)\n",
    "\n",
    "print(\"Created DBSCAN cluster directories:\")\n",
    "for cluster_id, cluster_dir in sorted(dbscan_dirs.items()):\n",
    "    cluster_size = sum(1 for label in dbscan_labels if label == cluster_id)\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  {cluster_dir.name} (noise): {cluster_size} images\")\n",
    "    else:\n",
    "        print(f\"  {cluster_dir.name}: {cluster_size} images\")\n",
    "\n",
    "# Copy images into their DBSCAN cluster folders\n",
    "dbscan_copied, dbscan_errors = copy_images_to_clusters(\n",
    "    image_files, dbscan_dirs, None, 'dbscan', dbscan_labels\n",
    ")\n",
    "\n",
    "# Save per-cluster metadata JSONs\n",
    "save_cluster_metadata(dbscan_dirs, image_files, None, 'dbscan', dbscan_labels)\n",
    "\n",
    "print(f\"\\nDBSCAN clustering results saved to: {Path(CLUSTERS_PATH) / 'dbscan'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde73ad7",
   "metadata": {},
   "source": [
    "### 11. Tenant-Cluster-Analyse (Heatmap & Detailstatistik)\n",
    "**Was:**  \n",
    "- Baut eine Tenant×Cluster-Matrix auf mit zeilenweiser Normalisierung und berechnet Prozentanteile je Tenant pro Cluster\n",
    "- Erstellt eine Seaborn-Heatmap (`sns.heatmap`) mit Zellbeschriftung und `YlOrRd`-Farbskala zur Visualisierung der Verteilungen\n",
    " \n",
    "**Warum:**  \n",
    "- Identifiziert tenant-spezifische Clustering-Muster. Zeigt an, ob bestimmte Verkehrsunternehmen charakteristische Untergrund-/Situationstypen aufweisen\n",
    "- Starke Ungleichverteilungen können auf systematische Bias oder domänenspezifische Besonderheiten hinweisen\n",
    "- Visualisiert, ob das Clustering echte visueller Muster oder nur tenant-basierte Artefakte erfasst hat\n",
    " \n",
    "**Outputs:**  \n",
    "- Heatmap-Grafik: `tenant_cluster_heatmap.png` in `RESULTS_PATH` mit prozentualer Tenant-Verteilung über alle Cluster\n",
    "- Konsolen-Report: Detaillierte Aufschlüsselung je Tenant mit Gesamtzahl und clusterspezifischen Prozent-/Absolutwerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tenant distribution across clusters\n",
    "print(\"=\" * 80)\n",
    "print(\"TENANT DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create tenant-cluster matrix\n",
    "tenant_cluster_matrix = defaultdict(lambda: defaultdict(int))\n",
    "total_by_tenant = defaultdict(int)\n",
    "\n",
    "for img_file in image_files:\n",
    "    tenant = img_file['tenant']\n",
    "    cluster = img_file['cluster']\n",
    "    tenant_cluster_matrix[tenant][cluster] += 1\n",
    "    total_by_tenant[tenant] += 1\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "tenants = sorted(total_by_tenant.keys())\n",
    "clusters = sorted(set(cluster_labels))\n",
    "\n",
    "matrix_data = []\n",
    "for tenant in tenants:\n",
    "    row = []\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        row.append(percentage)\n",
    "    matrix_data.append(row)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = np.array(matrix_data)\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    xticklabels=[f'Cluster {c}' for c in clusters],\n",
    "    yticklabels=tenants,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Percentage of Tenant Images'}\n",
    ")\n",
    "plt.title('Tenant Distribution Across Clusters (%)')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Tenants')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/tenant_cluster_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Tenant-Cluster Distribution:\")\n",
    "for tenant in tenants:\n",
    "    print(f\"\\n{tenant} ({total_by_tenant[tenant]} images):\")\n",
    "    for cluster in clusters:\n",
    "        count = tenant_cluster_matrix[tenant][cluster]\n",
    "        percentage = (count / total_by_tenant[tenant]) * 100\n",
    "        if count > 0:\n",
    "            print(f\"  Cluster {cluster}: {count} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965da81",
   "metadata": {},
   "source": [
    "### 12. Clustering-Report (JSON) erstellen & Kernergebnisse ausgeben\n",
    "**Was:**  \n",
    "- erstellt `clustering_report.json` mit Zeitstempel-Informationen, Run-Metadaten und vollständigen Clustering-Ergebnissen\n",
    "- Sammelt Dataset-Info (Bildanzahl, Feature-Dimension, PCA-Varianz), K-Means-Resultate (optimal_k, Silhouette-Scores, Evaluation-Historie) und DBSCAN-Parameter\n",
    "- Integriert Tenant-Cluster-Analyse mit detaillierter Verteilungsmatrix pro Tenant über alle Cluster\n",
    " \n",
    "**Warum:**  \n",
    "- Alle Parameter, Metriken und Ergebnisse werden zentral für Nachvollziehbarkeit und Vergleichbarkeit archiviert\n",
    "- Run-spezifische Metadaten ermöglichen systematische Analyse von Clustering-Stabilität über mehrere Läufe\n",
    "- Parallele Dokumentation von K-Means und DBSCAN erleichtert objektive Bewertung beider Ansätze\n",
    "- Geschäftsspezifische Analyse zeigt, ob Clustering domänenrelevante Muster (Verkehrsunternehmen) erfasst\n",
    " \n",
    "**Outputs:**  \n",
    "- JSON-Report `clustering_report.json` in `RESULTS_PATH` mit vollständiger Run-Dokumentation und Parametern\n",
    "- Kompakte Übersicht mit Timestamp, optimalen Parametern, Cluster-/Noise-Anzahlen und PCA-Varianz in der Ausgabe\n",
    "- Absolute Pfade zu Analysis- und Cluster-Verzeichnissen für einfache Navigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive clustering report with timestamp info (FINAL)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMESTAMPED CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fallback: compute silhouette scores here if not already available\n",
    "try:\n",
    "    kmeans_silhouette\n",
    "except NameError:\n",
    "    kmeans_silhouette = silhouette_score(features_pca, cluster_labels)\n",
    "\n",
    "# DBSCAN metrics from final labels (no grid-search objects)\n",
    "dbscan_labels_arr = np.asarray(dbscan_labels)\n",
    "labels_set = set(dbscan_labels_arr.tolist())\n",
    "dbscan_n_clusters = len([lbl for lbl in labels_set if lbl != -1])\n",
    "dbscan_n_noise = int(np.sum(dbscan_labels_arr == -1))\n",
    "\n",
    "try:\n",
    "    dbscan_silhouette\n",
    "except NameError:\n",
    "    if dbscan_n_clusters > 1:\n",
    "        mask = dbscan_labels_arr != -1\n",
    "        dbscan_silhouette = silhouette_score(features_pca[mask], dbscan_labels_arr[mask]) if mask.sum() > 1 else -1.0\n",
    "    else:\n",
    "        dbscan_silhouette = -1.0\n",
    "\n",
    "# Use existing tenant_distribution if it already exists; otherwise compute it\n",
    "try:\n",
    "    tenant_distribution\n",
    "except NameError:\n",
    "    tenant_distribution = dict(Counter(img['tenant'] for img in image_files))\n",
    "\n",
    "# Build tenant-cluster analysis from image_files (independent of previous cells)\n",
    "tenant_cluster_matrix_report = defaultdict(lambda: defaultdict(int))\n",
    "total_by_tenant_report = Counter()\n",
    "for img in image_files:\n",
    "    tenant = img['tenant']\n",
    "    cl = int(img['cluster'])\n",
    "    tenant_cluster_matrix_report[tenant][cl] += 1\n",
    "    total_by_tenant_report[tenant] += 1\n",
    "tenants_report = sorted(total_by_tenant_report.keys())\n",
    "\n",
    "clustering_report = {\n",
    "    \"run_info\": {\n",
    "        \"timestamp\": TIMESTAMP,\n",
    "        \"run_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"analysis_results_path\": str(Path(RESULTS_PATH).resolve()),\n",
    "        \"clustered_images_path\": str(Path(CLUSTERS_PATH).resolve())\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"total_images\": int(len(image_files)),\n",
    "        \"feature_dimension\": int(FEATURE_DIM),\n",
    "        \"pca_components\": int(PCA_COMPONENTS),\n",
    "        \"pca_explained_variance\": float(pca.explained_variance_ratio_.sum()),\n",
    "        \"tenant_distribution\": {k: int(v) for k, v in tenant_distribution.items()}\n",
    "    },\n",
    "    \"kmeans_results\": {\n",
    "        \"k\": int(BEST_K),\n",
    "        \"silhouette_score\": float(kmeans_silhouette),\n",
    "        \"cluster_sizes\": {int(k): int(v) for k, v in Counter(int(l) for l in cluster_labels).items()}\n",
    "    },\n",
    "    \"dbscan_results\": {\n",
    "        \"eps\": float(BEST_EPS),\n",
    "        \"min_samples\": int(BEST_MIN_SAMPLES),\n",
    "        \"n_clusters\": int(dbscan_n_clusters),\n",
    "        \"n_noise\": int(dbscan_n_noise),\n",
    "        \"silhouette_score\": float(dbscan_silhouette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf527b3a",
   "metadata": {},
   "source": [
    "## 13. Ergebnisse der Clustering Phase\n",
    " \n",
    "- Das dichtebasierte Cluster mit DBSCAN konnte keinen zuverlässigen Erfolg liefern, trotz vielfacher Anpassung der Parameter.\n",
    "- Die Ergebnisse über die K-Means-Methode waren zufriedenstellend.\n",
    "- Die Ergebnisse der Cluster wurden mit drei unterschiedlichen Metriken bewertet. Neben den gezeigten Metriken wurden in einer separaten Analyse weitere Cluster-Gütemaße evaluiert, diese Ergebnisse sind in diesem Notebook noch nicht enthalten. Konkret kamen der Silhouette Score, der Davies–Bouldin Index und der Calinski–Harabasz Index zum Einsatz. Für Hintergrund und einen vergleichenden Überblick siehe: Arbelaitz et al. (2013).\n",
    "- Es wurde versucht mit Hilfe von einer Bildvorverarbeitung die Bilder vorzuverarbeiten, um z.B. dunkle Bilder aufzuhellen. Dies wurde im Schritt 5.1 optional in diesem Notebook eingebaut. Es konnte allerdings kein nennenswerter Effekt bzw. Verschlechterungen festgestellt werden, weshalb wir uns im folgenden dazu entschieden haben keine Bildvorverarbeitung vorzunehmen.\n",
    "- Zusätzlich wurden folgende Konfigurationen getestet. Diese Ergebnisse sind in diesem Notebook noch nicht enthalten.\n",
    "    - Feature-Extraction-Model: ResNet50,VGG16\n",
    "    - je Modell verschiedene Layer im Model:\n",
    "    `\n",
    "    'levels': {\n",
    "            'top': {'pooling': 'avg', 'layer': None},\n",
    "            'mid': {'layer': 'conv4_block6_out'/'block4_conv3'}\n",
    "            'low': {'layer': 'conv3_block4_out'/'block2_conv2'}\n",
    "        }\n",
    "    ` \n",
    "- Als beste Konfiguration hat sich das ResNet50 mit conv4_block6_out als Layer und keiner Bildvorverarbeitung herausgestellt.\n",
    "    - Die Ergebnisse aller Experimente sind in der mitgeschickten results.json.\n",
    "- Es wurden zwei Clustering Läufe mit jeweils 10.000 Bildern initialisiert (also insgesamt 20.000 unterschiedlichen Bildern). Ein Test mit 50.000 Bildern führte zu nicht plausiblen Clustern mit K-Means und wurde deshalb verworfen. Eine Analyse, weshalb der Lauf mit 50.000 Bildern nicht funktionierte wurde nicht angestrebt, da ein Clustering mit 20.000 Bildern für unsere Zwecke vollkommen ausreichend ist.\n",
    "- Die entstandenen 5 Cluster wurden anschließend manuell gesichtet und bereinigt. Es wurden folgende fünf Klassen identifiziert:\n",
    "    - Beton/Asphalt\n",
    "    - Gras/natürliche Untergründe (z.B. auch Laub)\n",
    "    - Schotter\n",
    "    - Stein (verschiedene Pflaster-Arten)\n",
    "    - Messfehler (zu dunkle Bilder, verrauschte Bilder)\n",
    "- Während der Bereinigung wurden fehlerhaft zugeordnete Bilder manuell entfernt, bevor ein Training des Klassifizierungsmodell stattfindet. Die entfernten Bilder wurden gesammelt und können somit später als Spezialfälle gesondert analysiert werden.\n",
    "- Das Ziel durch das Clustering die Daten zu sortieren und für das Labeling vorzubereiten konnte dadurch erreicht werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
